"""
Google Sheets ë°ì´í„° ë¡œë”

ì´ ëª¨ë“ˆì€ Google Sheets APIë¥¼ í†µí•´
ìŠ¤ëƒ…ìƒ·, ì´ë™ ì›ì¥, WIP ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import logging
from typing import Optional

import pandas as pd
import streamlit as st

from scm_dashboard_v9.common.performance import measure_time_context

logger = logging.getLogger(__name__)

from scm_dashboard_v9.data_sources.loaders import (
    load_from_gsheet_api,
    load_wip_from_incoming,
    merge_wip_as_moves,
    normalize_tk_stock_distrib,
)
from scm_dashboard_v9.domain.normalization import (
    normalize_moves,
)
from scm_dashboard_v9.domain.normalization import (
    normalize_snapshot as normalize_refined_snapshot,
)

from .excel import LoadedData


def load_from_gsheet(*, show_spinner_message: str) -> Optional[LoadedData]:
    """
    Google Sheets APIë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì •ê·œí™”í•©ë‹ˆë‹¤.

    Google Sheetsì—ì„œ ë‹¤ìŒ ì‹œíŠ¸ë¥¼ ì½ìŠµë‹ˆë‹¤:
    - ì´ë™ ì›ì¥ ì‹œíŠ¸
    - ìŠ¤ëƒ…ìƒ· ì‹œíŠ¸
    - ì…ê³  ì˜ˆì • ì‹œíŠ¸ (WIP)

    WIP ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì´ë™ ì›ì¥ì— ìë™ìœ¼ë¡œ ë³‘í•©ë©ë‹ˆë‹¤.

    Args:
        show_spinner_message: ë¡œë”© ì¤‘ í‘œì‹œí•  ìŠ¤í”¼ë„ˆ ë©”ì‹œì§€

    Returns:
        LoadedData ì¸ìŠ¤í„´ìŠ¤. ë¡œë“œ ì‹¤íŒ¨ ì‹œ None.

    Examples:
        >>> data = load_from_gsheet(show_spinner_message="ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
        >>> if data:
        ...     print(f"Loaded {len(data.moves)} moves")
    """
    # ========================================
    # 1ë‹¨ê³„: Google Sheets API í˜¸ì¶œ (ìŠ¤í”¼ë„ˆ í‘œì‹œ)
    # ========================================
    logger.info("Loading data from Google Sheets")
    try:
        with st.spinner(show_spinner_message):
            with measure_time_context("Google Sheets API fetch"):
                df_move, df_ref, df_incoming, df_tk_stock = load_from_gsheet_api()
                logger.debug(
                    f"Raw data loaded: {len(df_move)} moves, {len(df_ref)} snapshots, "
                    f"{len(df_incoming)} incoming, {len(df_tk_stock)} tk_stock"
                )

    except Exception as exc:  # pragma: no cover - streamlit feedback
        logger.error(f"Failed to load from Google Sheets: {exc}", exc_info=True)
        st.error(f"Google Sheets ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {exc}")
        return None

    # ========================================
    # 2ë‹¨ê³„: ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
    # ========================================
    if df_move.empty or df_ref.empty:
        logger.error("Google Sheets data is empty")
        st.error("Google Sheetsì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None

    # ========================================
    # 3ë‹¨ê³„: ë°ì´í„° ì •ê·œí™”
    # ========================================
    logger.info("Normalizing snapshot and moves data")

    # ë””ë²„ê·¸: normalize ì „ snap_time ì›ë³¸ ê°’ í™•ì¸
    if "snap_time" in df_ref.columns and "center" in df_ref.columns:
        st.write("### ğŸ” DEBUG: normalize ì „ snap_time ì›ë³¸ ê°’")
        for center in ["SBSMY", "SBSSG", "SBSTH", "SBSPH"]:
            center_data = df_ref[df_ref["center"] == center]
            if not center_data.empty:
                st.write(f"#### {center} (ì´ {len(center_data)}í–‰)")

                # ê³ ìœ ê°’ í™•ì¸
                unique_vals = center_data["snap_time"].unique()
                st.write(f"- ê³ ìœ  snap_time ê°’: {len(unique_vals)}ê°œ")

                # íƒ€ì…ë³„ ë¶„í¬
                type_counts = {}
                for val in center_data["snap_time"]:
                    t = type(val).__name__
                    type_counts[t] = type_counts.get(t, 0) + 1
                st.write(f"- íƒ€ì… ë¶„í¬: {type_counts}")

                # ìƒ˜í”Œ ê°’
                sample_values = center_data["snap_time"].head(5).tolist()
                st.write(f"- ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ): {sample_values}")

                # ë¹ˆ ê°’/ì´ìƒí•œ ê°’ ì²´í¬
                empty_count = center_data["snap_time"].isna().sum()
                zero_count = (center_data["snap_time"] == 0).sum()
                empty_str_count = (center_data["snap_time"] == "").sum()
                st.write(
                    f"- NaN: {empty_count}ê°œ, 0: {zero_count}ê°œ, ë¹ˆë¬¸ìì—´: {empty_str_count}ê°œ"
                )

                # ë³€í™˜ í…ŒìŠ¤íŠ¸
                test_converted = pd.to_datetime(
                    center_data["snap_time"], errors="coerce"
                )
                nat_count = test_converted.isna().sum()
                valid_count = test_converted.notna().sum()
                st.write(
                    f"- pd.to_datetime ë³€í™˜ ê²°ê³¼: ìœ íš¨ {valid_count}ê°œ, NaT {nat_count}ê°œ"
                )

                # NaTê°€ ë˜ëŠ” ì›ë³¸ ê°’ ìƒ˜í”Œ
                if nat_count > 0:
                    nat_originals = (
                        center_data[test_converted.isna()]["snap_time"]
                        .head(10)
                        .tolist()
                    )
                    st.write(f"- NaTê°€ ë˜ëŠ” ì›ë³¸ ê°’ ìƒ˜í”Œ: {nat_originals}")

                st.write("")

    with measure_time_context("Data normalization"):
        moves = normalize_moves(df_move)
        snapshot = normalize_refined_snapshot(df_ref)
        logger.debug(f"Normalized: {len(moves)} moves, {len(snapshot)} snapshots")

    # ë””ë²„ê·¸: normalize í›„ snap_time ê²°ê³¼ í™•ì¸
    if "snap_time" in snapshot.columns and "center" in snapshot.columns:
        st.write("### ğŸ” DEBUG: normalize í›„ snap_time ê²°ê³¼")
        for center in ["SBSMY", "SBSSG", "SBSTH", "SBSPH"]:
            center_data = snapshot[snapshot["center"] == center]
            if not center_data.empty:
                total = len(center_data)
                valid = center_data["snap_time"].notna().sum()
                nat = center_data["snap_time"].isna().sum()
                st.write(f"**{center}**: ì´ {total}í–‰, ìœ íš¨ {valid}ê°œ, NaT {nat}ê°œ")
        st.write("")

    # ========================================
    # 4ë‹¨ê³„: WIP ë°ì´í„° ë³‘í•© (ìˆëŠ” ê²½ìš°)
    # ========================================
    try:
        wip_df = load_wip_from_incoming(df_incoming)
        moves = merge_wip_as_moves(moves, wip_df)

        if wip_df is not None and not wip_df.empty:
            logger.info(f"WIP data merged: {len(wip_df)} rows")
            st.success(f"WIP {len(wip_df)}ê±´ ë°˜ì˜ ì™„ë£Œ")

    except Exception as exc:  # pragma: no cover - streamlit feedback
        logger.warning(f"Failed to load WIP data: {exc}", exc_info=True)
        st.warning(f"WIP ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {exc}")

    # ========================================
    # 5ë‹¨ê³„: ì„±ê³µ ë©”ì‹œì§€ í‘œì‹œ
    # ========================================
    logger.info("Google Sheets data loaded successfully")
    st.success("Google Sheets ë°ì´í„°ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # íƒœê´‘KR ê°€ìƒì°½ê³  ë°°ë¶„ ì‹œíŠ¸ë¥¼ ì •ê·œí™”í•˜ì—¬ ëŒ€ì‹œë³´ë“œì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì €ì¥
    tk_stock_distrib = normalize_tk_stock_distrib(df_tk_stock)

    return LoadedData(
        moves=moves,
        snapshot=snapshot,
        tk_stock_distrib=tk_stock_distrib,
    )
