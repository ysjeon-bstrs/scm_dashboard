
import pandas as pd
import numpy as np
import streamlit as st
import plotly.express as px
import re
from datetime import datetime, timedelta
from io import BytesIO
from typing import Dict, List, Optional, Tuple
import gspread
from google.oauth2.service_account import Credentials

from center_alias import normalize_center_series, normalize_center_value

# =========================
# Global configuration
# =========================
st.set_page_config(page_title="ê¸€ë¡œë²Œ ëŒ€ì‹œë³´ë“œ â€” v4", layout="wide")
st.title("ğŸ“¦ SCM ì¬ê³  íë¦„ ëŒ€ì‹œë³´ë“œ â€” v4")

# ë°ì´í„° ì†ŒìŠ¤ ìƒíƒœ(excel | csv | gsheet). UIì˜ ì…ë ¥ íƒ­ì—ì„œ ì„¸íŒ…í•¨
if "_data_source" not in st.session_state:
    st.session_state["_data_source"] = None  # ì•„ì§ ë¯¸ì •
# on-demandë¡œ ì‚¬ìš©í•  snapshot_raw ìºì‹œ (ì—‘ì…€ì—ì„œ ì½ì€ ê²½ìš° ì €ì¥)
if "_snapshot_raw_cache" not in st.session_state:
    st.session_state["_snapshot_raw_cache"] = None

# ì›ë³¸ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ID (Google Sheets íƒ­ì—ì„œë§Œ ì‚¬ìš©)
GSHEET_ID = "1RYjKW2UDJ2kWJLAqQH26eqx2-r9Xb0_qE_hfwu9WIj8"

# ì„¼í„°ë³„ ì›ë³¸ ì»¬ëŸ¼ ë§¤í•‘
CENTER_COL = {
    "íƒœê´‘KR": "stock2",
    "AMZUS": "fba_available_stock",
    "í’ˆê³ KR": "poomgo_v2_available_stock",
    "SBSPH": "shopee_ph_available_stock",
    "SBSSG": "shopee_sg_available_stock",
    "SBSMY": "shopee_my_available_stock",
    "AcrossBUS": "acrossb_available_stock",
    "ì–´í¬ë¡œìŠ¤ë¹„US": "acrossb_available_stock",  # ë³„ì¹­ í†µì¼
}

# -------------------- Small helpers --------------------
def _coalesce_columns(df: pd.DataFrame, candidates: List, parse_date: bool = False) -> pd.Series:
    """
    dfì—ì„œ í›„ë³´ ì»¬ëŸ¼ë“¤ ì¤‘ ì²« ë²ˆì§¸ ìœ íš¨ ì»¬ëŸ¼ì„ ì°¾ì•„ ê°’ì„ ë°˜í™˜.
    parse_date=Trueë©´ datetimeìœ¼ë¡œ íŒŒì‹±.
    """
    all_names = []
    for item in candidates:
        if isinstance(item, (list, tuple, set)):
            all_names.extend([str(x).strip() for x in item])
        else:
            all_names.append(str(item).strip())

    # ì •í™• ì¼ì¹˜ â†’ ëŒ€ì†Œë¬¸ì ë¬´ì‹œ í¬í•¨ ê²€ìƒ‰ â†’ ë¶€ë¶„ ì¼ì¹˜
    cols = [c for c in df.columns if str(c).strip() in all_names]
    if not cols:
        cols = [c for c in df.columns if any(name.lower() in str(c).lower() for name in all_names)]
    if not cols:
        cols = [c for c in df.columns if any(name.lower() in str(c).lower() or str(c).lower() in name.lower() for name in all_names)]

    if not cols:
        return pd.Series(pd.NaT if parse_date else np.nan, index=df.index)

    sub = df[cols].copy()
    if parse_date:
        for c in cols:
            sub[c] = pd.to_datetime(sub[c], errors="coerce")
    out = sub.bfill(axis=1).iloc[:, 0]
    return out


# -------------------- Google Sheets (API authentication) loader --------------------
@st.cache_data(ttl=300)
def load_from_gsheet_api():
    import json
    from google.oauth2.service_account import Credentials
    import gspread

    scopes = [
        "https://www.googleapis.com/auth/spreadsheets.readonly",
        "https://www.googleapis.com/auth/drive.readonly",
    ]

    # -- secrets 3í˜•ì‹ ì§€ì› --
    try:
        gs = st.secrets["google_sheets"]
    except Exception as e:
        st.error("Google Sheets API ì¸ì¦ ì‹¤íŒ¨: secretsì— [google_sheets] ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

    creds_obj = gs.get("credentials", None)
    creds_json = gs.get("credentials_json", None)

    if creds_obj is not None:
        # ì¤‘ì²©/ì¸ë¼ì¸ í…Œì´ë¸”
        if isinstance(creds_obj, dict):
            credentials_info = dict(creds_obj)
        else:
            # Streamlit Secrets ê°ì²´ â†’ dict
            credentials_info = {k: creds_obj[k] for k in creds_obj.keys()}
    elif creds_json:
        # ë©€í‹°ë¼ì¸ JSON ë¬¸ìì—´
        credentials_info = json.loads(str(creds_json))
    else:
        st.error("Google Sheets API ì¸ì¦ ì‹¤íŒ¨: credentials(or credentials_json) ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

    # ê°œí–‰ ë³µêµ¬(ì¸ë¼ì¸ í…Œì´ë¸” ëŒ€ë¹„)
    if "private_key" in credentials_info:
        credentials_info["private_key"] = credentials_info["private_key"].replace("\\n", "\n").strip()

    try:
        credentials = Credentials.from_service_account_info(credentials_info, scopes=scopes)
        gc = gspread.authorize(credentials)
        ss = gc.open_by_key(GSHEET_ID)
    except Exception as e:
        st.error(f"Google Sheets API ì¸ì¦ ì‹¤íŒ¨: {e}")
        st.error("secrets í˜•ì‹: [google_sheets.credentials] (ê¶Œì¥) ë˜ëŠ” [google_sheets] credentials_json")
        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

    # ì‹œíŠ¸ ì½ê¸°
    def _read(name):
        try:
            return pd.DataFrame(ss.worksheet(name).get_all_records())
        except Exception as e:
            st.warning(f"{name} ì‹œíŠ¸ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            return pd.DataFrame()

    # ê¸°ì¡´ 3ê°œ ì‹œíŠ¸
    df_move = _read("SCM_í†µí•©")
    df_ref = _read("snap_ì •ì œ")
    df_incoming = _read("ì…ê³ ì˜ˆì •ë‚´ì—­")

    # ğŸ”¹ snapshot_raw(ì„ íƒ)ë„ ì‹œë„í•´ì„œ ì½ê³ , ì½íˆë©´ ì„¸ì…˜ ìºì‹œì— ì €ì¥
    try:
        df_snap_raw = _read("snapshot_raw")
        if not df_snap_raw.empty:
            # ë©”ëª¨ ì ˆê°ì„ ìœ„í•´ ìµœì‹  ìŠ¤ëƒ…ìƒ·ë§Œ ë³´ê´€ (ì˜µì…˜)
            cols = {c.strip().lower(): c for c in df_snap_raw.columns}
            col_date = cols.get("snapshot_date") or cols.get("date")
            if col_date:
                df_snap_raw[col_date] = pd.to_datetime(df_snap_raw[col_date], errors="coerce").dt.normalize()
                latest = df_snap_raw[col_date].max()
                if pd.notna(latest):
                    df_snap_raw = df_snap_raw[df_snap_raw[col_date] == latest].copy()
            st.session_state["_snapshot_raw_cache"] = df_snap_raw  # âœ… ìºì‹œì— ì €ì¥
        else:
            st.session_state["_snapshot_raw_cache"] = None
    except Exception:
        # ì—†ê±°ë‚˜ ê¶Œí•œ ì—†ìœ¼ë©´ ì¡°ìš©íˆ íŒ¨ìŠ¤ (ë¡œíŠ¸ ìƒì„¸ëŠ” ìë™ìœ¼ë¡œ ë¯¸í‘œì‹œ)
        st.session_state["_snapshot_raw_cache"] = None

    return df_move, df_ref, df_incoming

# -------------------- Loaders --------------------
@st.cache_data(ttl=300)
def load_from_excel(file):
    """
    í•„ìˆ˜: SCM_í†µí•©, snap_ì •ì œ
    ì„ íƒ: ì…ê³ ì˜ˆì •ë‚´ì—­, snapshot_raw
    """
    data = file.read() if hasattr(file, "read") else file
    bio = BytesIO(data) if isinstance(data, (bytes, bytearray)) else file

    xl = pd.ExcelFile(bio, engine="openpyxl")

    if "SCM_í†µí•©" not in xl.sheet_names:
        st.error("ì—‘ì…€ì— 'SCM_í†µí•©' ì‹œíŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        st.stop()

    df_move = pd.read_excel(bio, sheet_name="SCM_í†µí•©", engine="openpyxl")
    bio.seek(0)

    refined_name = next((s for s in xl.sheet_names if s in ["snap_ì •ì œ","snap_refined","snap_refine","snap_ref"]), None)
    if refined_name is None:
        st.error("ì—‘ì…€ì— ì •ì œ ìŠ¤ëƒ…ìƒ· ì‹œíŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤. (ì‹œíŠ¸ëª…: 'snap_ì •ì œ' ë˜ëŠ” 'snap_refined')")
        st.stop()
    df_ref = pd.read_excel(bio, sheet_name=refined_name, engine="openpyxl")
    bio.seek(0)

    df_incoming = None
    if "ì…ê³ ì˜ˆì •ë‚´ì—­" in xl.sheet_names:
        df_incoming = pd.read_excel(bio, sheet_name="ì…ê³ ì˜ˆì •ë‚´ì—­", engine="openpyxl")
        bio.seek(0)

    # snapshot_rawë„ ìˆìœ¼ë©´ ìºì‹œì— ë³´ê´€ (ì¬ê³ ìì‚° ê³„ì‚°ìš©)
    snapshot_raw_df = None
    if "snapshot_raw" in xl.sheet_names:
        snapshot_raw_df = pd.read_excel(bio, sheet_name="snapshot_raw", engine="openpyxl")
        bio.seek(0)

    return df_move, df_ref, df_incoming, snapshot_raw_df


@st.cache_data(ttl=300)
def load_snapshot_raw():
    """
    ì¬ê³ ìì‚° ê³„ì‚°ìš© ì›ë³¸ ìŠ¤ëƒ…ìƒ·.
    - EXCEL/CSVë¥¼ ì‚¬ìš© ì¤‘ì´ë©´ ì—…ë¡œë“œ íŒŒì¼ì˜ snapshot_raw(ìˆì„ ë•Œë§Œ) ì‚¬ìš©
      (ì—†ìœ¼ë©´ ë¹ˆ DF, ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥í•˜ì§€ ì•ŠìŒ)
    - Apps Script í”„ë¡ì‹œ(_fetch_sheet_via_webapp)ê°€ ìˆìœ¼ë©´ ê·¸ìª½ì„ ìš°ì„  ì‚¬ìš©
    """
    # 1) ì—…ë¡œë“œ ìºì‹œê°€ ìˆìœ¼ë©´ ìµœìš°ì„ 
    if st.session_state.get("_snapshot_raw_cache") is not None:
        return st.session_state["_snapshot_raw_cache"]

    # 2) Apps Script í”„ë¡ì‹œ ì œê³µ ì‹œ ì‚¬ìš©
    fetch = globals().get("_fetch_sheet_via_webapp", None)
    if callable(fetch):
        try:
            df = fetch("snapshot_raw")
            if df is not None and not df.empty:
                return df
        except Exception:
            pass

    # 3) ê·¸ ì™¸ì—ëŠ” ì¡°ìš©íˆ ë¹ˆ DF
    return pd.DataFrame()

# -------------------- Normalizers --------------------
def normalize_refined_snapshot(df_ref: pd.DataFrame) -> pd.DataFrame:
    cols = {str(c).strip().lower(): c for c in df_ref.columns}

    # ë‚ ì§œ/ì„¼í„°/ì½”ë“œ/ìˆ˜ëŸ‰ ê¸°ë³¸ ì»¬ëŸ¼ íƒìƒ‰ (alias í™•ëŒ€)
    date_col     = next((cols[k] for k in ["date","ë‚ ì§œ","snapshot_date","ìŠ¤ëƒ…ìƒ·ì¼"] if k in cols), None)
    center_col   = next((cols[k] for k in ["center","ì„¼í„°","ì°½ê³ ","warehouse"] if k in cols), None)
    resource_col = next((cols[k] for k in ["resource_code","resource_cc","sku","ìƒí’ˆì½”ë“œ","product_code"] if k in cols), None)
    stock_col    = next((cols[k] for k in ["stock_qty","qty","ìˆ˜ëŸ‰","ì¬ê³ ","quantity"] if k in cols), None)

    # (ì‹ ê·œ) í’ˆëª… ì»¬ëŸ¼ íƒìƒ‰
    name_col     = next((cols[k] for k in ["resource_name","í’ˆëª…","ìƒí’ˆëª…","product_name"] if k in cols), None)

    missing = [n for n,v in {"date":date_col,"center":center_col,"resource_code":resource_col,"stock_qty":stock_col}.items() if not v]
    if missing:
        st.error(f"'snap_ì •ì œ' ì‹œíŠ¸ì— ëˆ„ë½ëœ ì»¬ëŸ¼: {missing}")
        st.stop()

    result = df_ref.rename(columns={
        date_col: "date",
        center_col: "center",
        resource_col: "resource_code",
        stock_col: "stock_qty",
        **({name_col: "resource_name"} if name_col else {})
    }).copy()

    result["date"] = pd.to_datetime(result["date"], errors="coerce").dt.normalize()
    result["center"] = normalize_center_series(result["center"])
    result["resource_code"] = result["resource_code"].astype(str)
    result["stock_qty"] = pd.to_numeric(result["stock_qty"], errors="coerce").fillna(0).astype(int)

    if "resource_name" in result.columns:
        # ë¬¸ìì—´ ì •ë¦¬ (NaN ë°©ì§€)
        result["resource_name"] = result["resource_name"].astype(str).str.strip().replace({"nan": "", "None": ""})

    return result.dropna(subset=["date","center","resource_code"])

def normalize_moves(df: pd.DataFrame) -> pd.DataFrame:
    df.columns = [str(c).strip() for c in df.columns]

    resource_code = _coalesce_columns(df, [["resource_code","ìƒí’ˆì½”ë“œ","RESOURCE_CODE","sku","SKU"]])
    qty_ea       = _coalesce_columns(df, [["qty_ea","QTY_EA","ìˆ˜ëŸ‰(EA)","qty","QTY","quantity","Quantity","ìˆ˜ëŸ‰","EA","ea"]])
    carrier_mode = _coalesce_columns(df, [["carrier_mode","ìš´ì†¡ë°©ë²•","carrier mode","ìš´ì†¡ìˆ˜ë‹¨"]])
    from_center  = _coalesce_columns(df, [["from_center","ì¶œë°œì°½ê³ ","from center"]])
    to_center    = _coalesce_columns(df, [["to_center","ë„ì°©ì°½ê³ ","to center"]])
    onboard_date = _coalesce_columns(df, [["onboard_date","ë°°ì •ì¼","ì¶œë°œì¼","H","onboard","depart_date"]], parse_date=True)
    arrival_date = _coalesce_columns(df, [["arrival_date","ë„ì°©ì¼","eta_date","ETA","arrival"]], parse_date=True)
    inbound_date = _coalesce_columns(df, [["inbound_date","ì…ê³ ì¼","ì…ê³ ì™„ë£Œì¼"]], parse_date=True)
    out = pd.DataFrame({
        "resource_code": resource_code.astype(str).str.strip(),
        "qty_ea": pd.to_numeric(qty_ea.astype(str).str.replace(',',''), errors="coerce").fillna(0).astype(int),
        "carrier_mode": carrier_mode.astype(str).str.strip(),
        "from_center": normalize_center_series(from_center),
        "to_center": normalize_center_series(to_center),
        "onboard_date": onboard_date,
        "arrival_date": arrival_date,
        "inbound_date": inbound_date,
    })
    out["event_date"] = out["inbound_date"].where(out["inbound_date"].notna(), out["arrival_date"])
    for col in ["onboard_date", "arrival_date", "inbound_date", "event_date"]:
        out[col] = pd.to_datetime(out[col], errors="coerce").dt.normalize()
    return out

# PO ë²ˆí˜¸ â†’ ë‚ ì§œ íŒŒì‹±
def _parse_po_date(po_str: str) -> pd.Timestamp:
    if not isinstance(po_str, str):
        return pd.NaT
    m = re.search(r"[A-Za-z](\d{2})(\d{2})(\d{2})", po_str)
    if not m:
        return pd.NaT
    yy, mm, dd = m.groups()
    year = 2000 + int(yy)
    try:
        return pd.Timestamp(datetime(year, int(mm), int(dd)))
    except Exception:
        return pd.NaT

def load_wip_from_incoming(df_incoming: Optional[pd.DataFrame], default_center: str = "íƒœê´‘KR") -> pd.DataFrame:
    if df_incoming is None or df_incoming.empty:
        return pd.DataFrame()

    df_incoming.columns = [str(c).strip().lower() for c in df_incoming.columns]
    po_col   = next((c for c in df_incoming.columns if c in ["po_no","ponumber","po"]), None)
    date_col = next((c for c in df_incoming.columns if "intended_push_date" in c or "ì…ê³ " in c), None)
    sku_col  = next((c for c in df_incoming.columns if c in ["product_code","resource_code","ìƒí’ˆì½”ë“œ"]), None)
    qty_col  = next((c for c in df_incoming.columns if c in ["quantity","qty","ìˆ˜ëŸ‰","total_quantity"]), None)
    lot_col  = next((c for c in df_incoming.columns if c in ["lot","ì œì¡°ë²ˆí˜¸","lot_no","lotnumber"]), None)

    if not date_col or not sku_col or not qty_col:
        return pd.DataFrame()

    out = pd.DataFrame({
        "resource_code": df_incoming[sku_col].astype(str).str.strip(),
        "to_center": default_center,
        "wip_ready": pd.to_datetime(df_incoming[date_col], errors="coerce"),
        "qty_ea": pd.to_numeric(df_incoming[qty_col].astype(str).str.replace(',',''), errors="coerce").fillna(0).astype(int),
        "lot": df_incoming[lot_col].astype(str).str.strip() if lot_col else ""
    })
    out["wip_start"] = df_incoming[po_col].map(_parse_po_date) if po_col else pd.NaT
    mask_na = out["wip_start"].isna() & out["wip_ready"].notna()
    out.loc[mask_na, "wip_start"] = out.loc[mask_na, "wip_ready"] - pd.to_timedelta(30, unit="D")

    out = out.dropna(subset=["resource_code","wip_ready","wip_start"]).reset_index(drop=True)
    return out[["resource_code","to_center","wip_start","wip_ready","qty_ea","lot"]]

def merge_wip_as_moves(moves_df: pd.DataFrame, wip_df: Optional[pd.DataFrame]) -> pd.DataFrame:
    if wip_df is None or wip_df.empty:
        return moves_df
    wip_df_norm = wip_df.copy()
    wip_df_norm["to_center"] = normalize_center_series(wip_df_norm["to_center"])
    wip_df_norm["wip_start"] = pd.to_datetime(wip_df_norm["wip_start"], errors="coerce").dt.normalize()
    wip_df_norm["wip_ready"] = pd.to_datetime(wip_df_norm["wip_ready"], errors="coerce").dt.normalize()

    def _first_valid_center(series: Optional[pd.Series]) -> Optional[str]:
        if series is None:
            return None
        for value in series.dropna().astype(str).str.strip():
            if value and value.upper() != "WIP":
                return value
        return None

    default_center = _first_valid_center(wip_df_norm.get("to_center"))
    if default_center is None and "to_center" in moves_df:
        default_center = _first_valid_center(moves_df["to_center"])
    if default_center is None and "from_center" in moves_df:
        default_center = _first_valid_center(moves_df["from_center"])

    wip_moves = pd.DataFrame({
        "resource_code": wip_df_norm["resource_code"],
        "qty_ea": wip_df_norm["qty_ea"].astype(int),
        "carrier_mode": "WIP",
        "from_center": "WIP",
        "to_center": wip_df_norm["to_center"],
        "onboard_date": wip_df_norm["wip_start"],
        "arrival_date": wip_df_norm["wip_ready"],
        "inbound_date": pd.NaT,
        "event_date": wip_df_norm["wip_ready"],
        "lot": wip_df_norm.get("lot", "")
    })

    wip_moves["to_center"] = normalize_center_series(wip_moves["to_center"])
    mask_to_wip = (wip_moves["to_center"].str.upper() == "WIP").fillna(False)
    if default_center:
        wip_moves.loc[mask_to_wip, "to_center"] = default_center
    else:
        wip_moves.loc[mask_to_wip, "to_center"] = pd.NA

    wip_moves["from_center"] = normalize_center_series(wip_moves["from_center"])
    mask_from_wip = (wip_moves["from_center"].str.upper() == "WIP").fillna(False)
    wip_moves.loc[mask_from_wip, "from_center"] = (
        wip_moves.loc[mask_from_wip, "to_center"].fillna(default_center)
    )

    for col in ["onboard_date", "arrival_date", "event_date"]:
        wip_moves[col] = pd.to_datetime(wip_moves[col], errors="coerce").dt.normalize()
    return pd.concat([moves_df, wip_moves], ignore_index=True)

# -------------------- ì†Œë¹„(ì†Œì§„) ì¶”ì„¸ + ì´ë²¤íŠ¸ ê°€ì¤‘ì¹˜ --------------------
@st.cache_data(ttl=3600)
def estimate_daily_consumption(snap_long: pd.DataFrame,
                               centers_sel: List[str], skus_sel: List[str],
                               asof_dt: pd.Timestamp,
                               lookback_days: int = 28) -> Dict[Tuple[str, str], float]:
    snap = snap_long.rename(columns={"snapshot_date":"date"}).copy()
    snap["date"] = pd.to_datetime(snap["date"], errors="coerce").dt.normalize()
    start = (asof_dt - pd.Timedelta(days=int(lookback_days)-1)).normalize()

    hist = snap[(snap["date"] >= start) & (snap["date"] <= asof_dt) &
                (snap["center"].isin(centers_sel)) &
                (snap["resource_code"].isin(skus_sel))]

    rates = {}
    if hist.empty:
        return rates

    for (ct, sku), g in hist.groupby(["center","resource_code"]):
        ts = (g.sort_values("date")
                .set_index("date")["stock_qty"]
                .asfreq("D").ffill())
        if ts.dropna().shape[0] < max(7, lookback_days//2):
            continue
        x = np.arange(len(ts), dtype=float)
        y = ts.values.astype(float)
        rate_reg = max(0.0, -np.polyfit(x, y, 1)[0])  # ê°ì†Œë§Œ
        dec = (-np.diff(y)).clip(min=0)
        rate_dec = dec.mean() if len(dec) else 0.0
        rate = max(rate_reg, rate_dec)
        if rate > 0:
            rates[(ct, sku)] = float(rate)
    return rates

@st.cache_data(ttl=1800)
def apply_consumption_with_events(
    timeline: pd.DataFrame,
    snap_long: pd.DataFrame,
    centers_sel: List[str], skus_sel: List[str],
    start_dt: pd.Timestamp, end_dt: pd.Timestamp,
    lookback_days: int = 28,
    events: Optional[List[Dict]] = None
) -> pd.DataFrame:
    out = timeline.copy()
    if out.empty:
        return out
    out["date"] = pd.to_datetime(out["date"]).dt.normalize()

    snap_cols = {c.lower(): c for c in snap_long.columns}
    date_col = snap_cols.get("date") or snap_cols.get("snapshot_date")
    if date_col is None:
        raise KeyError("snap_longì—ëŠ” 'date' ë˜ëŠ” 'snapshot_date' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    latest_snap = pd.to_datetime(snap_long[date_col]).max().normalize()
    cons_start = max(latest_snap + pd.Timedelta(days=1), start_dt)
    if cons_start > end_dt:
        return out

    idx = pd.date_range(cons_start, end_dt, freq="D")
    uplift = pd.Series(1.0, index=idx)
    if events:
        for e in events:
            s = pd.to_datetime(e.get("start"), errors="coerce")
            t = pd.to_datetime(e.get("end"), errors="coerce")
            u = min(3.0, max(-1.0, float(e.get("uplift", 0.0))))
            if pd.notna(s) and pd.notna(t):
                s = s.normalize(); t = t.normalize()
                s = max(s, idx[0]); t = min(t, idx[-1])
                if s <= t:
                    uplift.loc[s:t] = uplift.loc[s:t] * (1.0 + u)

    rates = estimate_daily_consumption(snap_long, centers_sel, skus_sel, latest_snap, int(lookback_days))

    chunks: list[pd.DataFrame] = []
    for (ct, sku), g in out.groupby(["center","resource_code"]):
        if ct in ("In-Transit", "WIP"):
            chunks.append(g)
            continue

        rate = float(rates.get((ct, sku), 0.0))
        if rate <= 0:
            chunks.append(g)
            continue

        g = g.sort_values("date").copy()
        mask = g["date"] >= cons_start
        if not mask.any():
            chunks.append(g)
            continue

        daily = g.loc[mask, "date"].map(uplift).fillna(1.0).values * rate
        stk = g.loc[mask, "stock_qty"].astype(float).values
        for i in range(len(stk)):
            dec = daily[i]
            stk[i:] = np.maximum(0.0, stk[i:] - dec)
        g.loc[mask, "stock_qty"] = stk
        chunks.append(g)

    if not chunks:
        return out

    out = pd.concat(chunks, ignore_index=True)
    # ë” ê°•ë ¥í•œ NaN ì²˜ë¦¬
    out["stock_qty"] = pd.to_numeric(out["stock_qty"], errors="coerce")
    out["stock_qty"] = out["stock_qty"].fillna(0)
    out["stock_qty"] = out["stock_qty"].replace([np.inf, -np.inf], 0)
    out["stock_qty"] = out["stock_qty"].round().clip(lower=0).astype(int)
    return out


# -------------------- Timeline --------------------
def build_timeline(snap_long: pd.DataFrame, moves: pd.DataFrame, 
                   centers_sel: List[str], skus_sel: List[str],
                   start_dt: pd.Timestamp, end_dt: pd.Timestamp, 
                   horizon_days: int = 0, today: Optional[pd.Timestamp] = None,
                   lag_days: int = 7) -> pd.DataFrame:
    if today is None:
        today = pd.Timestamp.today().normalize()
    horizon_end = end_dt + pd.Timedelta(days=horizon_days)
    full_dates = pd.date_range(start_dt, horizon_end, freq="D")

    # ì´ë™ê±´ ë³µì‚¬
    mv_all = moves.copy()

    base = snap_long[
        snap_long["center"].isin(centers_sel) &
        snap_long["resource_code"].isin(skus_sel)
    ].copy().rename(columns={"snapshot_date":"date"})
    if base.empty:
        return pd.DataFrame(columns=["date","center","resource_code","stock_qty"])
    base = base[(base["date"] >= start_dt) & (base["date"] <= end_dt)]

    lines = []

    # 1) ì‹¤ì œ ì„¼í„° ë¼ì¸
    for (ct, sku), grp in base.groupby(["center","resource_code"]):
        grp = grp.sort_values("date")
        last_dt = grp["date"].max()

        if horizon_days > 0:
            proj_dates = pd.date_range(last_dt + pd.Timedelta(days=1), horizon_end, freq="D")
            proj_df = pd.DataFrame({"date": proj_dates, "center": ct,
                                    "resource_code": sku, "stock_qty": np.nan})
            ts = pd.concat([grp[["date","center","resource_code","stock_qty"]], proj_df], ignore_index=True)
        else:
            ts = grp[["date","center","resource_code","stock_qty"]].copy()

        ts = ts.sort_values("date")
        ts["stock_qty"] = ts["stock_qty"].ffill()

        mv = mv_all[mv_all["resource_code"] == sku].copy()

        # ì¶œê³ (-) ì´ë²¤íŠ¸
        eff_minus = (
            mv[(mv["from_center"].astype(str) == str(ct)) &
               (mv["onboard_date"].notna()) &
               (mv["onboard_date"] > last_dt)]
            .groupby("onboard_date", as_index=False)["qty_ea"].sum()
            .rename(columns={"onboard_date":"date","qty_ea":"delta"})
        )
        eff_minus["delta"] *= -1

        # ì…ê³ (+) ì´ë²¤íŠ¸ (ì˜ˆì¸¡ ì…ê³ ì¼ ê³„ì‚°)
        mv_center = mv[(mv["to_center"].astype(str) == str(ct))].copy()
        if not mv_center.empty:
            # ì˜ˆì¸¡ ì…ê³ ì¼ ê³„ì‚° (ì´ë™ì¤‘ ì¢…ë£Œì¼ê³¼ ë™ì¼í•œ ë¡œì§)
            pred_inbound = pd.Series(pd.NaT, index=mv_center.index, dtype="datetime64[ns]")
            
            # 1) inboundê°€ ìˆìœ¼ë©´ ê·¸ ë‚  ì…ê³ 
            mask_inb = mv_center["inbound_date"].notna()
            pred_inbound.loc[mask_inb] = mv_center.loc[mask_inb, "inbound_date"]
            
            # 2) inbound ì—†ê³  arrival ìˆëŠ” ê²½ìš°
            mask_arr = (~mask_inb) & mv_center["arrival_date"].notna()
            if mask_arr.any():
                # 2-1) arrivalì´ ê³¼ê±°ë©´: arrival + Nì¼ì— ì…ê³ (ê°€ì •)
                past_arr = mask_arr & (mv_center["arrival_date"] <= today)
                pred_inbound.loc[past_arr] = (
                    mv_center.loc[past_arr, "arrival_date"] + pd.Timedelta(days=int(lag_days))
                )
                
                # 2-2) arrivalì´ ë¯¸ë˜ë©´: arrivalì— ì…ê³ 
                fut_arr = mask_arr & (mv_center["arrival_date"] > today)
                pred_inbound.loc[fut_arr] = mv_center.loc[fut_arr, "arrival_date"]
            
            mv_center["pred_inbound_date"] = pred_inbound
            
            eff_plus = (
                mv_center[(mv_center["pred_inbound_date"].notna()) &
                          (mv_center["pred_inbound_date"] > last_dt)]
                .groupby("pred_inbound_date", as_index=False)["qty_ea"].sum()
                .rename(columns={"pred_inbound_date":"date","qty_ea":"delta"})
            )
        else:
            eff_plus = pd.DataFrame(columns=["date","delta"])

        # ë²¡í„°í™”ëœ ì²˜ë¦¬: ë‚ ì§œë³„ ì¦ê°(Delta) ì‹œë¦¬ì¦ˆë¡œ ë³€ê²½
        eff_all = pd.concat([eff_minus, eff_plus], ignore_index=True)
        if not eff_all.empty:
            # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í•©ê³„ ê³„ì‚°
            delta_series = eff_all.groupby("date")["delta"].sum()
            # ë‚ ì§œ ì¸ë±ìŠ¤ì— ë§ì¶° reindexí•˜ê³  ëˆ„ì í•© ê³„ì‚°
            delta_series = delta_series.reindex(ts["date"], fill_value=0).fillna(0)
            # ëˆ„ì í•© ëŒ€ì‹  ì§ì ‘ ë”í•˜ê¸° (ë” ì•ˆì „)
            for i, (date, delta) in enumerate(delta_series.items()):
                if delta != 0:
                    ts.loc[ts["date"] >= date, "stock_qty"] = ts.loc[ts["date"] >= date, "stock_qty"] + delta

        ts["stock_qty"] = ts["stock_qty"].fillna(0).replace([np.inf, -np.inf], 0).clip(lower=0)
        lines.append(ts)

        # (ë³´ê°•) WIP ì™„ë£Œ ë¬¼ëŸ‰ì„ í•´ë‹¹ ë„ì°© ì„¼í„° ë¼ì¸ì— ë°˜ì˜
        wip_complete = moves[
            (moves["resource_code"] == sku) &
            (moves["carrier_mode"].astype(str).str.upper() == "WIP") &
            (moves["to_center"] == ct) &
            (moves["event_date"].notna())
        ].copy()
        if not wip_complete.empty:
            wip_add = (
                wip_complete.groupby("event_date", as_index=False)["qty_ea"].sum()
                .rename(columns={"event_date":"date","qty_ea":"delta"})
            )
            # ë²¡í„°í™”ëœ WIP ì²˜ë¦¬
            wip_delta_series = wip_add.groupby("date")["delta"].sum()
            wip_delta_series = wip_delta_series.reindex(ts["date"], fill_value=0).fillna(0)
            # ëˆ„ì í•© ëŒ€ì‹  ì§ì ‘ ë”í•˜ê¸° (ë” ì•ˆì „)
            for date, delta in wip_delta_series.items():
                if delta != 0:
                    ts.loc[ts["date"] >= date, "stock_qty"] = ts.loc[ts["date"] >= date, "stock_qty"] + delta
            ts["stock_qty"] = ts["stock_qty"].fillna(0).replace([np.inf, -np.inf], 0).clip(lower=0)
            lines[-1] = ts  # ê°±ì‹ 

    # 2) In-Transit & WIP ê°€ìƒ ë¼ì¸
    moves_str = mv_all.copy()
    moves_str["from_center"] = moves_str["from_center"].astype(str)
    moves_str["to_center"] = moves_str["to_center"].astype(str)
    moves_str["carrier_mode"] = moves_str["carrier_mode"].astype(str).str.upper()

    mv_sel = moves_str[
        moves_str["resource_code"].isin(skus_sel) &
        (moves_str["from_center"].isin(centers_sel) | 
         moves_str["to_center"].isin(centers_sel) | 
         (moves_str["carrier_mode"] == "WIP"))
    ]

    for sku, g in mv_sel.groupby("resource_code"):
        # --- Non-WIP In-Transit (ë²¡í„°í™”) ----
        g_nonwip = g[g["carrier_mode"] != "WIP"]
        if not g_nonwip.empty:
            g_selected = g_nonwip[g_nonwip["to_center"].isin(centers_sel)]
            if not g_selected.empty:
                idx = pd.date_range(start_dt, horizon_end, freq="D")
                today_norm = (today or pd.Timestamp.today()).normalize()

                # ì¢…ë£Œì¼ ê³„ì‚°: inbound(ìˆìœ¼ë©´) / arrival(ë¯¸ë˜ë©´) / ê·¸ ì™¸ today+1
                end_eff = pd.Series(pd.NaT, index=g_selected.index, dtype="datetime64[ns]")

                # 1) inboundê°€ ìˆìœ¼ë©´ ê·¸ ë‚  ì¢…ë£Œ
                mask_inb = g_selected["inbound_date"].notna()
                end_eff.loc[mask_inb] = g_selected.loc[mask_inb, "inbound_date"]

                # 2) inbound ì—†ê³  arrival ìˆëŠ” ê²½ìš°
                mask_arr = (~mask_inb) & g_selected["arrival_date"].notna()
                if mask_arr.any():
                    # 2-1) arrivalì´ ê³¼ê±°ë©´: arrival + Nì¼ì— ì¢…ë£Œ(ê°€ì •)
                    past_arr = mask_arr & (g_selected["arrival_date"] <= today_norm)
                    end_eff.loc[past_arr] = (
                        g_selected.loc[past_arr, "arrival_date"] + pd.Timedelta(days=int(lag_days))
                    )

                    # 2-2) arrivalì´ ë¯¸ë˜ë©´: arrivalì— ì¢…ë£Œ
                    fut_arr = mask_arr & (g_selected["arrival_date"] > today_norm)
                    end_eff.loc[fut_arr] = g_selected.loc[fut_arr, "arrival_date"]

                # 3) ê·¸ë˜ë„ ë¹„ì–´ìˆìœ¼ë©´: today+1 (í™”ë©´ìƒ ì˜¤ëŠ˜ê¹Œì§€ë§Œ ì´ë™ì¤‘ìœ¼ë¡œ ë³´ì´ë„ë¡)
                end_eff = end_eff.fillna(min(today_norm + pd.Timedelta(days=1), idx[-1] + pd.Timedelta(days=1)))

                # starts/ends ë¸íƒ€ ë§Œë“¤ì–´ ëˆ„ì í•©
                g_selected_with_end = g_selected.copy()
                g_selected_with_end["end_date"] = end_eff

                starts = (g_selected_with_end
                          .dropna(subset=["onboard_date"])
                          .groupby("onboard_date")["qty_ea"].sum())
                ends = (g_selected_with_end
                        .groupby("end_date")["qty_ea"].sum() * -1)

                delta = (starts.rename_axis("date").to_frame("delta")
                           .add(ends.rename_axis("date").to_frame("delta"), fill_value=0)["delta"]
                           .sort_index())

                s = delta.reindex(idx, fill_value=0).cumsum().clip(lower=0)

                # carry(ê¸°ê°„ ì‹œì‘ ì´ì „ ì¶œë°œí•´ ì•„ì§ ì•ˆ ëë‚œ ê±´) ì²˜ë¦¬
                carry_mask = (
                    g_selected["onboard_date"].notna() &
                    (g_selected["onboard_date"] < idx[0]) &
                    (end_eff > idx[0])
                )
                carry = int(g_selected.loc[carry_mask, "qty_ea"].sum())
                if carry:
                    s = (s + carry).clip(lower=0)

                if s.any():
                    lines.append(pd.DataFrame({
                        "date": s.index, "center": "In-Transit",
                        "resource_code": sku, "stock_qty": s.values.astype(int)
                    }))

        # --- WIP ---
        g_wip = g[g["carrier_mode"] == "WIP"]
        if not g_wip.empty:
            # ë²¡í„°í™”ëœ WIP ì²˜ë¦¬
            s = pd.Series(0, index=pd.to_datetime(full_dates))
            
            # onboard +, event - ì˜ ëˆ„ì  íš¨ê³¼ë¥¼ ì—°ì† ê°’ìœ¼ë¡œ ë³€í™˜
            add_onboard = (
                g_wip[g_wip["onboard_date"].notna()]
                .groupby("onboard_date", as_index=False)["qty_ea"].sum()
                .rename(columns={"onboard_date":"date","qty_ea":"delta"})
            )
            add_event = (
                g_wip[g_wip["event_date"].notna()]
                .groupby("event_date", as_index=False)["qty_ea"].sum()
                .rename(columns={"event_date":"date","qty_ea":"delta"})
            )
            add_event["delta"] *= -1
            deltas = pd.concat([add_onboard, add_event], ignore_index=True)
            
            if not deltas.empty:
                # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í•©ê³„ ê³„ì‚°
                delta_series = deltas.groupby("date")["delta"].sum()
                # ë‚ ì§œ ì¸ë±ìŠ¤ì— ë§ì¶° reindexí•˜ê³  ì§ì ‘ ë”í•˜ê¸°
                delta_series = delta_series.reindex(s.index, fill_value=0).fillna(0)
                for date, delta in delta_series.items():
                    if delta != 0:
                        s.loc[s.index >= date] = s.loc[s.index >= date] + delta
                
                vdf = pd.DataFrame({"date": s.index, "center": "WIP",
                                    "resource_code": sku, "stock_qty": s.values})
                vdf["stock_qty"] = vdf["stock_qty"].fillna(0).replace([np.inf, -np.inf], 0).clip(lower=0)
                lines.append(vdf)

    if not lines:
        return pd.DataFrame(columns=["date","center","resource_code","stock_qty"])

    out = pd.concat(lines, ignore_index=True)
    out = out[(out["date"] >= start_dt) & (out["date"] <= horizon_end)]
    
    # ìµœì¢… NaN ì²˜ë¦¬
    out["stock_qty"] = pd.to_numeric(out["stock_qty"], errors="coerce")
    out["stock_qty"] = out["stock_qty"].fillna(0)
    out["stock_qty"] = out["stock_qty"].replace([np.inf, -np.inf], 0)
    out["stock_qty"] = out["stock_qty"].clip(lower=0).astype(int)
    
    return out

# -------------------- ë¹„ìš©(ì¬ê³ ìì‚°) í”¼ë²— --------------------
def pivot_inventory_cost_from_raw(snap_raw: pd.DataFrame,
                                  _latest_dt: pd.Timestamp,
                                  centers: list[str]) -> pd.DataFrame:
    if snap_raw is None or snap_raw.empty:
        return pd.DataFrame(columns=["resource_code"] + [f"{c}_ì¬ê³ ìì‚°" for c in centers])

    df = snap_raw.copy()
    cols = {str(c).strip().lower(): c for c in df.columns}
    col_date = cols.get("snapshot_date") or cols.get("date")
    col_sku  = cols.get("resource_code") or cols.get("sku") or cols.get("ìƒí’ˆì½”ë“œ") or cols.get("option1")
    col_cogs = cols.get("cogs")

    if not all([col_date, col_sku, col_cogs]):
        return pd.DataFrame(columns=["resource_code"] + [f"{c}_ì¬ê³ ìì‚°" for c in centers])

    df[col_date] = pd.to_datetime(df[col_date], errors="coerce").dt.normalize()
    df[col_sku]  = df[col_sku].astype(str)
    df[col_cogs] = pd.to_numeric(df[col_cogs], errors="coerce").fillna(0).clip(lower=0)

    # ìµœì‹  ìŠ¤ëƒ…ìƒ·(ì˜¤ëŠ˜ ìš°ì„  â†’ ì—†ìœ¼ë©´ ê°€ì¥ ìµœê·¼)
    today = pd.Timestamp.today().normalize()
    sub = df[df[col_date] == today].copy()
    if sub.empty:
        latest_date = df[col_date].max()
        sub = df[df[col_date] == latest_date].copy()
    if sub.empty:
        return pd.DataFrame(columns=["resource_code"] + [f"{c}_ì¬ê³ ìì‚°" for c in centers])

    cost_cols = {}
    for ct in centers:
        src_col = CENTER_COL.get(ct)
        if not src_col or src_col not in sub.columns:
            continue
        qty = pd.to_numeric(sub[src_col], errors="coerce").fillna(0).clip(lower=0)
        cost = (sub[col_cogs] * qty)
        g = sub[[col_sku]].copy()
        g[f"{ct}_ì¬ê³ ìì‚°"] = cost
        cost_cols[ct] = g.groupby(col_sku, as_index=False)[f"{ct}_ì¬ê³ ìì‚°"].sum()

    if not cost_cols:
        return pd.DataFrame(columns=["resource_code"] + [f"{c}_ì¬ê³ ìì‚°" for c in centers])

    base = pd.DataFrame({"resource_code": pd.unique(sub[col_sku])})
    for ct, g in cost_cols.items():
        base = base.merge(g.rename(columns={col_sku: "resource_code"}), on="resource_code", how="left")
    num_cols = [c for c in base.columns if c.endswith("_ì¬ê³ ìì‚°")]
    base[num_cols] = base[num_cols].fillna(0).round().astype(int)
    return base

# ==================== Tabs for inputs ====================
tab1, tab2 = st.tabs(["ì—‘ì…€ ì—…ë¡œë“œ", "Google Sheets"])

with tab1:
    xfile = st.file_uploader("ì—‘ì…€ ì—…ë¡œë“œ (.xlsx)", type=["xlsx"], key="excel")
    if xfile is not None:
        df_move, df_ref, df_incoming, snap_raw_df = load_from_excel(xfile)
        st.session_state["_data_source"] = "excel"
        st.session_state["_snapshot_raw_cache"] = snap_raw_df  # snapshot_raw ìˆìœ¼ë©´ ì €ì¥

        moves_raw = normalize_moves(df_move)
        snap_long = normalize_refined_snapshot(df_ref)

        try:
            wip_df = load_wip_from_incoming(df_incoming)
            moves = merge_wip_as_moves(moves_raw, wip_df)
            st.success(f"WIP {len(wip_df)}ê±´ ë°˜ì˜ ì™„ë£Œ" if wip_df is not None and not wip_df.empty else "WIP ì—†ìŒ")
        except Exception as e:
            moves = moves_raw
            st.warning(f"WIP ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")

with tab2:
    st.info("Google Sheets APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
    st.caption("ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ì¸ì¦í•©ë‹ˆë‹¤.")
    
    if st.button("Google Sheetsì—ì„œ ë°ì´í„° ë¡œë“œ", type="primary"):
        try:
            df_move, df_ref, df_incoming = load_from_gsheet_api()
            
            # ë°ì´í„°ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if df_move.empty or df_ref.empty:
                st.error("âŒ Google Sheets APIë¡œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë¹„ìŠ¤ ê³„ì • ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                st.stop()
            
            st.session_state["_data_source"] = "gsheet"

            moves_raw = normalize_moves(df_move)
            snap_long = normalize_refined_snapshot(df_ref)
            try:
                wip_df = load_wip_from_incoming(df_incoming)
                moves = merge_wip_as_moves(moves_raw, wip_df)
                st.success(f"âœ… Google Sheets ë¡œë“œ ì™„ë£Œ! WIP {len(wip_df)}ê±´ ë°˜ì˜" if wip_df is not None and not wip_df.empty else "âœ… Google Sheets ë¡œë“œ ì™„ë£Œ! WIP ì—†ìŒ")
            except Exception as e:
                moves = moves_raw
                st.warning(f"âš ï¸ WIP ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        except Exception as e:
            st.error(f"âŒ Google Sheets ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            st.info("ğŸ’¡ í•´ê²° ë°©ë²•:\n- ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸\n- ìŠ¤í”„ë ˆë“œì‹œíŠ¸ì— ì„œë¹„ìŠ¤ ê³„ì • ì´ë©”ì¼ì´ ê³µìœ ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n- ì‹œíŠ¸ëª…ì´ ì •í™•í•œì§€ í™•ì¸ (SCM_í†µí•©, snap_ì •ì œ)")

# ì´ˆê¸° ìë™ ë¡œë“œ(ì—†ì„ ë•Œë§Œ): Google Sheets API ì‹œë„ â†’ ì‹¤íŒ¨í•˜ë©´ ì•ˆë‚´
if "snap_long" not in locals():
    try:
        df_move, df_ref, df_incoming = load_from_gsheet_api()
        if not df_move.empty and not df_ref.empty:
            st.session_state["_data_source"] = "gsheet"
            moves = normalize_moves(df_move)
            snap_long = normalize_refined_snapshot(df_ref)
            try:
                wip_df = load_wip_from_incoming(df_incoming)
                moves = merge_wip_as_moves(moves, wip_df)
            except Exception:
                pass
            st.success("âœ… Google Sheetsì—ì„œ ë°ì´í„° ë¡œë“œë¨ (í•„ìš” ì‹œ ì—‘ì…€ ì—…ë¡œë“œ íƒ­ ì‚¬ìš© ê°€ëŠ¥)")
        else:
            st.info("ì—‘ì…€ ì—…ë¡œë“œ ë˜ëŠ” Google Sheetsì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ë©´ í•„í„°/ì°¨íŠ¸ê°€ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.")
            st.stop()
    except Exception:
        st.info("ì—‘ì…€ ì—…ë¡œë“œ ë˜ëŠ” Google Sheetsì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ë©´ í•„í„°/ì°¨íŠ¸ê°€ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.")
        st.stop()

# -------------------- Filters --------------------
centers_snap = set(snap_long["center"].dropna().astype(str).unique().tolist())
centers_moves = set(moves["from_center"].dropna().astype(str).unique().tolist() + 
                   moves["to_center"].dropna().astype(str).unique().tolist())

def normalize_center_name(center):
    return normalize_center_value(center)

all_centers = set()
for center in centers_snap | centers_moves:
    normalized = normalize_center_name(center)
    if normalized:
        all_centers.add(normalized)

centers = sorted(list(all_centers))
skus = sorted(snap_long["resource_code"].dropna().astype(str).unique().tolist())

today = pd.Timestamp.today().normalize()
PAST_DAYS = 42
FUTURE_DAYS = 60

snap_min = pd.to_datetime(snap_long["date"]).min().normalize()
snap_max = pd.to_datetime(snap_long["date"]).max().normalize()

bound_min = max(today - pd.Timedelta(days=PAST_DAYS), snap_min)
bound_max = min(today + pd.Timedelta(days=FUTURE_DAYS), snap_max + pd.Timedelta(days=60))

def _init_range():
    if "date_range" not in st.session_state:
        st.session_state.date_range = (max(today - pd.Timedelta(days=20), bound_min),
                                       min(today + pd.Timedelta(days=20), bound_max))
    if "horizon_days" not in st.session_state:
        st.session_state.horizon_days = 20

def _apply_horizon_to_range():
    h = int(st.session_state.horizon_days)
    h = max(0, min(h, FUTURE_DAYS))
    st.session_state.horizon_days = h
    start = max(today - pd.Timedelta(days=h), bound_min)
    end   = min(today + pd.Timedelta(days=h), bound_max)
    st.session_state.date_range = (start, end)

def _clamp_range(r):
    s, e = pd.Timestamp(r[0]).normalize(), pd.Timestamp(r[1]).normalize()
    s = max(min(s, bound_max), bound_min)
    e = max(min(e, bound_max), bound_min)
    if e < s:
        e = s
    return (s, e)

_init_range()

st.sidebar.header("í•„í„°")
centers_sel = st.sidebar.multiselect("ì„¼í„° ì„ íƒ", centers, default=(["íƒœê´‘KR"] if "íƒœê´‘KR" in centers else centers[:1]))
skus_sel = st.sidebar.multiselect("SKU ì„ íƒ", skus, default=([s for s in ["BA00022","BA00021"] if s in skus] or skus[:2]))

st.sidebar.subheader("ê¸°ê°„ ì„¤ì •")
st.sidebar.number_input("ë¯¸ë˜ ì „ë§ ì¼ìˆ˜", min_value=0, max_value=FUTURE_DAYS, step=1,
                        key="horizon_days", on_change=_apply_horizon_to_range)

date_range = st.sidebar.slider("ê¸°ê°„",
    min_value=bound_min.to_pydatetime(),
    max_value=bound_max.to_pydatetime(),
    value=tuple(d.to_pydatetime() for d in _clamp_range(st.session_state.date_range)),
    format="YYYY-MM-DD")

start_dt = pd.Timestamp(date_range[0]).normalize()
end_dt   = pd.Timestamp(date_range[1]).normalize()
_latest_snap = snap_long["date"].max()
proj_days_for_build = max(0, int((end_dt - _latest_snap).days))

st.sidebar.header("í‘œì‹œ ì˜µì…˜")
show_prod = st.sidebar.checkbox("ìƒì‚°ì¤‘(ë¯¸ì™„ë£Œ) í‘œì‹œ", value=True)
show_transit = st.sidebar.checkbox("ì´ë™ì¤‘ í‘œì‹œ", value=True)
use_cons_forecast = st.sidebar.checkbox("ì¶”ì„¸ ê¸°ë°˜ ì¬ê³  ì˜ˆì¸¡", value=True)
lookback_days = st.sidebar.number_input("ì¶”ì„¸ ê³„ì‚° ê¸°ê°„(ì¼)", min_value=7, max_value=56, value=28, step=7)

# ì…ê³  ë°˜ì˜ ê°€ì • ì˜µì…˜
st.sidebar.subheader("ì…ê³  ë°˜ì˜ ê°€ì •")
lag_days = st.sidebar.number_input("ì…ê³  ë°˜ì˜ ë¦¬ë“œíƒ€ì„(ì¼) â€“ inbound ë¯¸ê¸°ë¡ ì‹œ arrival+N", 
                                   min_value=0, max_value=21, value=7, step=1)

with st.sidebar.expander("í”„ë¡œëª¨ì…˜ ê°€ì¤‘ì¹˜(+%)", expanded=False):
    enable_event = st.checkbox("ê°€ì¤‘ì¹˜ ì ìš©", value=False)
    ev_start = st.date_input("ì‹œì‘ì¼")
    ev_end   = st.date_input("ì¢…ë£Œì¼")
    ev_pct   = st.number_input("ê°€ì¤‘ì¹˜(%)", min_value=-100.0, max_value=300.0, value=30.0, step=5.0)
events = [{"start": pd.Timestamp(ev_start).strftime("%Y-%m-%d"),
           "end":   pd.Timestamp(ev_end).strftime("%Y-%m-%d"),
           "uplift": ev_pct/100.0}] if enable_event else []

# -------------------- KPIs (SKUë³„ ë¶„í•´) --------------------
st.subheader("ìš”ì•½ KPI")

# ìŠ¤ëƒ…ìƒ· ë‚ ì§œ ì»¬ëŸ¼ ì´ë¦„ í˜¸í™˜('date' ë˜ëŠ” 'snapshot_date')
_snap_date_col = "date" if "date" in snap_long.columns else "snapshot_date"
_latest_dt = pd.to_datetime(snap_long[_snap_date_col]).max().normalize()
_latest_dt_str = _latest_dt.strftime("%Y-%m-%d")

# í’ˆëª… ë§¤í•‘(ì„ íƒ)
_name_col = None
for cand in ["resource_name", "ìƒí’ˆëª…", "í’ˆëª…"]:
    if cand in snap_long.columns:
        _name_col = cand
        break
_name_map = {}
if _name_col:
    name_rows = (snap_long[snap_long[_snap_date_col] == _latest_dt]
                    .dropna(subset=["resource_code"])[["resource_code", _name_col]]
                    .drop_duplicates())
    _name_map = dict(zip(name_rows["resource_code"].astype(str), name_rows[_name_col].astype(str)))

# moves ê°€ê³µ
_today = pd.Timestamp.today().normalize()
mv = moves.copy()
mv["carrier_mode"] = mv["carrier_mode"].astype(str).str.upper()
mv["resource_code"] = mv["resource_code"].astype(str)

def _kpi_breakdown_per_sku(snap_long, mv, centers_sel, skus_sel, today):
    # í˜„ì¬ ì¬ê³ (ìµœì‹  ìŠ¤ëƒ…ìƒ·, ì„ íƒ ì„¼í„° í•©ê³„)
    cur = (snap_long[
        (snap_long[_snap_date_col] == _latest_dt) &
        (snap_long["center"].isin(centers_sel)) &
        (snap_long["resource_code"].astype(str).isin(skus_sel))
    ].groupby("resource_code", as_index=True)["stock_qty"].sum())

    # ì´ë™ì¤‘: ì˜ˆì¸¡ ì¢…ë£Œì¼ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ëŠ˜ ì´í›„ê¹Œì§€ ì´ë™ì¤‘ì¸ ê±´ë§Œ
    mv_kpi = mv.copy()
    if not mv_kpi.empty:
        # ì˜ˆì¸¡ ì¢…ë£Œì¼ ê³„ì‚°
        pred_end = pd.Series(pd.NaT, index=mv_kpi.index, dtype="datetime64[ns]")
        
        # 1) inboundê°€ ìˆìœ¼ë©´ ê·¸ ë‚  ì¢…ë£Œ
        mask_inb = mv_kpi["inbound_date"].notna()
        pred_end.loc[mask_inb] = mv_kpi.loc[mask_inb, "inbound_date"]
        
        # 2) inbound ì—†ê³  arrival ìˆëŠ” ê²½ìš°
        mask_arr = (~mask_inb) & mv_kpi["arrival_date"].notna()
        if mask_arr.any():
            # 2-1) arrivalì´ ê³¼ê±°ë©´: arrival + Nì¼ì— ì¢…ë£Œ(ê°€ì •)
            past_arr = mask_arr & (mv_kpi["arrival_date"] <= today)
            pred_end.loc[past_arr] = (
                mv_kpi.loc[past_arr, "arrival_date"] + pd.Timedelta(days=int(lag_days))
            )
            
            # 2-2) arrivalì´ ë¯¸ë˜ë©´: arrivalì— ì¢…ë£Œ
            fut_arr = mask_arr & (mv_kpi["arrival_date"] > today)
            pred_end.loc[fut_arr] = mv_kpi.loc[fut_arr, "arrival_date"]
        
        # 3) ê·¸ë˜ë„ ë¹„ì–´ìˆìœ¼ë©´: today+1
        pred_end = pred_end.fillna(today + pd.Timedelta(days=1))
        mv_kpi["pred_end_date"] = pred_end
    
    it = (mv_kpi[
        (mv_kpi["carrier_mode"] != "WIP") &
        (mv_kpi["to_center"].isin(centers_sel)) &
        (mv_kpi["resource_code"].isin(skus_sel)) &
        (mv_kpi["onboard_date"].notna()) &
        (mv_kpi["onboard_date"] <= today) &
        (today < mv_kpi["pred_end_date"])  # ì˜¤ëŠ˜ ì´í›„ê¹Œì§€ ì´ë™ì¤‘ì¸ ê±´ë§Œ
    ].groupby("resource_code", as_index=True)["qty_ea"].sum())

    # WIP: SKUÃ—ë‚ ì§œë³„ (onboard +, event -) ëˆ„ì í•©ì„ ì˜¤ëŠ˜ê¹Œì§€ ê³„ì‚°í•œ ì”ëŸ‰
    w = mv[
        (mv["carrier_mode"] == "WIP") &
        (mv["to_center"].isin(centers_sel)) &
        (mv["resource_code"].isin(skus_sel))
    ].copy()
    if w.empty:
        wip = pd.Series(0, index=pd.Index(skus_sel, name="resource_code"))
    else:
        add = (w.dropna(subset=["onboard_date"])
                .set_index(["resource_code","onboard_date"])["qty_ea"])
        rem = (w.dropna(subset=["event_date"])
                .set_index(["resource_code","event_date"])["qty_ea"] * -1)
        flow = pd.concat([add, rem]).groupby(level=[0,1]).sum()
        flow = flow[flow.index.get_level_values(1) <= today]  # ì˜¤ëŠ˜ê¹Œì§€ë§Œ
        wip = (flow.groupby(level=0).cumsum()
                    .groupby(level=0).last()
                    .clip(lower=0))

    out = pd.DataFrame({
        "current": cur,
        "in_transit": it,
        "wip": wip
    }).reindex(skus_sel).fillna(0).astype(int)
    return out

kpi_df = _kpi_breakdown_per_sku(snap_long, mv, centers_sel, skus_sel, _today)

# SKUë³„ KPI ì¹´ë“œ ë Œë”ë§
def _chunks(lst, n):  # ì¤„ë°”ê¿ˆì„ ìœ„í•´ 2~4ê°œì”© ëŠì–´ì„œ ì¶œë ¥
    for i in range(0, len(lst), n):
        yield lst[i:i+n]

for group in _chunks(skus_sel, 2):  # í•œ ì¤„ì— 2ê°œì”© ë³´ê¸° ì¢‹ê²Œ
    cols = st.columns(len(group))
    for i, sku in enumerate(group):
        with cols[i].container(border=True):
            name = _name_map.get(sku, "")
            if name:
                st.markdown(f"**{name}**  \n`{sku}`")
            else:
                st.markdown(f"`{sku}`")
            c1, c2, c3 = st.columns(3)
            c1.metric("í˜„ì¬ ì¬ê³ ", f"{kpi_df.loc[sku,'current']:,}")
            c2.metric("ì´ë™ì¤‘", f"{kpi_df.loc[sku,'in_transit']:,}")
            c3.metric("ìƒì‚°ì¤‘", f"{kpi_df.loc[sku,'wip']:,}")

# (ì„ íƒ) ì „ì²´ í•©ê³„ë„ ê°™ì´ ë³´ê³  ì‹¶ìœ¼ë©´ ì•„ë˜ 4ì¤„ì„ í•´ì œ
# total = kpi_df.sum()
# t1, t2, t3 = st.columns(3)
# t1.metric("ì„ íƒ SKU í˜„ì¬ ì¬ê³  í•©ê³„", f"{total['current']:,}")
# t2.metric("ì„ íƒ SKU ì´ë™ì¤‘ í•©ê³„", f"{total['in_transit']:,}"); t3.metric("ì„ íƒ SKU ìƒì‚°ì¤‘ í•©ê³„", f"{total['wip']:,}")

st.divider()

# -------------------- Step Chart --------------------
st.subheader("ê³„ë‹¨ì‹ ì¬ê³  íë¦„")
timeline = build_timeline(snap_long, moves, centers_sel, skus_sel,
                          start_dt, end_dt, horizon_days=proj_days_for_build, today=today,
                          lag_days=int(lag_days))

if use_cons_forecast and not timeline.empty:
    timeline = apply_consumption_with_events(
        timeline, snap_long, centers_sel, skus_sel,
        start_dt, end_dt, lookback_days=int(lookback_days), events=events
    )

if timeline.empty:
    st.info("ì„ íƒ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” íƒ€ì„ë¼ì¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    vis_df = timeline[(timeline["date"] >= start_dt) & (timeline["date"] <= end_dt)].copy()
    vis_df["center"] = vis_df["center"].str.replace(r"^In-Transit.*$", "ì´ë™ì¤‘", regex=True)
    vis_df.loc[vis_df["center"] == "WIP", "center"] = "ìƒì‚°ì¤‘"
    if "íƒœê´‘KR" not in centers_sel:
        vis_df = vis_df[vis_df["center"] != "ìƒì‚°ì¤‘"]
    if not show_prod:
        vis_df = vis_df[vis_df["center"] != "ìƒì‚°ì¤‘"]
    if not show_transit:
        vis_df = vis_df[~vis_df["center"].str.startswith("ì´ë™ì¤‘")]
    
    # ì¬ê³ ëŸ‰ì´ 0ë³´ë‹¤ í° ë°ì´í„°ë§Œ í‘œì‹œ
    vis_df = vis_df[vis_df["stock_qty"] > 0]
    vis_df["label"] = vis_df["resource_code"] + " @ " + vis_df["center"]

    fig = px.line(vis_df, x="date", y="stock_qty", color="label", line_shape="hv",
                  title="ì„ íƒí•œ SKU Ã— ì„¼í„°(ë° ì´ë™ì¤‘/ìƒì‚°ì¤‘) ê³„ë‹¨ì‹ ì¬ê³  íë¦„", render_mode="svg")
    fig.update_layout(hovermode="x unified", xaxis_title="ë‚ ì§œ", yaxis_title="ì¬ê³ ëŸ‰(EA)",
                      legend_title_text="SKU @ Center / ì´ë™ì¤‘(ì ì„ ) / ìƒì‚°ì¤‘(ì ì„ )",
                      margin=dict(l=20, r=20, t=60, b=20))

    if start_dt <= today <= end_dt:
        fig.add_vline(x=today, line_width=1, line_dash="solid", line_color="rgba(255, 0, 0, 0.4)")
        fig.add_annotation(x=today, y=1.02, xref="x", yref="paper", text="ì˜¤ëŠ˜",
                           showarrow=False, font=dict(size=12, color="#555"), align="center", yanchor="bottom")

    fig.update_yaxes(tickformat=",.0f")
    fig.update_traces(hovertemplate="ë‚ ì§œ: %{x|%Y-%m-%d}<br>ì¬ê³ : %{y:,.0f} EA<br>%{fullData.name}<extra></extra>")

    # ì„  ìŠ¤íƒ€ì¼
    PALETTE = [
        "#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd",
        "#8c564b", "#e377c2", "#7f7f7f", "#bcbd22", "#17becf",
        "#aec7e8", "#ffbb78", "#98df8a", "#ff9896", "#c5b0d5",
        "#c49c94", "#f7b6d3", "#c7c7c7", "#dbdb8d", "#9edae5",
        "#4e79a7", "#f28e2b", "#e15759", "#76b7b2", "#59a14f",
        "#edc948", "#b07aa1", "#ff9da7", "#9c755f", "#bab0ac"
    ]
    line_colors = {}
    color_idx = 0
    for tr in fig.data:
        name = tr.name or ""
        if " @ " in name and name not in line_colors:
            line_colors[name] = PALETTE[color_idx % len(PALETTE)]
            color_idx += 1
    for i, tr in enumerate(fig.data):
        name = tr.name or ""
        if " @ " not in name:
            continue
        sku, kind = name.split(" @ ", 1)
        line_color = line_colors.get(name, PALETTE[0])
        if kind == "ì´ë™ì¤‘":
            fig.data[i].update(line=dict(color=line_color, dash="dot", width=1.2), opacity=0.9)
        elif kind == "ìƒì‚°ì¤‘":
            fig.data[i].update(line=dict(color=line_color, dash="dash", width=1.0), opacity=0.8)
        else:
            fig.data[i].update(line=dict(color=line_color, dash="solid", width=1.5), opacity=1.0)

    chart_key = (f"stepchart|centers={','.join(centers_sel)}|skus={','.join(skus_sel)}|"
                 f"{start_dt:%Y%m%d}-{end_dt:%Y%m%d}|h{int(st.session_state.horizon_days)}|"
                 f"prod{int(show_prod)}|tran{int(show_transit)}")
    st.plotly_chart(fig, use_container_width=True, config={"displaylogo": False}, key=chart_key)

# -------------------- Upcoming Arrivals (fixed) --------------------
st.subheader("ì…ê³  ì˜ˆì • ë‚´ì—­ (ì„ íƒ ì„¼í„°/SKU)")
window_start = start_dt
window_end   = end_dt

# 1) ìš´ì†¡(ë¹„ WIP) â€” ì•„ì§ ì…ê³ ì™„ë£Œë˜ì§€ ì•Šì€ ê±´ë§Œ
arr_transport = mv[
    (mv["carrier_mode"] != "WIP") &
    (mv["to_center"].isin(centers_sel)) &
    (mv["resource_code"].isin(skus_sel)) &
    (mv["inbound_date"].isna())                    # âœ… ì…ê³ ì™„ë£Œ ì œì™¸
].copy()

# ë„ì°©(ì˜ˆì •)ì¼: arrival_date(= ETA/ë„ì°©ì¼) ìš°ì„ , ì—†ìœ¼ë©´ onboard_date ë³´ì¡°
arr_transport["display_date"] = arr_transport["arrival_date"].fillna(arr_transport["onboard_date"])
arr_transport = arr_transport[arr_transport["display_date"].notna()]
arr_transport = arr_transport[
    (arr_transport["display_date"] >= window_start) &
    (arr_transport["display_date"] <= window_end)
]

# 2) WIP â€” íƒœê´‘KRì¼ ë•Œë§Œ, wip_ready(event_date) ê¸°ì¤€
arr_wip = pd.DataFrame()
if "íƒœê´‘KR" in centers_sel:
    arr_wip = mv[
        (mv["carrier_mode"] == "WIP") &
        (mv["to_center"] == "íƒœê´‘KR") &
        (mv["resource_code"].isin(skus_sel)) &
        (mv["event_date"].notna()) &
        (mv["event_date"] >= window_start) &
        (mv["event_date"] <= window_end)
    ].copy()
    arr_wip["display_date"] = arr_wip["event_date"]

# 3) ë³‘í•© + í‘œ ë Œë”
upcoming = pd.concat([arr_transport, arr_wip], ignore_index=True)

# ì˜ˆìƒ ì…ê³ ì¼ ì¶”ê°€
mv_view = mv.copy()
if not mv_view.empty:
    # ì˜ˆì¸¡ ì…ê³ ì¼ ê³„ì‚°
    pred_inbound = pd.Series(pd.NaT, index=mv_view.index, dtype="datetime64[ns]")
    
    # 1) inboundê°€ ìˆìœ¼ë©´ ê·¸ ë‚  ì…ê³ 
    mask_inb = mv_view["inbound_date"].notna()
    pred_inbound.loc[mask_inb] = mv_view.loc[mask_inb, "inbound_date"]
    
    # 2) inbound ì—†ê³  arrival ìˆëŠ” ê²½ìš°
    mask_arr = (~mask_inb) & mv_view["arrival_date"].notna()
    if mask_arr.any():
        # 2-1) arrivalì´ ê³¼ê±°ë©´: arrival + Nì¼ì— ì…ê³ (ê°€ì •)
        past_arr = mask_arr & (mv_view["arrival_date"] <= today)
        pred_inbound.loc[past_arr] = (
            mv_view.loc[past_arr, "arrival_date"] + pd.Timedelta(days=int(lag_days))
        )
        
        # 2-2) arrivalì´ ë¯¸ë˜ë©´: arrivalì— ì…ê³ 
        fut_arr = mask_arr & (mv_view["arrival_date"] > today)
        pred_inbound.loc[fut_arr] = mv_view.loc[fut_arr, "arrival_date"]
    
    mv_view["pred_inbound_date"] = pred_inbound

upcoming = upcoming.merge(
    mv_view[["resource_code","onboard_date","pred_inbound_date"]],
    on=["resource_code","onboard_date"], how="left"
)

# í’ˆëª… ë¶™ì´ê¸° (ìˆì„ ë•Œë§Œ)
if _name_map:
    upcoming["resource_name"] = upcoming["resource_code"].map(_name_map).fillna("")

if upcoming.empty:
    st.caption("ë„ì°© ì˜ˆì • ì—†ìŒ (ì˜¤ëŠ˜ ì´í›„ / ì„ íƒ ê¸°ê°„)")
else:
    upcoming["days_to_arrival"] = (upcoming["display_date"].dt.normalize() - today).dt.days
    upcoming["days_to_inbound"] = (upcoming["pred_inbound_date"].dt.normalize() - today).dt.days
    upcoming = upcoming.sort_values(["display_date","to_center","resource_code","qty_ea"],
                                    ascending=[True, True, True, False])
    cols = ["display_date","days_to_arrival","to_center","resource_code","resource_name","qty_ea",
            "carrier_mode","onboard_date","pred_inbound_date","days_to_inbound","lot"]
    cols = [c for c in cols if c in upcoming.columns]
    st.dataframe(upcoming[cols].head(1000), use_container_width=True, height=300)
    st.caption("â€» days_to_arrivalê°€ ìŒìˆ˜(â€“)ë¡œ ë³´ì´ë©´: í™”ë¬¼ì€ 'ë„ì°©'í–ˆìœ¼ë‚˜ ì¸ë°”ìš´ë“œ(ì…ê³ ì™„ë£Œ) ë“±ë¡ ì „ ìƒíƒœì…ë‹ˆë‹¤.")
    st.caption("â€» pred_inbound_date: ì˜ˆìƒ ì…ê³ ì¼ (ë„ì°©ì¼ + ë¦¬ë“œíƒ€ì„), days_to_inbound: ì˜ˆìƒ ì…ê³ ê¹Œì§€ ë‚¨ì€ ì¼ìˆ˜")

# -------------------- ì„ íƒ ì„¼í„° í˜„ì¬ ì¬ê³  (ì „ì²´ SKU) --------------------
st.subheader(f"ì„ íƒ ì„¼í„° í˜„ì¬ ì¬ê³  (ìŠ¤ëƒ…ìƒ· {_latest_dt_str} / ì „ì²´ SKU)")

cur = snap_long[(snap_long["date"] == _latest_dt) & (snap_long["center"].isin(centers_sel))].copy()
pivot = (cur.groupby(["resource_code","center"], as_index=False)["stock_qty"].sum()
           .pivot(index="resource_code", columns="center", values="stock_qty").fillna(0).astype(int))
pivot["ì´í•©"] = pivot.sum(axis=1)

col1, col2 = st.columns([2,1])
with col1:
    q = st.text_input(
        "SKU í•„í„° â€” í’ˆëª©ë²ˆí˜¸ ê²€ìƒ‰ ì‹œ í•´ë‹¹ SKUì˜ ì„¼í„°ë³„ ì œì¡°ë²ˆí˜¸(LOT) í™•ì¸",
        "",
        key="sku_filter_text"
    )
with col2:
    sort_by = st.selectbox("ì •ë ¬ ê¸°ì¤€", ["ì´í•©"] + list(pivot.columns.drop("ì´í•©")), index=0)

col1, col2 = st.columns(2)
with col1:
    hide_zero = st.checkbox("ì´í•©=0 ìˆ¨ê¸°ê¸°", value=True)
with col2:
    show_cost = st.checkbox("ì¬ê³ ìì‚°(ì œì¡°ì›ê°€) í‘œì‹œ", value=False)

view = pivot.copy()
if q.strip():
    view = view[view.index.str.contains(q.strip(), case=False, regex=False)]
if hide_zero:
    view = view[view["ì´í•©"] > 0]
view = view.sort_values(by=sort_by, ascending=False)

base_df = view.reset_index().rename(columns={"resource_code":"SKU"})
if _name_map:
    base_df.insert(1, "í’ˆëª…", base_df["SKU"].map(_name_map).fillna(""))

if show_cost:
    snap_raw_df = load_snapshot_raw()
    cost_pivot = pivot_inventory_cost_from_raw(snap_raw_df, _latest_dt, centers_sel)
    if cost_pivot.empty:
        st.warning("ì¬ê³ ìì‚° ê³„ì‚°ì„ ìœ„í•œ 'snapshot_raw' ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ì–´ ìˆ˜ëŸ‰ë§Œ í‘œì‹œí•©ë‹ˆë‹¤. (ì—‘ì…€ì— 'snapshot_raw' ì‹œíŠ¸ê°€ ìˆìœ¼ë©´ ìë™ ì‚¬ìš©ë©ë‹ˆë‹¤)")
        show_df = base_df
    else:
        merged = base_df.merge(cost_pivot.rename(columns={"resource_code":"SKU"}), on="SKU", how="left")
        cost_cols2 = [c for c in merged.columns if c.endswith("_ì¬ê³ ìì‚°")] + (["ì´ ì¬ê³ ìì‚°"] if "ì´ ì¬ê³ ìì‚°" in merged.columns else [])
        if cost_cols2:
            merged[cost_cols2] = merged[cost_cols2].fillna(0).astype(int)
            for col in cost_cols2:
                merged[col] = merged[col].apply(lambda x: f"{x:,}ì›")
        qty_center_cols = [c for c in merged.columns if c not in ["SKU","í’ˆëª…","ì´í•©"] + cost_cols2]
        ordered = ["SKU"] + (["í’ˆëª…"] if "í’ˆëª…" in merged.columns else []) + qty_center_cols + (["ì´í•©"] if "ì´í•©" in merged.columns else []) + cost_cols2
        show_df = merged[ordered]
else:
    show_df = base_df

# ìˆ˜ëŸ‰ í¬ë§·íŒ…
qty_cols = [c for c in show_df.columns if c not in ["SKU"] and not c.endswith("_ì¬ê³ ìì‚°") and c != "ì´ ì¬ê³ ìì‚°"]
for col in qty_cols:
    if col in show_df.columns:
        show_df[col] = show_df[col].apply(lambda x: f"{x:,}" if pd.notna(x) and isinstance(x, (int, float)) else x)

st.dataframe(show_df, use_container_width=True, height=380)

csv_bytes = show_df.to_csv(index=False).encode("utf-8-sig")
st.download_button("í˜„ì¬ í‘œ CSV ë‹¤ìš´ë¡œë“œ", data=csv_bytes,
                   file_name=f"centers_{'-'.join(centers_sel)}_snapshot_{_latest_dt_str}.csv", mime="text/csv")

st.caption("â€» ì´ í‘œëŠ” **ì„ íƒëœ ì„¼í„° ì „ì²´ SKU**ì˜ ìµœì‹  ìŠ¤ëƒ…ìƒ· ì¬ê³ ì…ë‹ˆë‹¤. ìƒë‹¨ 'SKU ì„ íƒ'ê³¼ ë¬´ê´€í•˜ê²Œ ëª¨ë“  SKUê°€ í¬í•¨ë©ë‹ˆë‹¤.")

# === ë¡œíŠ¸ ìƒì„¸: SKUê°€ 1ê°œì¼ ë•Œ ìë™ í‘œì‹œ ===
filtered_df = show_df if "SKU" in show_df.columns else view.reset_index().rename(columns={"resource_code":"SKU"})
visible_skus = filtered_df["SKU"].dropna().astype(str).unique().tolist()

if len(visible_skus) == 1:
    lot_sku = visible_skus[0]
    snap_raw_df = load_snapshot_raw()
    # lot ìƒì„¸ í…Œì´ë¸” ë§Œë“¤ê¸°
    if snap_raw_df is None or snap_raw_df.empty:
        # _latest_dt_strë¥¼ ì—¬ê¸°ì„œ ê³„ì‚°
        latest_dt_str = pd.to_datetime(snap_long["date"]).max().strftime("%Y-%m-%d")
        st.markdown(f"### ë¡œíŠ¸ ìƒì„¸ (ìŠ¤ëƒ…ìƒ· {latest_dt_str} / **{', '.join(centers_sel)}** / **{lot_sku}**)")
        st.caption("í•´ë‹¹ ì¡°ê±´ì˜ ë¡œíŠ¸ ìƒì„¸ê°€ ì—†ìŠµë‹ˆë‹¤. (snapshot_raw ì—†ìŒ)")
    else:
        sr = snap_raw_df.copy()
        cols_map = {c.strip().lower(): c for c in sr.columns}
        col_date = cols_map.get("snapshot_date") or cols_map.get("date")
        col_sku  = cols_map.get("resource_code") or cols_map.get("sku") or cols_map.get("ìƒí’ˆì½”ë“œ")
        col_lot  = cols_map.get("lot")
        used_centers = [ct for ct in centers_sel if CENTER_COL.get(ct) in sr.columns]
        if not all([col_date, col_sku, col_lot]) or not used_centers:
            st.caption("í•´ë‹¹ ì¡°ê±´ì˜ ë¡œíŠ¸ ìƒì„¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            sr[col_date] = pd.to_datetime(sr[col_date], errors="coerce")
            sub = sr[(sr[col_date].dt.normalize()==_latest_dt.normalize()) & (sr[col_sku].astype(str)==str(lot_sku))].copy()
            if sub.empty:
                st.caption("í•´ë‹¹ ì¡°ê±´ì˜ ë¡œíŠ¸ ìƒì„¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                for ct in used_centers:
                    c = CENTER_COL[ct]
                    sub[c] = pd.to_numeric(sub[c], errors="coerce").fillna(0).clip(lower=0)
                out = pd.DataFrame({"lot": sub[col_lot].astype(str).fillna("(no lot)")})
                for ct in used_centers:
                    out[ct] = sub.groupby(col_lot)[CENTER_COL[ct]].transform("sum")
                out = out.drop_duplicates()
                out["í•©ê³„"] = out[used_centers].sum(axis=1)
                out = out[out["í•©ê³„"] > 0]
                # _latest_dt_strë¥¼ ì—¬ê¸°ì„œ ê³„ì‚°
                latest_dt_str = pd.to_datetime(snap_long["date"]).max().strftime("%Y-%m-%d")
                st.markdown(f"### ë¡œíŠ¸ ìƒì„¸ (ìŠ¤ëƒ…ìƒ· {latest_dt_str} / **{', '.join(centers_sel)}** / **{lot_sku}**)")
                if out.empty:
                    st.caption("í•´ë‹¹ ì¡°ê±´ì˜ ë¡œíŠ¸ ìƒì„¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.dataframe(out[["lot"] + used_centers + ["í•©ê³„"]].sort_values("í•©ê³„", ascending=False).reset_index(drop=True),
                                 use_container_width=True, height=320)
