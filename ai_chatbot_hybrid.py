"""
AI ì±—ë´‡ 1.5ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ (ì •ëŸ‰ ê³„ì‚° + ë²¡í„° ê²€ìƒ‰)

í•µì‹¬ ê°œì„ :
- ì§ˆë¬¸ ë¶„ë¥˜: ì •ëŸ‰í˜• vs íƒìƒ‰í˜• ìë™ êµ¬ë¶„
- ì •ëŸ‰ ê³„ì‚°: íŒë‹¤ìŠ¤ë¡œ ì •í™•í•œ ìˆ«ì ê³„ì‚° (í•©ê³„, í‰ê· , ìµœëŒ€/ìµœì†Œ ë“±)
- íƒìƒ‰ ê²€ìƒ‰: ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
- AI ì—­í• : ê³„ì‚° ê²°ê³¼ ì„¤ëª… + ê·¼ê±° ìš”ì•½ (í™˜ê° ë°©ì§€)
"""

import streamlit as st
import pandas as pd
import chromadb
import google.generativeai as genai
import uuid
import hashlib
from typing import Tuple, List, Dict, Optional, Literal
import re
import numpy as np


# ============================================================================
# 1. ì§ˆë¬¸ ë¶„ë¥˜ ì—”ì§„
# ============================================================================

def classify_question(question: str) -> Literal["quantitative", "exploratory", "business"]:
    """
    ì§ˆë¬¸ì„ 3ê°€ì§€ ìœ í˜•ìœ¼ë¡œ ë¶„ë¥˜

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸

    Returns:
        - "quantitative": ì •ëŸ‰ ê³„ì‚° í•„ìš” (í•©ê³„, í‰ê· , ê°œìˆ˜ ë“±)
        - "exploratory": íƒìƒ‰ ê²€ìƒ‰ í•„ìš” (ì–´ë””ì—, ë¬´ì—‡ì´, ì–¸ì œ ë“±)
        - "business": ë¹„ì¦ˆë‹ˆìŠ¤ íŒë‹¨ í•„ìš” (ë¶€ì¡±, ì¶©ë¶„, ê¶Œê³  ë“±) - í–¥í›„ 2ë‹¨ê³„
    """
    q = question.lower()

    # ì •ëŸ‰í˜• í‚¤ì›Œë“œ (íŒë‹¤ìŠ¤ ê³„ì‚° í•„ìš”)
    quantitative_keywords = [
        # ì§‘ê³„ í•¨ìˆ˜
        "ì´", "ì „ì²´", "í•©ê³„", "ì´í•©", "ëª¨ë‘", "all", "total", "sum",
        "í‰ê· ", "average", "avg", "mean",
        "ìµœëŒ€", "ìµœì†Œ", "ê°€ì¥ ë§", "ê°€ì¥ ì ", "max", "min", "maximum", "minimum",
        "ê°œìˆ˜", "ëª‡ ê°œ", "ëª‡ê°œ", "count", "number of",

        # ì„¼í„°ë³„/SKUë³„ ì§‘ê³„
        "ì„¼í„°ë³„", "SKUë³„", "ì œí’ˆë³„", "ì°½ê³ ë³„",
        "ê° ì„¼í„°", "ê° SKU", "ê°ê°",

        # ë¹„êµ
        "ë§ì€", "ì ì€", "ë†’ì€", "ë‚®ì€",

        # ìœ„ì¹˜ ì§ˆë¬¸ (íŠ¹ì • ì„¼í„°/SKU ì°¾ê¸°)
        "ì–´ëŠ ì„¼í„°", "ì–´ë””ì—", "ì–´ë”” ìˆ", "ì–´ëŠ ì°½ê³ ", "ì–´ë–¤ ì„¼í„°",
        "where", "which center", "which warehouse",
    ]

    # ë¹„ì¦ˆë‹ˆìŠ¤í˜• í‚¤ì›Œë“œ (í–¥í›„ 2ë‹¨ê³„)
    business_keywords = [
        "ë¶€ì¡±", "ì¶©ë¶„", "ê¶Œê³ ", "ê¶Œì¥", "ìœ„í—˜", "ê²½ê³ ",
        "ì•ˆì „ì¬ê³ ", "ë¦¬ë“œíƒ€ì„", "ì…ê³  í•„ìš”", "ë°œì£¼",
        "insufficient", "enough", "recommend", "risk",
    ]

    # íƒìƒ‰í˜• í‚¤ì›Œë“œ (ë²¡í„° ê²€ìƒ‰ í•„ìš”)
    exploratory_keywords = [
        "ë¬´ì—‡", "ì–¸ì œ", "ì–´ë–¤",
        "what", "when",
        "ëª©ë¡", "ë¦¬ìŠ¤íŠ¸", "list",
        "ë³´ìœ ", "ì¡´ì¬",
    ]

    # í‚¤ì›Œë“œ ë§¤ì¹­ (ìˆœì„œ ì¤‘ìš”: ì •ëŸ‰í˜•ì„ ë¨¼ì € ì²´í¬)
    if any(kw in q for kw in quantitative_keywords):
        return "quantitative"
    elif any(kw in q for kw in business_keywords):
        return "business"
    elif any(kw in q for kw in exploratory_keywords):
        return "exploratory"
    else:
        # ê¸°ë³¸ê°’: ì •ëŸ‰í˜• (ê°„ë‹¨í•œ í•„í„°ë§ì´ ë” ì •í™•)
        return "quantitative"


def extract_entities(question: str, available_centers: List[str], available_skus: List[str]) -> Dict[str, List[str]]:
    """
    ì§ˆë¬¸ì—ì„œ ì„¼í„°/SKU ê°œì²´ëª… ì¶”ì¶œ

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        available_centers: í˜„ì¬ í•„í„°ì˜ ì„¼í„° ëª©ë¡
        available_skus: í˜„ì¬ í•„í„°ì˜ SKU ëª©ë¡

    Returns:
        {"centers": [...], "skus": [...]}
    """
    q = question.upper()

    # ì„¼í„° ì¶”ì¶œ
    centers = [c for c in available_centers if c.upper() in q]

    # SKU ì¶”ì¶œ
    skus = [s for s in available_skus if s.upper() in q]

    return {
        "centers": centers if centers else available_centers,  # ì—†ìœ¼ë©´ ì „ì²´
        "skus": skus if skus else available_skus,
    }


# ============================================================================
# 2. ì •ëŸ‰ ê³„ì‚° ì—”ì§„ (íŒë‹¤ìŠ¤)
# ============================================================================

def calculate_quantitative(
    question: str,
    snapshot_df: pd.DataFrame,
    entities: Dict[str, List[str]]
) -> Dict[str, any]:
    """
    íŒë‹¤ìŠ¤ë¡œ ì •í™•í•œ ì •ëŸ‰ ê³„ì‚° ìˆ˜í–‰

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        snapshot_df: í•„í„°ë§ëœ ìŠ¤ëƒ…ìƒ· ë°ì´í„°
        entities: ì¶”ì¶œëœ ì„¼í„°/SKU ì •ë³´

    Returns:
        {
            "type": "total" | "average" | "max" | "min" | "count" | "group_by",
            "result": ê³„ì‚° ê²°ê³¼ (ìˆ«ì ë˜ëŠ” DataFrame),
            "description": ê²°ê³¼ ì„¤ëª… í…ìŠ¤íŠ¸
        }
    """
    q = question.lower()

    # ë°ì´í„° í•„í„°ë§
    df = snapshot_df.copy()
    if "center" in df.columns:
        df = df[df["center"].isin(entities["centers"])]
    if "resource_code" in df.columns:
        df = df[df["resource_code"].isin(entities["skus"])]

    # ìµœì‹  ìŠ¤ëƒ…ìƒ·ë§Œ ì‚¬ìš© (ì„¼í„°-SKUë³„ ìµœì‹  ë‚ ì§œ)
    if "date" in df.columns and not df.empty:
        df["date"] = pd.to_datetime(df["date"], errors="coerce")
        df = df.sort_values("date").groupby(["center", "resource_code"], as_index=False).last()

    if df.empty:
        return {
            "type": "empty",
            "result": None,
            "description": "í•´ë‹¹ ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        }

    # ì¬ê³  ìˆ˜ëŸ‰ ìˆ«ì ë³€í™˜
    df["stock_qty"] = pd.to_numeric(df.get("stock_qty", 0), errors="coerce").fillna(0)

    # 0. ìœ„ì¹˜ ì°¾ê¸° (ì–´ëŠ ì„¼í„°ì— ìˆë‚˜ìš”?)
    if any(kw in q for kw in ["ì–´ë””", "ì–´ëŠ", "where", "which"]):
        # SKUê°€ ì§ˆë¬¸ì— ëª…ì‹œë˜ì–´ ìˆëŠ” ê²½ìš°
        if len(entities["skus"]) < len(snapshot_df["resource_code"].unique()):
            # íŠ¹ì • SKUë¥¼ ì°¾ëŠ” ê²½ìš°
            centers_with_stock = df[df["stock_qty"] > 0].groupby("center")["stock_qty"].sum().sort_values(ascending=False)

            if centers_with_stock.empty:
                return {
                    "type": "location",
                    "result": [],
                    "description": f"**{', '.join(entities['skus'])}**ëŠ” í˜„ì¬ ì–´ëŠ ì„¼í„°ì—ë„ ì¬ê³ ê°€ ì—†ìŠµë‹ˆë‹¤."
                }

            lines = [f"- **{center}**: {qty:,.0f}ê°œ" for center, qty in centers_with_stock.items()]
            sku_names = ', '.join(entities['skus'][:3])
            if len(entities['skus']) > 3:
                sku_names += f" ì™¸ {len(entities['skus']) - 3}ê°œ"

            return {
                "type": "location",
                "result": centers_with_stock.to_dict(),
                "description": f"**{sku_names}**ëŠ” ë‹¤ìŒ ì„¼í„°ì— ìˆìŠµë‹ˆë‹¤:\n" + "\n".join(lines)
            }

        # ì„¼í„°ê°€ ì§ˆë¬¸ì— ëª…ì‹œë˜ì–´ ìˆëŠ” ê²½ìš° (ì–´ë–¤ SKUê°€ ìˆë‚˜ìš”?)
        elif len(entities["centers"]) < len(snapshot_df["center"].unique()):
            skus_with_stock = df[df["stock_qty"] > 0].groupby("resource_code")["stock_qty"].sum().sort_values(ascending=False)

            if skus_with_stock.empty:
                return {
                    "type": "location",
                    "result": [],
                    "description": f"**{', '.join(entities['centers'])}**ì—ëŠ” í˜„ì¬ ì¬ê³ ê°€ ì—†ìŠµë‹ˆë‹¤."
                }

            lines = [f"- **{sku}**: {qty:,.0f}ê°œ" for sku, qty in skus_with_stock.head(10).items()]
            if len(skus_with_stock) > 10:
                lines.append(f"... ì™¸ {len(skus_with_stock) - 10}ê°œ SKU")

            return {
                "type": "location",
                "result": skus_with_stock.to_dict(),
                "description": f"**{', '.join(entities['centers'])}**ì—ëŠ” ë‹¤ìŒ SKUê°€ ìˆìŠµë‹ˆë‹¤:\n" + "\n".join(lines)
            }

    # 1. ì´ ì¬ê³  / í•©ê³„
    if any(kw in q for kw in ["ì´", "ì „ì²´", "í•©ê³„", "ì´í•©", "total", "sum"]):
        total = df["stock_qty"].sum()
        centers_str = ", ".join(entities["centers"][:3])
        if len(entities["centers"]) > 3:
            centers_str += f" ì™¸ {len(entities["centers"]) - 3}ê³³"

        return {
            "type": "total",
            "result": total,
            "description": f"{centers_str}ì˜ ì´ ì¬ê³ ëŠ” **{total:,.0f}ê°œ**ì…ë‹ˆë‹¤.",
            "breakdown": df.groupby("center")["stock_qty"].sum().to_dict()
        }

    # 2. í‰ê· 
    if any(kw in q for kw in ["í‰ê· ", "average", "avg", "mean"]):
        avg = df["stock_qty"].mean()
        return {
            "type": "average",
            "result": avg,
            "description": f"í‰ê·  ì¬ê³ ëŠ” **{avg:,.0f}ê°œ**ì…ë‹ˆë‹¤.",
        }

    # 3. ìµœëŒ€ / ìµœì†Œ
    if any(kw in q for kw in ["ìµœëŒ€", "ê°€ì¥ ë§", "max", "maximum"]):
        max_row = df.loc[df["stock_qty"].idxmax()]
        return {
            "type": "max",
            "result": max_row["stock_qty"],
            "description": f"ì¬ê³ ê°€ ê°€ì¥ ë§ì€ ê²ƒì€ **{max_row['center']} {max_row['resource_code']}** ({max_row['stock_qty']:,.0f}ê°œ)ì…ë‹ˆë‹¤.",
        }

    if any(kw in q for kw in ["ìµœì†Œ", "ê°€ì¥ ì ", "min", "minimum"]):
        min_row = df.loc[df["stock_qty"].idxmin()]
        return {
            "type": "min",
            "result": min_row["stock_qty"],
            "description": f"ì¬ê³ ê°€ ê°€ì¥ ì ì€ ê²ƒì€ **{min_row['center']} {min_row['resource_code']}** ({min_row['stock_qty']:,.0f}ê°œ)ì…ë‹ˆë‹¤.",
        }

    # 4. ì„¼í„°ë³„ / SKUë³„ ì§‘ê³„
    if any(kw in q for kw in ["ì„¼í„°ë³„", "ê° ì„¼í„°", "ì°½ê³ ë³„"]):
        by_center = df.groupby("center")["stock_qty"].sum().sort_values(ascending=False)
        lines = [f"- **{center}**: {qty:,.0f}ê°œ" for center, qty in by_center.items()]
        return {
            "type": "group_by_center",
            "result": by_center.to_dict(),
            "description": "ì„¼í„°ë³„ ì¬ê³ :\n" + "\n".join(lines)
        }

    if any(kw in q for kw in ["SKUë³„", "ì œí’ˆë³„", "ê° SKU"]):
        by_sku = df.groupby("resource_code")["stock_qty"].sum().sort_values(ascending=False)
        lines = [f"- **{sku}**: {qty:,.0f}ê°œ" for sku, qty in by_sku.head(10).items()]
        if len(by_sku) > 10:
            lines.append(f"... ì™¸ {len(by_sku) - 10}ê°œ SKU")
        return {
            "type": "group_by_sku",
            "result": by_sku.to_dict(),
            "description": "SKUë³„ ì¬ê³  (ìƒìœ„ 10ê°œ):\n" + "\n".join(lines)
        }

    # 5. ê°œìˆ˜
    if any(kw in q for kw in ["ê°œìˆ˜", "ëª‡ ê°œ", "ëª‡ê°œ", "count"]):
        if "ì„¼í„°" in q:
            count = df["center"].nunique()
            return {
                "type": "count",
                "result": count,
                "description": f"ì„¼í„° ê°œìˆ˜ëŠ” **{count}ê°œ**ì…ë‹ˆë‹¤.",
            }
        elif "SKU" in q or "ì œí’ˆ" in q:
            count = df["resource_code"].nunique()
            return {
                "type": "count",
                "result": count,
                "description": f"SKU ê°œìˆ˜ëŠ” **{count}ê°œ**ì…ë‹ˆë‹¤.",
            }

    # ê¸°ë³¸: ì´ ì¬ê³  ë°˜í™˜
    total = df["stock_qty"].sum()
    return {
        "type": "total",
        "result": total,
        "description": f"ì¡°íšŒëœ ì¬ê³ ëŠ” **{total:,.0f}ê°œ**ì…ë‹ˆë‹¤.",
    }


# ============================================================================
# 3. ë²¡í„° ê²€ìƒ‰ ì—”ì§„ (ê¸°ì¡´ ë¡œì§)
# ============================================================================

@st.cache_resource
def _chroma_cloud():
    """Chroma Cloud í´ë¼ì´ì–¸íŠ¸"""
    try:
        client = chromadb.CloudClient(
            api_key=st.secrets["chroma"]["api_key"],
            tenant=st.secrets["chroma"]["tenant"],
            database=st.secrets["chroma"]["database"],
        )
        return client
    except Exception as e:
        st.error(f"Chroma Cloud ì—°ê²° ì‹¤íŒ¨: {e}")
        return None


def _get_filter_hash(centers: list, skus: list) -> str:
    """í•„í„° ì¡°í•© í•´ì‹œ"""
    key = f"{sorted(centers)}_{sorted(skus)}"
    return hashlib.md5(key.encode()).hexdigest()[:8]


def _build_session_collection_name() -> str:
    """ì„¸ì…˜ ì»¬ë ‰ì…˜ëª… (v3: ì™„ì „íˆ ìƒˆë¡œìš´ ì»¬ë ‰ì…˜)"""
    sid = st.session_state.get("_ai_session_id")
    if not sid:
        sid = st.session_state["_ai_session_id"] = uuid.uuid4().hex[:8]
    # v3: ì™„ì „íˆ ìƒˆë¡œìš´ ë„¤ì´ë°ìœ¼ë¡œ ê¹¨ë—í•˜ê²Œ ì‹œì‘
    return f"scm_v3_{sid}"


def _normalize_embeddings(embeddings: List[List[float]]) -> List[List[float]]:
    """
    ì„ë² ë”©ì„ L2 ì •ê·œí™” (ë‹¨ìœ„ ë²¡í„°ë¡œ ë³€í™˜)

    ì •ê·œí™”ëœ ì„ë² ë”©ì—ì„œëŠ” L2 ê±°ë¦¬ì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ìˆ˜í•™ì ìœ¼ë¡œ ë™ë“±:
    - L2 distanceÂ² = 2 - 2*cosine_similarity
    - ë”°ë¼ì„œ L2 ê±°ë¦¬ ìµœì†Œí™” = ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìµœëŒ€í™”

    ì´ë ‡ê²Œ í•˜ë©´ Chroma Cloudì˜ ê¸°ë³¸ ë©”íŠ¸ë¦­(L2)ì„ ì‚¬ìš©í•˜ë©´ì„œë„
    ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ ë™ì¼í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    normalized = []
    for emb in embeddings:
        emb_array = np.array(emb)
        norm = np.linalg.norm(emb_array)
        if norm > 0:
            normalized.append((emb_array / norm).tolist())
        else:
            # ì œë¡œ ë²¡í„°ëŠ” ê·¸ëŒ€ë¡œ (ë°œìƒí•˜ì§€ ì•Šì•„ì•¼ í•¨)
            normalized.append(emb)
    return normalized


def _embed_batch(texts: list[str], batch_size: int = 100) -> Tuple[list[list[float]], list[int]]:
    """
    Gemini ì„ë² ë”© (ë°°ì¹˜ ì²˜ë¦¬) + L2 ì •ê·œí™”

    Returns:
        (embeddings, failed_indices): ì •ê·œí™”ëœ ì„ë² ë”© ë¦¬ìŠ¤íŠ¸ì™€ ì‹¤íŒ¨í•œ ë¬¸ì„œ ì¸ë±ìŠ¤
    """
    if not texts:
        return [], []

    genai.configure(api_key=st.secrets["gemini"]["api_key"])
    model = st.secrets["gemini"].get("embedding_model", "text-embedding-004")

    all_embeddings = []
    failed_indices = []

    for batch_start in range(0, len(texts), batch_size):
        batch_end = min(batch_start + batch_size, len(texts))
        batch = texts[batch_start:batch_end]

        try:
            res = genai.embed_content(
                model=model,
                content=batch,
                task_type="retrieval_document"
            )

            # ì„ë² ë”© ì¶”ì¶œ
            if hasattr(res, "embeddings"):
                batch_embeddings = [e.values for e in res.embeddings]
            elif isinstance(res, dict) and "embeddings" in res:
                batch_embeddings = [r["embedding"] for r in res["embeddings"]]
            else:
                batch_embeddings = [res["embedding"]]

            # L2 ì •ê·œí™”: L2 ê±°ë¦¬ë¥¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ ë™ë“±í•˜ê²Œ ë§Œë“¦
            batch_embeddings = _normalize_embeddings(batch_embeddings)

            all_embeddings.extend(batch_embeddings)

        except Exception as e:
            st.warning(f"ì„ë² ë”© ë°°ì¹˜ {batch_start//batch_size + 1} ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨í•œ ë°°ì¹˜ì˜ ëª¨ë“  ì¸ë±ìŠ¤ ê¸°ë¡
            failed_indices.extend(range(batch_start, batch_end))
            continue

    return all_embeddings, failed_indices


def _documents_from_snapshot(snap: pd.DataFrame, max_rows: int = 2000) -> Tuple[List[str], List[Dict], List[str]]:
    """ìŠ¤ëƒ…ìƒ· â†’ ë¬¸ì„œ ë³€í™˜"""
    docs, metas, ids = [], [], []
    use = snap.head(max_rows)

    for i, r in use.iterrows():
        # ë‚ ì§œë¥¼ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
        date_val = r.get('date')
        if pd.isna(date_val):
            date_str = "N/A"
        else:
            try:
                date_str = pd.to_datetime(date_val).strftime('%Y-%m-%d')
            except:
                date_str = str(date_val)

        doc = (
            f"[SNAPSHOT] "
            f"ë‚ ì§œ:{date_str} "
            f"ì„¼í„°:{r.get('center')} "
            f"SKU:{r.get('resource_code')} "
            f"ì¬ê³ :{r.get('stock_qty')}ê°œ"
        )
        if pd.notna(r.get('resource_name')):
            doc += f" ({r.get('resource_name')})"

        docs.append(doc)
        # "type" í‚¤ì›Œë“œëŠ” Chroma ë‚´ë¶€ ì˜ˆì•½ì–´ì´ë¯€ë¡œ "doc_type"ìœ¼ë¡œ ë³€ê²½
        # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ None ê°’ì€ ì œì™¸ (Chroma í˜¸í™˜ì„±)
        meta = {"doc_type": "snapshot"}

        center_val = r.get("center")
        if pd.notna(center_val) and str(center_val).strip():
            meta["center"] = str(center_val)

        sku_val = r.get("resource_code")
        if pd.notna(sku_val) and str(sku_val).strip():
            meta["sku"] = str(sku_val)

        if date_str and date_str != "N/A":
            meta["date"] = date_str

        metas.append(meta)
        ids.append(f"snap-{i}")

    return docs, metas, ids


def _ensure_session_index(snap_filtered: pd.DataFrame, filter_hash: str, max_rows: int = 2000):
    """ì„¸ì…˜ ì¸ë±ìŠ¤ ìƒì„±/ì¬ì‚¬ìš© (ìºì‹±)"""
    client = _chroma_cloud()
    if client is None:
        return None, 0

    col_name = _build_session_collection_name()

    # ìºì‹±: ê°™ì€ í•„í„°ë©´ ì¬ì‚¬ìš©
    if st.session_state.get("_last_filter_hash") == filter_hash:
        try:
            col = client.get_collection(name=col_name)
            if col.count() > 0:
                st.caption(f"â™»ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ ì¬ì‚¬ìš© ({col.count():,}ê°œ ë¬¸ì„œ)")
                return col, col.count()
            else:
                st.caption("ìºì‹œëœ ì»¬ë ‰ì…˜ì´ ë¹„ì–´ìˆìŒ, ì¬ìƒì„± ì¤‘...")
        except Exception as e:
            # ì»¬ë ‰ì…˜ì´ ì—†ê±°ë‚˜ ì—ëŸ¬ ë°œìƒ ì‹œ ì¬ìƒì„±
            st.caption(f"ìºì‹œëœ ì»¬ë ‰ì…˜ ë¡œë“œ ì‹¤íŒ¨: {e}, ì¬ìƒì„± ì¤‘...")

    # í•„í„°ê°€ ë³€ê²½ë˜ì—ˆê±°ë‚˜ ì»¬ë ‰ì…˜ì´ ë¹„ì–´ìˆìŒ â†’ ì¬ìƒì„± í•„ìš”
    # ê¸°ì¡´ ì»¬ë ‰ì…˜ ê°•ì œ ì‚­ì œ
    try:
        # ë¨¼ì € ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        try:
            existing = client.get_collection(col_name)
            st.caption(f"ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ '{col_name}' ì‚­ì œ ì¤‘... (ë¬¸ì„œ {existing.count():,}ê°œ)")
            client.delete_collection(col_name)
        except Exception:
            # ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ë¬´ì‹œ
            pass
    except Exception as e:
        st.caption(f"ì»¬ë ‰ì…˜ ì‚­ì œ ì‹œë„ ì¤‘ ì—ëŸ¬ (ë¬´ì‹œ): {e}")

    # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
    try:
        # get_or_create_collectionì€ _type ì—ëŸ¬ë¥¼ ìœ ë°œí•˜ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ ë¶„ë¦¬
        try:
            col = client.get_collection(name=col_name)
            # í˜¹ì‹œ ì´ë¯¸ ì¡´ì¬í•˜ê³  ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚­ì œ í›„ ì¬ìƒì„±
            if col.count() > 0:
                st.caption(f"âš ï¸ ì»¬ë ‰ì…˜ì´ ì—¬ì „íˆ ë°ì´í„° í¬í•¨ ({col.count():,}ê°œ), ê°•ì œ ì¬ìƒì„±...")
                client.delete_collection(col_name)
                # ë©”íƒ€ë°ì´í„° ì—†ì´ ìƒì„± (Chroma Cloud í˜¸í™˜ì„± ë¬¸ì œ íšŒí”¼)
                # ê¸°ë³¸ ë©”íŠ¸ë¦­ì€ L2ì´ì§€ë§Œ, ì„ë² ë”©ì„ ì •ê·œí™”í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ ë™ë“±í•˜ê²Œ ë™ì‘
                col = client.create_collection(name=col_name)
        except Exception:
            # ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒì„± (ë©”íƒ€ë°ì´í„° ì—†ì´)
            # ê¸°ë³¸ ë©”íŠ¸ë¦­ì€ L2ì´ì§€ë§Œ, ì„ë² ë”©ì„ ì •ê·œí™”í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ ë™ë“±í•˜ê²Œ ë™ì‘
            col = client.create_collection(name=col_name)
    except Exception as e:
        import traceback
        st.error(f"ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
        st.error(f"ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
        if hasattr(e, 'args') and e.args:
            st.caption(f"ì—ëŸ¬ ìƒì„¸: {e.args}")
        st.text("Traceback:")
        st.text(traceback.format_exc())
        return None, 0

    docs, metas, ids = _documents_from_snapshot(snap_filtered, max_rows)
    if not docs:
        return col, 0

    # ì„ë² ë”© ìƒì„± (ì‹¤íŒ¨í•œ ì¸ë±ìŠ¤ ì¶”ì )
    embs, failed_indices = _embed_batch(docs, batch_size=100)

    # ì‹¤íŒ¨í•œ ë°°ì¹˜ì˜ ë¬¸ì„œë¥¼ ì œê±° (ì •ë ¬ì„ ìœ ì§€í•˜ê¸° ìœ„í•´)
    if failed_indices:
        st.warning(f"âš ï¸ {len(failed_indices)}ê°œ ë¬¸ì„œ ì„ë² ë”© ì‹¤íŒ¨ (ì œì™¸ë¨)")
        # ì‹¤íŒ¨í•˜ì§€ ì•Šì€ ì¸ë±ìŠ¤ë§Œ ìœ ì§€
        failed_set = set(failed_indices)
        valid_indices = [i for i in range(len(docs)) if i not in failed_set]

        docs = [docs[i] for i in valid_indices]
        metas = [metas[i] for i in valid_indices]
        ids = [ids[i] for i in valid_indices]

    # ìµœì¢… ê²€ì¦: ì„ë² ë”©ê³¼ ë¬¸ì„œ ê°œìˆ˜ ì¼ì¹˜ í™•ì¸
    if len(embs) != len(docs):
        st.error(f"âŒ ì„ë² ë”©({len(embs)})ê³¼ ë¬¸ì„œ({len(docs)}) ê°œìˆ˜ ë¶ˆì¼ì¹˜!")
        # ì•ˆì „í•˜ê²Œ ì§§ì€ ìª½ì— ë§ì¶¤
        min_len = min(len(embs), len(docs))
        embs = embs[:min_len]
        docs = docs[:min_len]
        metas = metas[:min_len]
        ids = ids[:min_len]

    if embs:
        try:
            col.add(ids=ids, documents=docs, metadatas=metas, embeddings=embs)
        except Exception as e:
            import traceback
            st.error(f"ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            st.error(f"ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            # ì „ì²´ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
            error_detail = str(e)
            if hasattr(e, 'args') and e.args:
                st.caption(f"ìƒì„¸ ì—ëŸ¬: {e.args}")
            # ë©”íƒ€ë°ì´í„° ìƒ˜í”Œ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            if metas:
                st.caption(f"ì²« ë©”íƒ€ë°ì´í„° ìƒ˜í”Œ: {metas[0]}")
                st.caption(f"ë©”íƒ€ë°ì´í„° í‚¤ë“¤: {list(metas[0].keys())}")
            # traceback ì¶œë ¥
            st.text(traceback.format_exc())
            return col, 0

    st.session_state["_last_filter_hash"] = filter_hash
    return col, len(docs)


def search_documents(col: chromadb.Collection, question: str, k: int = 5) -> List[str]:
    """ë²¡í„° ê²€ìƒ‰ (ëª…ì‹œì  ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± + L2 ì •ê·œí™”)"""
    try:
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (ì»¬ë ‰ì…˜ì— embedding_functionì´ ì—†ìœ¼ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ ìƒì„±)
        genai.configure(api_key=st.secrets["gemini"]["api_key"])
        model = st.secrets["gemini"].get("embedding_model", "text-embedding-004")

        try:
            # Gemini APIë¡œ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            res = genai.embed_content(
                model=model,
                content=[question],
                task_type="retrieval_query"  # ê²€ìƒ‰ìš© ì„ë² ë”©
            )

            # ì„ë² ë”© ì¶”ì¶œ
            if hasattr(res, "embeddings"):
                query_emb_raw = [res.embeddings[0].values]
            elif isinstance(res, dict) and "embeddings" in res:
                query_emb_raw = [res["embeddings"][0]["embedding"]]
            else:
                query_emb_raw = [res["embedding"]]

            # L2 ì •ê·œí™”: ë¬¸ì„œ ì„ë² ë”©ê³¼ ë™ì¼í•˜ê²Œ ì •ê·œí™”í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë³´ì¥
            query_emb = _normalize_embeddings(query_emb_raw)

        except Exception as e:
            st.error(f"ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return []

        # ì„ë² ë”©ìœ¼ë¡œ ê²€ìƒ‰
        search_res = col.query(query_embeddings=query_emb, n_results=k)
        return search_res.get("documents", [[]])[0]

    except Exception as e:
        import traceback
        st.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        st.error(f"ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
        if hasattr(e, 'args') and e.args:
            st.caption(f"ì—ëŸ¬ ìƒì„¸: {e.args}")
        st.text("Traceback:")
        st.text(traceback.format_exc())
        return []


# ============================================================================
# 4. í•˜ì´ë¸Œë¦¬ë“œ ë‹µë³€ ìƒì„±
# ============================================================================

def generate_hybrid_answer(
    question: str,
    question_type: str,
    calc_result: Optional[Dict] = None,
    search_docs: Optional[List[str]] = None,
    session_digest: str = ""
) -> str:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ë‹µë³€ ìƒì„±

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        question_type: "quantitative" | "exploratory"
        calc_result: ì •ëŸ‰ ê³„ì‚° ê²°ê³¼
        search_docs: ë²¡í„° ê²€ìƒ‰ ë¬¸ì„œ
        session_digest: ì„¸ì…˜ ìš”ì•½

    Returns:
        AI ë‹µë³€ í…ìŠ¤íŠ¸
    """
    try:
        genai.configure(api_key=st.secrets["gemini"]["api_key"])
        model_name = st.secrets["gemini"].get("generation_model", "gemini-1.5-flash")
        model = genai.GenerativeModel(model_name)

        if question_type == "quantitative":
            # ì •ëŸ‰í˜•: ê³„ì‚° ê²°ê³¼ë¥¼ ì„¤ëª…/í•´ì„
            prompt = f"""ë‹¹ì‹ ì€ SCM ì¬ê³  ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ ê³„ì‚° ê²°ê³¼ë¥¼ ì‚¬ìš©ìì—ê²Œ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.

[ì§ˆë¬¸]
{question}

[ì •í™•í•œ ê³„ì‚° ê²°ê³¼]
{calc_result.get('description', '')}

[ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸]
{session_digest}

**ë‹µë³€ ê·œì¹™:**
1. ê³„ì‚° ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ìˆ«ì ë³€ê²½ ê¸ˆì§€)
2. 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ
3. ì¶”ê°€ ì¸ì‚¬ì´íŠ¸ë‚˜ í•´ì„ ì œê³µ ê°€ëŠ¥
4. í•œêµ­ì–´ë¡œ ì‘ì„±

ë‹µë³€:"""

        else:  # exploratory
            # íƒìƒ‰í˜•: ê²€ìƒ‰ëœ ë¬¸ì„œ ìš”ì•½
            prompt = f"""ë‹¹ì‹ ì€ SCM ì¬ê³  ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ ê·¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©í•´ì„œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

[ì§ˆë¬¸]
{question}

[ê·¼ê±° ë°ì´í„°]
{chr(10).join(f"- {d}" for d in search_docs)}

[ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸]
{session_digest}

**ë‹µë³€ ê·œì¹™:**
1. ê·¼ê±° ë°ì´í„°ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©
2. ìˆ˜ì¹˜/ì„¼í„°/SKUë¥¼ ëª…ì‹œì ìœ¼ë¡œ í¬í•¨
3. 2-4ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ
4. ì¶”ì •í•˜ì§€ ë§ ê²ƒ
5. í•œêµ­ì–´ë¡œ ì‘ì„±

ë‹µë³€:"""

        response = model.generate_content(prompt)
        return response.text

    except Exception as e:
        st.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
        # í´ë°±
        if calc_result:
            return calc_result.get('description', 'ê³„ì‚° ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.')
        else:
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\n" + \
                   "\n".join(f"â€¢ {d}" for d in (search_docs or [])[:3])


# ============================================================================
# 5. ë©”ì¸ UI í•¨ìˆ˜
# ============================================================================

def render_hybrid_chatbot_tab(
    snapshot_df: pd.DataFrame,
    selected_centers: List[str],
    selected_skus: List[str]
):
    """
    í•˜ì´ë¸Œë¦¬ë“œ AI ì±—ë´‡ íƒ­ ë Œë”ë§

    Args:
        snapshot_df: ì „ì²´ ìŠ¤ëƒ…ìƒ· ë°ì´í„°
        selected_centers: ì„ íƒëœ ì„¼í„°
        selected_skus: ì„ íƒëœ SKU
    """
    st.subheader("ğŸ¤– AI ì–´ì‹œìŠ¤í„´íŠ¸ (1.5ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ)")

    # í•„í„°ë§
    snap = snapshot_df.copy()
    if "center" in snap.columns:
        snap = snap[snap["center"].astype(str).isin(selected_centers)]
    if "resource_code" in snap.columns:
        snap = snap[snap["resource_code"].astype(str).isin(selected_skus)]

    if snap.empty:
        st.warning("ì„ íƒëœ í•„í„°ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return

    # ë°ì´í„° í’ˆì§ˆ í™•ì¸ ë° ì¤‘ë³µ ì œê±°
    st.caption(f"ğŸ” í•„í„°ë§ ì „: {len(snapshot_df):,}í–‰ â†’ í•„í„°ë§ í›„: {len(snap):,}í–‰")

    # ë‚ ì§œë³„ ìŠ¤ëƒ…ìƒ· ë°ì´í„° ì •ê·œí™”: ê° (ì„¼í„°, SKU)ì˜ ìµœì‹  ë‚ ì§œë§Œ ìœ ì§€
    # í•„ìš”í•œ ëª¨ë“  ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
    required_cols = ['date', 'center', 'resource_code']
    if all(col in snap.columns for col in required_cols):
        # ë‚ ì§œ ë³€í™˜
        snap['date'] = pd.to_datetime(snap['date'], errors='coerce')

        # NaT í–‰ ì œê±° (ì˜ëª»ëœ ë‚ ì§œ ë˜ëŠ” ëˆ„ë½ëœ ë‚ ì§œ)
        # NaTëŠ” sort ì‹œ ë§ˆì§€ë§‰ìœ¼ë¡œ ê°€ì„œ "ìµœì‹ "ìœ¼ë¡œ ì˜ëª» ì„ íƒë˜ëŠ” ë¬¸ì œ ë°©ì§€
        nat_count = snap['date'].isna().sum()
        if nat_count > 0:
            st.caption(f"â„¹ï¸ ë‚ ì§œ ëˆ„ë½/ì˜¤ë¥˜ {nat_count:,}í–‰ ì œê±° (NaT)")
            snap = snap.dropna(subset=['date'])

        if snap.empty:
            st.warning("ìœ íš¨í•œ ë‚ ì§œê°€ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return

        # ê° (ì„¼í„°, SKU) ì¡°í•©ì´ ì—¬ëŸ¬ ë‚ ì§œì— ê±¸ì³ ìˆëŠ”ì§€ í™•ì¸
        group_counts = snap.groupby(['center', 'resource_code']).size()
        multi_date_groups = (group_counts > 1).sum()

        if multi_date_groups > 0:
            total_rows_before = len(snap)
            st.warning(f"âš ï¸ {multi_date_groups:,}ê°œ (ì„¼í„°, SKU) ì¡°í•©ì´ ì—¬ëŸ¬ ë‚ ì§œì— ì¡´ì¬ - ìµœì‹  ë°ì´í„°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤")

            # ìµœì‹  ë‚ ì§œë§Œ ìœ ì§€ (ê° ì„¼í„°-SKU ì¡°í•©ë³„ë¡œ)
            # NaTê°€ ì´ë¯¸ ì œê±°ë˜ì—ˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ sort ê°€ëŠ¥
            snap = snap.sort_values('date').groupby(['center', 'resource_code'], as_index=False).last()

            st.caption(f"âœ… {total_rows_before:,}í–‰ â†’ {len(snap):,}í–‰ (ë‚ ì§œë³„ ìŠ¤ëƒ…ìƒ· ì •ê·œí™”)")
        else:
            st.caption(f"âœ… ê° (ì„¼í„°, SKU)ê°€ ë‹¨ì¼ ë‚ ì§œë§Œ ì¡´ì¬ (ì •ê·œí™” ë¶ˆí•„ìš”)")
    else:
        missing = [col for col in required_cols if col not in snap.columns]
        st.caption(f"â„¹ï¸ ë‚ ì§œë³„ ì •ê·œí™” ìŠ¤í‚µ (ì»¬ëŸ¼ ëˆ„ë½: {', '.join(missing)})")

    # ì„¸ì…˜ ìš”ì•½ (NaT ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
    latest_date = pd.to_datetime(snap.get('date'), errors='coerce').max()
    if pd.isna(latest_date):
        latest_date_str = "ë¯¸ì •"
    else:
        latest_date_str = latest_date.strftime('%Y-%m-%d')

    # ì¬ê³  ê³„ì‚° (ë””ë²„ê¹… ì •ë³´ í¬í•¨)
    stock_values = pd.to_numeric(snap.get('stock_qty'), errors='coerce').fillna(0)
    total_stock = stock_values.sum()

    # ë¹„ì •ìƒì ìœ¼ë¡œ í° ê°’ ê²½ê³ 
    if total_stock > 1000000:  # 100ë§Œê°œ ì´ìƒì´ë©´ ê²½ê³ 
        st.warning(f"âš ï¸ ë¹„ì •ìƒì ìœ¼ë¡œ í° ì¬ê³  ìˆ˜ëŸ‰: {total_stock:,.0f}ê°œ")
        st.caption(f"í‰ê·  ì¬ê³ : {stock_values.mean():,.0f}ê°œ, ìµœëŒ€: {stock_values.max():,.0f}ê°œ")
        # ìƒìœ„ 10ê°œ í–‰ í™•ì¸
        top_stocks = snap.nlargest(10, 'stock_qty')[['center', 'resource_code', 'stock_qty', 'date']]
        with st.expander("ğŸ” ì¬ê³  ìƒìœ„ 10ê°œ í™•ì¸"):
            st.dataframe(top_stocks)

    session_digest = (
        f"ì´ ì¬ê³ ={total_stock:,.0f}ê°œ / "
        f"ì„¼í„° {snap.get('center', pd.Series([], dtype=str)).nunique()}ê³³ / "
        f"SKU {snap.get('resource_code', pd.Series([], dtype=str)).nunique()}ê°œ / "
        f"ìµœì‹ ì¼={latest_date_str}"
    )

    # í•„í„° í•´ì‹œ
    filter_hash = _get_filter_hash(selected_centers, selected_skus)

    # ìë™ ì¸ë±ì‹± (íƒìƒ‰í˜• ì§ˆë¬¸ ëŒ€ë¹„)
    if st.session_state.get("_last_filter_hash") != filter_hash:
        with st.spinner("ğŸ“š ë²¡í„° ì¸ë±ìŠ¤ ì¤€ë¹„ ì¤‘..."):
            try:
                col, n = _ensure_session_index(snap, filter_hash, max_rows=2000)
                if col and n > 0:
                    st.success(f"âœ… {n:,}ê±´ ì¸ë±ì‹± ì™„ë£Œ")
            except Exception as e:
                st.warning(f"ì¸ë±ì‹± ì‹¤íŒ¨ (ì •ëŸ‰ ê³„ì‚°ì€ ê°€ëŠ¥): {e}")

    # ì„¸ì…˜ ì •ë³´
    with st.expander("â„¹ï¸ ì„¸ì…˜ ì •ë³´"):
        st.caption(session_digest)
        st.caption("**ì‹œìŠ¤í…œ ëª¨ë“œ:** í•˜ì´ë¸Œë¦¬ë“œ (ì •ëŸ‰ ê³„ì‚° + ë²¡í„° ê²€ìƒ‰)")

    st.divider()

    # ì§ˆë¬¸ ì…ë ¥
    q = st.text_input(
        "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
        placeholder="ì˜ˆ: REVVNì˜ ì´ ì¬ê³ ëŠ”? / ì¬ê³ ê°€ ê°€ì¥ ë§ì€ ì„¼í„°ëŠ”?",
        key="hybrid_q"
    )

    col1, col2 = st.columns([4, 1])
    with col2:
        k = st.slider("ê²€ìƒ‰ ê²°ê³¼", 1, 10, 5, key="hybrid_k")

    if st.button("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°", type="primary") and q:
        with st.spinner("ğŸ” ë¶„ì„ ì¤‘..."):
            # 1. ì§ˆë¬¸ ë¶„ë¥˜
            q_type = classify_question(q)
            entities = extract_entities(q, selected_centers, selected_skus)

            st.caption(f"ğŸ”– ì§ˆë¬¸ ìœ í˜•: **{q_type}** | ëŒ€ìƒ: {entities['centers'][:2]} Ã— {entities['skus'][:2]}")

            # 2. ì§ˆë¬¸ ìœ í˜•ë³„ ì²˜ë¦¬
            if q_type == "quantitative":
                # ì •ëŸ‰ ê³„ì‚°
                calc_result = calculate_quantitative(q, snap, entities)

                # AI ì„¤ëª… ìƒì„±
                answer = generate_hybrid_answer(
                    question=q,
                    question_type="quantitative",
                    calc_result=calc_result,
                    session_digest=session_digest
                )

                st.markdown("### ğŸ“Š ë‹µë³€")
                st.markdown(answer)

                # ê³„ì‚° ìƒì„¸
                with st.expander("ğŸ”¢ ê³„ì‚° ìƒì„¸"):
                    st.write(f"**ê³„ì‚° ìœ í˜•:** {calc_result['type']}")
                    st.write(f"**ê²°ê³¼:** {calc_result['result']}")
                    if "breakdown" in calc_result:
                        st.write("**ì„¼í„°ë³„ ë¶„í•´:**")
                        for center, qty in calc_result["breakdown"].items():
                            st.write(f"- {center}: {qty:,.0f}ê°œ")

            elif q_type == "exploratory":
                # ë²¡í„° ê²€ìƒ‰
                try:
                    client = _chroma_cloud()
                    col = client.get_collection(_build_session_collection_name())
                    docs = search_documents(col, q, k=k)

                    if not docs:
                        st.info("ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í•„í„°ë¥¼ ì¡°ì •í•´ë³´ì„¸ìš”.")
                    else:
                        # AI ìš”ì•½ ìƒì„±
                        answer = generate_hybrid_answer(
                            question=q,
                            question_type="exploratory",
                            search_docs=docs,
                            session_digest=session_digest
                        )

                        st.markdown("### ğŸ“Š ë‹µë³€")
                        st.markdown(answer)

                        # ê·¼ê±°
                        with st.expander("ğŸ” ê·¼ê±° ë°ì´í„°"):
                            for i, d in enumerate(docs, 1):
                                st.write(f"{i}. {d}")

                except Exception as e:
                    st.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

            else:  # business
                st.info("ğŸš§ ë¹„ì¦ˆë‹ˆìŠ¤ íŒë‹¨ ì§ˆë¬¸ì€ 2ë‹¨ê³„ì—ì„œ ì§€ì› ì˜ˆì •ì…ë‹ˆë‹¤.")
                st.caption("í˜„ì¬ëŠ” ì •ëŸ‰ ê³„ì‚°ê³¼ íƒìƒ‰ ê²€ìƒ‰ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

    # ì˜ˆì‹œ ì§ˆë¬¸
    st.divider()
    st.caption("**ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸:**")

    col1, col2 = st.columns(2)
    with col1:
        st.caption("**ì •ëŸ‰ ê³„ì‚°í˜• (ë¹ ë¥¸ ë°ì´í„° ì¡°íšŒ)**")
        st.caption("â€¢ ì´ ì¬ê³ ëŠ”?")
        st.caption("â€¢ ì„¼í„°ë³„ ì¬ê³ ëŠ”?")
        st.caption("â€¢ ì¬ê³ ê°€ ê°€ì¥ ë§ì€ ì„¼í„°ëŠ”?")
        st.caption("â€¢ BA00021ì€ ì–´ëŠ ì„¼í„°ì— ìˆë‚˜ìš”?")
        st.caption("â€¢ íƒœê´‘KRì—ëŠ” ì–´ë–¤ SKUê°€ ìˆë‚˜ìš”?")

    with col2:
        st.caption("**íƒìƒ‰í˜• (AI ê²€ìƒ‰, ëŠë¦¼)**")
        st.caption("â€¢ ìµœê·¼ ìŠ¤ëƒ…ìƒ· ë‚ ì§œëŠ”?")
        st.caption("â€¢ ì¬ê³  ê´€ë¦¬ ì •ì±…ì€?")
        st.caption("â€¢ íŠ¹ì´ì‚¬í•­ì´ ìˆë‚˜ìš”?")
        st.caption("â€¢ ë°ì´í„° í’ˆì§ˆì€ ì–´ë–¤ê°€ìš”?")
