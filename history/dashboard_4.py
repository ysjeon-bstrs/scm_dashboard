
import pandas as pd
import numpy as np
import streamlit as st
import plotly.express as px
import re
import time
from datetime import datetime, timedelta
from io import BytesIO, StringIO
from typing import Dict, List, Optional, Tuple
import json
import requests
import os
from urllib.parse import quote
import google.auth
from google.oauth2 import service_account
from googleapiclient.discovery import build

# === Google Sheets ë°ì´í„° ë¡œë” ì„¤ì • ===
GSHEET_ID = "1RYjKW2UDJ2kWJLAqQH26eqx2-r9Xb0_qE_hfwu9WIj8"
CREDENTIALS_FILE = r"C:\Users\BST-Desktop-051\Downloads\PY\python-spreadsheet-409212-3df25e0dc166.json"

"""
ë°ì´í„° ë¡œë” ì„¤ì • ë°©ë²•:

1. Streamlit Secrets (ê¶Œì¥):
   .streamlit/secrets.toml íŒŒì¼ì— ì¶”ê°€:
   [data_loader]
   method = "api"  # api, webapp, gviz
   api_credentials = "path/to/credentials.json"
   webapp_url = "https://script.google.com/..."
   webapp_token = "your_token"
   gviz_url = "https://docs.google.com/spreadsheets/..."

2. í™˜ê²½ë³€ìˆ˜:
   export DATA_LOADER_METHOD=api
   export GOOGLE_CREDENTIALS_FILE=path/to/credentials.json
   export GS_WEBAPP_URL=https://script.google.com/...
   export GS_WEBAPP_TOKEN=your_token
   export GS_GVIZ_URL=https://docs.google.com/spreadsheets/...

3. ê¸°ë³¸ê°’: Google Sheets API v4 (ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦)
"""

# ë°ì´í„° ë¡œë” ë°©ì‹ ì„¤ì • (í™˜ê²½ë³€ìˆ˜/Secrets ìš°ì„ )
def get_data_loader_config():
    """ë°ì´í„° ë¡œë” ì„¤ì •ì„ í™˜ê²½ë³€ìˆ˜/Secretsì—ì„œ ê°€ì ¸ì˜´"""
    # 1ìˆœìœ„: Streamlit Secrets
    if "data_loader" in st.secrets:
        return st.secrets["data_loader"]
    
    # 2ìˆœìœ„: í™˜ê²½ë³€ìˆ˜
    return {
        "method": os.getenv("DATA_LOADER_METHOD", "api"),  # api, webapp, gviz
        "api_credentials": os.getenv("GOOGLE_CREDENTIALS_FILE", CREDENTIALS_FILE),
        "webapp_url": os.getenv("GS_WEBAPP_URL", ""),
        "webapp_token": os.getenv("GS_WEBAPP_TOKEN", ""),
        "gviz_url": os.getenv("GS_GVIZ_URL", "")
    }

# ì „ì—­ ì„¤ì • ë¡œë“œ
DATA_LOADER_CONFIG = get_data_loader_config()

def show_data_loader_info():
    """í˜„ì¬ ë°ì´í„° ë¡œë” ì„¤ì • ì •ë³´ í‘œì‹œ"""
    method = DATA_LOADER_CONFIG.get("method", "api")
    st.info(f"ğŸ“Š ë°ì´í„° ë¡œë”: {method.upper()}")
    
    if method == "api":
        st.caption("Google Sheets API v4 ì‚¬ìš© (ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦)")
    elif method == "webapp":
        st.caption("Apps Script Web App ì‚¬ìš©")
    elif method == "gviz":
        st.caption("Google Viz API ì‚¬ìš©")
    
    # ë””ë²„ê·¸ ì •ë³´ (ê°œë°œ ëª¨ë“œì—ì„œë§Œ)
    if st.checkbox("ğŸ”§ ì„¤ì • ìƒì„¸ ì •ë³´", help="ê°œë°œììš© ì„¤ì • ì •ë³´"):
        st.json(DATA_LOADER_CONFIG)

# ì„¼í„°ë³„ ì›ë³¸ ì»¬ëŸ¼ ë§¤í•‘
CENTER_COL = {
    "íƒœê´‘KR": "stock2",
    "AMZUS": "fba_available_stock",
    "í’ˆê³ KR": "poomgo_v2_available_stock",
    "SBSPH": "shopee_ph_available_stock",
    "SBSSG": "shopee_sg_available_stock",
    "SBSMY": "shopee_my_available_stock",
    "AcrossBUS": "acrossb_available_stock",
    "ì–´í¬ë¡œìŠ¤ë¹„US": "acrossb_available_stock",  # ë³„ì¹­ ì¶”ê°€
}


def gs_csv(sheet_name: str) -> pd.DataFrame:
    """
    í†µí•© ë°ì´í„° ë¡œë” - ì„¤ì •ì— ë”°ë¼ ë‹¤ë¥¸ ë°©ì‹ ì‚¬ìš©
    - ê¸°ë³¸: Google Sheets API v4
    - ëŒ€ì•ˆ: Apps Script Web App, Google Viz API
    """
    method = DATA_LOADER_CONFIG.get("method", "api")
    
    if method == "api":
        return _load_via_api(sheet_name)
    elif method == "webapp":
        return _load_via_webapp(sheet_name)
    elif method == "gviz":
        return _load_via_gviz(sheet_name)
    else:
        st.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° ë¡œë” ë°©ì‹: {method}")
        return pd.DataFrame()

def _load_via_api(sheet_name: str) -> pd.DataFrame:
    """Google Sheets API v4 ì‚¬ìš©"""
    try:
        credentials = service_account.Credentials.from_service_account_file(
            DATA_LOADER_CONFIG.get("api_credentials", CREDENTIALS_FILE),
            scopes=['https://www.googleapis.com/auth/spreadsheets.readonly']
        )
        
        service = build('sheets', 'v4', credentials=credentials)
        result = service.spreadsheets().values().get(
            spreadsheetId=GSHEET_ID,
            range=f'{sheet_name}!A:Z'
        ).execute()
        
        values = result.get('values', [])
        if not values:
            return pd.DataFrame()
        
        headers = values[0]
        data = values[1:]
        return pd.DataFrame(data, columns=headers)
        
    except Exception as e:
        st.error(f"Google Sheets API ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

def _load_via_webapp(sheet_name: str) -> pd.DataFrame:
    """Apps Script Web App ì‚¬ìš©"""
    try:
        webapp_url = DATA_LOADER_CONFIG.get("webapp_url")
        token = DATA_LOADER_CONFIG.get("webapp_token")
        
        if not webapp_url or not token:
            st.error("WebApp ì„¤ì •ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤ (webapp_url, webapp_token)")
            return pd.DataFrame()
        
        params = {
            "id": GSHEET_ID,
            "sheet": sheet_name,
            "token": token,
            "_": str(int(time.time()))
        }
        
        r = requests.get(webapp_url, params=params, timeout=15)
        r.raise_for_status()
        data = r.json()
        
        if not data.get("ok"):
            raise RuntimeError(f"WebApp ì˜¤ë¥˜: {data.get('error')}")
        
        rows = data.get("rows", [])
        return pd.DataFrame(rows)
        
    except Exception as e:
        st.error(f"WebApp ë¡œë“œ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

def _load_via_gviz(sheet_name: str) -> pd.DataFrame:
    """Google Viz API ì‚¬ìš©"""
    try:
        gviz_url = DATA_LOADER_CONFIG.get("gviz_url")
        if not gviz_url:
            st.error("GVIZ URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return pd.DataFrame()
        
        # ì‹œíŠ¸ëª… URL ì¸ì½”ë”©
        encoded_sheet = quote(sheet_name)
        url = f"{gviz_url}&sheet={encoded_sheet}"
        
        r = requests.get(url, timeout=15)
        r.raise_for_status()
        
        # CSV íŒŒì‹±
        df = pd.read_csv(StringIO(r.text))
        return df
        
    except Exception as e:
        st.error(f"GVIZ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

@st.cache_data(ttl=300)
def load_from_gsheet():
    df_move = gs_csv("SCM_í†µí•©")             # ì´ë™ ì›ì¥
    df_ref  = gs_csv("snap_ì •ì œ")           # ì •ì œ ìŠ¤ëƒ…ìƒ· (long: date|center|resource_code|stock_qty)
    # ì…ê³  ì˜ˆì •ì€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë¹ˆ DF
    try:
        df_incoming = gs_csv("ì…ê³ ì˜ˆì •ë‚´ì—­")
    except Exception:
        df_incoming = pd.DataFrame()
    return df_move, df_ref, df_incoming

@st.cache_data(ttl=300)
def load_snapshot_raw():
    # 1ìˆœìœ„: Google Sheets APIë¡œ ì§ì ‘ ë¡œë“œ
    try:
        df = gs_csv("snapshot_raw")
        if df is not None and not df.empty:
            return df
    except Exception:
        pass

    # 2ìˆœìœ„(ì„ íƒ): ì›¹ì•± í”„ë¡ì‹œê°€ ì •ì˜ë¼ ìˆìœ¼ë©´ ì‚¬ìš©
    try:
        fetch = globals().get("_fetch_sheet_via_webapp", None)
        if callable(fetch):
            return fetch("snapshot_raw")
    except Exception:
        pass

    # ì‹¤íŒ¨ ì‹œ ë¹ˆ DF
    return pd.DataFrame()

def build_lot_detail(snap_raw: pd.DataFrame, date_latest: pd.Timestamp, sku: str, centers: list[str]) -> pd.DataFrame:
    if snap_raw is None or snap_raw.empty:
        return pd.DataFrame(columns=["lot"] + centers + ["í•©ê³„"])

    sr = snap_raw.copy()
    # ë‚ ì§œ/í—¤ë” ìœ ì—° ì¸ì‹
    cols = {c.strip().lower(): c for c in sr.columns}
    col_date = cols.get("snapshot_date") or cols.get("date")
    col_sku  = cols.get("resource_code") or cols.get("sku") or cols.get("ìƒí’ˆì½”ë“œ")
    col_lot  = cols.get("lot")

    if not all([col_date, col_sku, col_lot]):
        return pd.DataFrame(columns=["lot"] + centers + ["í•©ê³„"])

    # ë‚ ì§œ ì •ê·œí™”
    sr[col_date] = pd.to_datetime(sr[col_date], errors="coerce")
    # í•„í„°: ìµœì‹ ì¼ + SKU
    sub = sr[(sr[col_date].dt.normalize()==date_latest.normalize()) & (sr[col_sku].astype(str)==str(sku))].copy()

    if sub.empty:
        return pd.DataFrame(columns=["lot"] + centers + ["í•©ê³„"])

    # ì„¼í„°ë³„ ìˆ˜ëŸ‰ ì»¬ëŸ¼(ì—†ìœ¼ë©´ ê±´ë„ˆëœ€)
    used_centers = []
    for ct in centers:
        if CENTER_COL.get(ct) in sr.columns:
            used_centers.append(ct)
    if not used_centers:
        return pd.DataFrame(columns=["lot"] + centers + ["í•©ê³„"])

    # ìˆ«ìí™” + ìŒìˆ˜ 0 í´ë¦½
    for ct in used_centers:
        c = CENTER_COL[ct]
        sub[c] = pd.to_numeric(sub[c], errors="coerce").fillna(0).clip(lower=0)

    # lotë³„ í•©ê³„
    out = pd.DataFrame({"lot": sub[col_lot].astype(str).fillna("(no lot)")})
    for ct in used_centers:
        out[ct] = sub.groupby(col_lot)[CENTER_COL[ct]].transform("sum")
    out = out.drop_duplicates()
    out["í•©ê³„"] = out[used_centers].sum(axis=1)
    # ì„ íƒ ì„¼í„° ì™¸ëŠ” 0 ì—´ ì¶”ê°€(ë³´ê¸°ìš©)
    for ct in centers:
        if ct not in used_centers:
            out[ct] = 0
    # ìˆ˜ëŸ‰ 0ì¸ ë¡œíŠ¸ ì œê±°
    out = out[out["í•©ê³„"] > 0]
    
    # ì •ë ¬
    return out[["lot"] + centers + ["í•©ê³„"]].sort_values("í•©ê³„", ascending=False).reset_index(drop=True)

def pivot_inventory_cost_from_raw(snap_raw: pd.DataFrame,
                                  latest_dt: pd.Timestamp,
                                  centers: list[str]) -> pd.DataFrame:
    """
    snapshot_rawì˜ **latest_dt(ìŠ¤ëƒ…ìƒ· ìµœì‹ ì¼)** ì—ì„œ
    lotë³„ COGS Ã— ê° ì„¼í„° ìˆ˜ëŸ‰ì„ í•©ì‚°í•´ SKUÃ—ì„¼í„° ë¹„ìš© í”¼ë²— ë°˜í™˜.
    """
    if snap_raw is None or snap_raw.empty:
        return pd.DataFrame(columns=["resource_code"] + [f"{c}_ì¬ê³ ìì‚°" for c in centers])

    df = snap_raw.copy()
    cols = {str(c).strip().lower(): c for c in df.columns}

    col_date = cols.get("snapshot_date") or cols.get("date")
    col_sku  = cols.get("resource_code") or cols.get("sku") or cols.get("ìƒí’ˆì½”ë“œ") or cols.get("option1")
    col_cogs = cols.get("cogs")  # ì œì¡°ì›ê°€
    if not all([col_date, col_sku, col_cogs]):
        return pd.DataFrame(columns=["resource_code"] + [f"{c}_ì¬ê³ ìì‚°" for c in centers])

    # íƒ€ì… ì •ê·œí™”
    df[col_date] = pd.to_datetime(df[col_date], errors="coerce").dt.normalize()
    df[col_sku]  = df[col_sku].astype(str)
    df[col_cogs] = pd.to_numeric(df[col_cogs], errors="coerce").fillna(0).clip(lower=0)

    # âœ… latest_dtë§Œ ì‚¬ìš© (ì˜¤ëŠ˜/ìµœì‹ ì¼ ìš°ì„ ìˆœìœ„ ì œê±°)
    sub = df[df[col_date] == pd.Timestamp(latest_dt).normalize()].copy()
    if sub.empty:
        return pd.DataFrame(columns=["resource_code"] + [f"{c}_ì¬ê³ ìì‚°" for c in centers])

    # ì„¼í„°ë³„ ë¹„ìš© ê³„ì‚°: sum_lot( cogs Ã— qty_center )
    cost_cols = {}
    for ct in centers:
        src_col = CENTER_COL.get(ct)
        if not src_col or src_col not in sub.columns:
            continue
        qty = pd.to_numeric(sub[src_col], errors="coerce").fillna(0).clip(lower=0)
        cost = (sub[col_cogs] * qty)  # ë‹¨ìˆœ COGS Ã— ìˆ˜ëŸ‰
        # SKUë³„ í•©ê³„
        g = sub[[col_sku]].copy()
        g[f"{ct}_ì¬ê³ ìì‚°"] = cost
        cost_cols[ct] = g.groupby(col_sku, as_index=False)[f"{ct}_ì¬ê³ ìì‚°"].sum()

    if not cost_cols:
        return pd.DataFrame(columns=["resource_code"] + [f"{c}_ì¬ê³ ìì‚°" for c in centers])

    # ë³‘í•©: SKU ê¸°ì¤€ìœ¼ë¡œ ëª¨ë“  ì„¼í„° ë¹„ìš© ë¶™ì´ê¸°
    base = pd.DataFrame({"resource_code": pd.unique(sub[col_sku])})
    for ct, g in cost_cols.items():
        base = base.merge(g.rename(columns={col_sku: "resource_code"}), on="resource_code", how="left")
    # ëˆ„ë½ 0, ì •ìˆ˜ ë°˜ì˜¬ë¦¼
    num_cols = [c for c in base.columns if c.endswith("_ì¬ê³ ìì‚°")]
    base[num_cols] = base[num_cols].fillna(0).round().astype(int)
    return base

st.set_page_config(page_title="ê¸€ë¡œë²Œ ëŒ€ì‹œë³´ë“œ â€” v4", layout="wide")

st.title("ğŸ“¦ SCM ì¬ê³  íë¦„ ëŒ€ì‹œë³´ë“œ â€” v4")

st.caption("í˜„ì¬ ì¬ê³ ëŠ” í•­ìƒ **ìŠ¤ëƒ…ìƒ· ê¸°ì¤€(snap_ì •ì œ)**ì…ë‹ˆë‹¤. ì´ë™ì¤‘ / ìƒì‚°ì¤‘ ë¼ì¸ì€ ì˜ˆì¸¡ìš© ê°€ìƒ ë¼ì¸ì…ë‹ˆë‹¤. â€˜ìƒì‚°ì¤‘(ë¯¸ì™„ë£Œ)â€™ ê·¸ë˜í”„ëŠ” **íƒœê´‘KR ì„¼í„° ì„ íƒ ì‹œì—ë§Œ** í‘œì‹œë©ë‹ˆë‹¤.")

# -------------------- Helpers --------------------
def _coalesce_columns(df: pd.DataFrame, candidates: List, parse_date: bool = False) -> pd.Series:
    all_names = []
    for item in candidates:
        if isinstance(item, (list, tuple, set)):
            all_names.extend([str(x).strip() for x in item])
        else:
            all_names.append(str(item).strip())
    
    # 1ë‹¨ê³„: ì •í™•í•œ ë§¤ì¹­ ì‹œë„
    cols = [c for c in df.columns if str(c).strip() in all_names]
    
    # 2ë‹¨ê³„: ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ë§¤ì¹­ ì‹œë„
    if not cols:
        cols = [c for c in df.columns if any(name.lower() in str(c).lower() for name in all_names)]
    
    # 3ë‹¨ê³„: ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­ ì‹œë„
    if not cols:
        cols = [c for c in df.columns if any(name.lower() in str(c).lower() or str(c).lower() in name.lower() for name in all_names)]
    
    if not cols:
        return pd.Series(pd.NaT if parse_date else np.nan, index=df.index)
    sub = df[cols].copy()
    if parse_date:
        for c in cols:
            sub[c] = pd.to_datetime(sub[c], errors="coerce")
        # âœ… ë‚ ì§œ ì •ê·œí™” ê°•í™”: ëª¨ë“  ë‚ ì§œëŠ” .dt.normalize() ì ìš©
        out = sub.bfill(axis=1).iloc[:, 0]
        if out.dtype == 'datetime64[ns]':
            out = out.dt.normalize()
    else:
        out = sub.bfill(axis=1).iloc[:, 0]
    return out

@st.cache_data(ttl=300)
def load_from_excel(file):
    data = file.read() if hasattr(file, "read") else file
    bio = BytesIO(data) if isinstance(data, (bytes, bytearray)) else file

    xl = pd.ExcelFile(bio, engine="openpyxl")
    # í•„ìˆ˜: ì´ë™ë°ì´í„° ì‹œíŠ¸(ì´ë¦„ ìœ ì§€), ì •ì œ ìŠ¤ëƒ…ìƒ· ì‹œíŠ¸(ì—¬ëŸ¬ í›„ë³´ëª… ì§€ì›), ì…ê³ ì˜ˆì •(ì„ íƒ)
    need = {"SCM_í†µí•©": None}
    for s in xl.sheet_names:
        if s == "SCM_í†µí•©":
            need["SCM_í†µí•©"] = s

    # ì •ì œ ìŠ¤ëƒ…ìƒ· í›„ë³´ëª…
    refined_name = next((s for s in xl.sheet_names if s in ["snap_ì •ì œ","snap_refined","snap_refine","snap_ref"]), None)
    if refined_name is None:
        st.error("ì—‘ì…€ì— ì •ì œ ìŠ¤ëƒ…ìƒ· ì‹œíŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤. (ì‹œíŠ¸ëª…: 'snap_ì •ì œ' ë˜ëŠ” 'snap_refined')")
        st.stop()

    df_move = pd.read_excel(bio, sheet_name=need["SCM_í†µí•©"], engine="openpyxl")
    bio.seek(0)
    df_ref = pd.read_excel(bio, sheet_name=refined_name, engine="openpyxl")
    bio.seek(0)

    wip_df = None
    if "ì…ê³ ì˜ˆì •ë‚´ì—­" in xl.sheet_names:
        wip_df = pd.read_excel(bio, sheet_name="ì…ê³ ì˜ˆì •ë‚´ì—­", engine="openpyxl")
    return df_move, df_ref, wip_df

def normalize_refined_snapshot(df_ref: pd.DataFrame) -> pd.DataFrame:
    """ì •ì œ ìŠ¤ëƒ…ìƒ· ì‹œíŠ¸ â†’ í•„ìš”í•œ íƒ€ì… ë³´ì •"""
    # ì»¬ëŸ¼ëª… ë§¤í•‘ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ, ê³µë°± ì œê±°)
    cols = {c.strip().lower(): c for c in df_ref.columns}
    
    # ìœ ì—°í•œ ì»¬ëŸ¼ëª… ë§¤í•‘
    date_col = None
    center_col = None
    resource_col = None
    stock_col = None
    
    # date ì»¬ëŸ¼ ì°¾ê¸°
    for key in ["date", "ë‚ ì§œ", "snapshot_date", "ìŠ¤ëƒ…ìƒ·ì¼"]:
        if key in cols:
            date_col = cols[key]
            break
    
    # center ì»¬ëŸ¼ ì°¾ê¸°
    for key in ["center", "ì„¼í„°", "ì°½ê³ ", "warehouse"]:
        if key in cols:
            center_col = cols[key]
            break
    
    # resource_code ì»¬ëŸ¼ ì°¾ê¸°
    for key in ["resource_code", "sku", "ìƒí’ˆì½”ë“œ", "product_code"]:
        if key in cols:
            resource_col = cols[key]
            break
    
    # stock_qty ì»¬ëŸ¼ ì°¾ê¸°
    for key in ["stock_qty", "qty", "ìˆ˜ëŸ‰", "ì¬ê³ ", "quantity"]:
        if key in cols:
            stock_col = cols[key]
            break
    
    # ëˆ„ë½ëœ ì»¬ëŸ¼ í™•ì¸
    missing = []
    if not date_col: missing.append("date")
    if not center_col: missing.append("center")
    if not resource_col: missing.append("resource_code")
    if not stock_col: missing.append("stock_qty")
    
    if missing:
        st.error(f"'snap_ì •ì œ' ì‹œíŠ¸ì— ëˆ„ë½ëœ ì»¬ëŸ¼: {missing}")
        st.write("ì‹¤ì œ ì»¬ëŸ¼ëª…:", list(df_ref.columns))
        st.write("ì»¬ëŸ¼ëª… ë§¤í•‘:", cols)
        st.stop()
    
    # ì´ë¦„ ì •ê·œí™” ë° ë°ì´í„° íƒ€ì… ë³€í™˜ (í•œ ë²ˆì— ì²˜ë¦¬)
    result = df_ref.rename(columns={date_col:"date",
                                   center_col:"center",
                                   resource_col:"resource_code",
                                   stock_col:"stock_qty"})
    
    # ë²¡í„°í™”ëœ ì—°ì‚°ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
    result["date"] = pd.to_datetime(result["date"], errors="coerce").dt.normalize()
    result["center"] = result["center"].astype(str)
    result["resource_code"] = result["resource_code"].astype(str)
    result["stock_qty"] = pd.to_numeric(result["stock_qty"], errors="coerce").fillna(0).astype(int)
    
    return result.dropna(subset=["date","center","resource_code"])


def normalize_moves(df: pd.DataFrame) -> pd.DataFrame:
    # ì»¬ëŸ¼ëª… ì •ê·œí™” (in-place)
    df.columns = [str(c).strip() for c in df.columns]

    resource_code = _coalesce_columns(df, [["resource_code","ìƒí’ˆì½”ë“œ","RESOURCE_CODE","sku","SKU"]])
    qty_ea       = _coalesce_columns(df, [
        ["qty_ea","QTY_EA","ìˆ˜ëŸ‰(EA)","qty","QTY","quantity","Quantity","ìˆ˜ëŸ‰","EA","ea"]
    ])
    carrier_mode = _coalesce_columns(df, [["carrier_mode","ìš´ì†¡ë°©ë²•","carrier mode","ìš´ì†¡ìˆ˜ë‹¨"]])
    from_center  = _coalesce_columns(df, [["from_center","ì¶œë°œì°½ê³ ","from center"]])
    to_center    = _coalesce_columns(df, [["to_center","ë„ì°©ì°½ê³ ","to center"]])
    onboard_date = _coalesce_columns(df, [["onboard_date","ë°°ì •ì¼","ì¶œë°œì¼","H","onboard","depart_date"]], parse_date=True)
    arrival_date = _coalesce_columns(df, [["arrival_date","ë„ì°©ì¼","eta_date","ETA","arrival"]], parse_date=True)
    inbound_date = _coalesce_columns(df, [["inbound_date","ì…ê³ ì¼","ì…ê³ ì™„ë£Œì¼"]], parse_date=True)
    real_depart  = _coalesce_columns(df, [["ì‹¤ì œ ì„ ì ì¼","real_departure","AI"]], parse_date=True)

    out = pd.DataFrame({
        "resource_code": resource_code.astype(str).str.strip(),
        "qty_ea": pd.to_numeric(qty_ea.astype(str).str.replace(',', ''), errors="coerce").fillna(0).astype(int),
        "carrier_mode": carrier_mode.astype(str).str.strip(),
        "from_center": from_center.astype(str).str.strip(),
        "to_center": to_center.astype(str).str.strip(),
        "onboard_date": onboard_date,
        "arrival_date": arrival_date,
        "inbound_date": inbound_date,
        "real_departure": real_depart,
    })
    out["event_date"] = out["inbound_date"].where(out["inbound_date"].notna(), out["arrival_date"])
    
    # âœ… ëª¨ë“  ë‚ ì§œ ì»¬ëŸ¼ì— .dt.normalize() ì ìš© (ì´ë²¤íŠ¸/ìŠ¤ëƒ…ìƒ· ì¶• ì•ˆì •í™”)
    date_columns = ["onboard_date", "arrival_date", "inbound_date", "real_departure", "event_date"]
    for col in date_columns:
        if col in out.columns and out[col].dtype == 'datetime64[ns]':
            out[col] = out[col].dt.normalize()
    
    return out


def _parse_po_date(po_str: str) -> pd.Timestamp:
    """
    ì˜ˆ: 'T250812-0001' -> 2025-08-12
    ê·œì¹™: ì˜ë¬¸ 1ì + YYMMDD + '-' ...
    """
    if not isinstance(po_str, str):
        return pd.NaT
    m = re.search(r"[A-Za-z](\d{2})(\d{2})(\d{2})", po_str)
    if not m:
        return pd.NaT
    yy, mm, dd = m.groups()
    year = 2000 + int(yy)  # 20xx ê°€ì •
    try:
        return pd.Timestamp(datetime(year, int(mm), int(dd)))
    except Exception:
        return pd.NaT

def load_wip_from_incoming(df_incoming: Optional[pd.DataFrame], default_center: str = "íƒœê´‘KR") -> pd.DataFrame:
    if df_incoming is None or df_incoming.empty:
        return pd.DataFrame()

    df = df_incoming.copy()
    df.columns = [str(c).strip() for c in df.columns]
    low = {c.lower(): c for c in df.columns}

    # SKU
    sku_col = next((low[k] for k in ["product_code","resource_code","ìƒí’ˆì½”ë“œ","sku"] if k in low), None)

    # ìˆ˜ëŸ‰: ìˆ«ì íŒŒì‹± ì„±ê³µë¥  í™•ì¸
    qty_col = None
    for candidate in ["quantity","qty","ìˆ˜ëŸ‰","total_quantity","ì…ê³ ìˆ˜ëŸ‰"]:
        if candidate in low:
            s = pd.to_numeric(df[low[candidate]].astype(str).str.replace(",",""), errors="coerce")
            if s.notna().mean() >= 0.6:  # 60%+
                qty_col = low[candidate]; break

    # ë‚ ì§œ: ì˜ë¯¸ì–´ + ë‚ ì§œ íŒŒì‹± ì„±ê³µë¥  í™•ì¸, 'ìˆ˜ëŸ‰/qty' ë‹¨ì–´ í¬í•¨ ì»¬ëŸ¼ ì œì™¸
    date_col = None
    for kw in ["ì…ê³ ì˜ˆì •", "ì˜ˆì •ì¼", "ready", "push", "intended_push", "ì…ê³ ì¼", "eta"]:
        cands = [c for c in df.columns
                 if (kw.lower() in c.lower()) and not any(x in c.lower() for x in ["ìˆ˜ëŸ‰","qty","quantity"])]
        for c in cands:
            s = pd.to_datetime(df[c], errors="coerce")
            if s.notna().mean() >= 0.6:
                date_col = c; break
        if date_col: break

    # ë°œì£¼(PO)
    po_col = next((low[k] for k in ["po_no","ponumber","po"] if k in low), None)
    lot_col = next((low[k] for k in ["lot","ì œì¡°ë²ˆí˜¸","lot_no","lotnumber"] if k in low), None)

    if not all([sku_col, qty_col, date_col]):
        return pd.DataFrame()

    out = pd.DataFrame({
        "resource_code": df[sku_col].astype(str).str.strip(),
        "to_center": default_center,
        "wip_ready": pd.to_datetime(df[date_col], errors="coerce").dt.normalize(),
        "qty_ea": pd.to_numeric(df[qty_col].astype(str).str.replace(",",""), errors="coerce").fillna(0).astype(int),
        "lot": df[lot_col].astype(str).str.strip() if lot_col else ""
    })

    # wip_start: PO YYMMDD â†’ ì‹¤íŒ¨ ì‹œ wip_ready-30D
    def _parse_po_date(po):
        m = re.search(r"[A-Za-z](\d{2})(\d{2})(\d{2})", str(po))
        return pd.Timestamp(2000+int(m.group(1)), int(m.group(2)), int(m.group(3))) if m else pd.NaT

    out["wip_start"] = df[po_col].map(_parse_po_date) if po_col else pd.NaT
    m = out["wip_start"].isna() & out["wip_ready"].notna()
    out.loc[m, "wip_start"] = out.loc[m, "wip_ready"] - pd.to_timedelta(30, unit="D")

    return out.dropna(subset=["resource_code","wip_ready","wip_start"])[
        ["resource_code","to_center","wip_start","wip_ready","qty_ea","lot"]
    ]

def merge_wip_as_moves(moves_df: pd.DataFrame, wip_df: Optional[pd.DataFrame]) -> pd.DataFrame:
    if wip_df is None or wip_df.empty:
        return moves_df
    wip_moves = pd.DataFrame({
        "resource_code": wip_df["resource_code"],
        "qty_ea": wip_df["qty_ea"].astype(int),
        "carrier_mode": "WIP",
        "from_center": "WIP",
        "to_center": wip_df["to_center"],
        "onboard_date": wip_df["wip_start"],
        "arrival_date": wip_df["wip_ready"],
        "inbound_date": pd.NaT,
        "real_departure": pd.NaT,
        "event_date": wip_df["wip_ready"],
        "lot": wip_df.get("lot", "")
    })
    return pd.concat([moves_df, wip_moves], ignore_index=True)

# ===== ì†Œë¹„(ì†Œì§„) ì¶”ì„¸ + ì´ë²¤íŠ¸ ê°€ì¤‘ì¹˜ =====

@st.cache_data(ttl=3600)  # 1ì‹œê°„ ìºì‹œ (í•˜ë£¨ 1íšŒ ê°±ì‹ ì— ì í•©)
def estimate_daily_consumption(snap_long: pd.DataFrame,
                               centers_sel: List[str], skus_sel: List[str],
                               asof_dt: pd.Timestamp,
                               lookback_days: int = 28) -> Dict[Tuple[str, str], float]:
    """
    ìµœê·¼ lookback_days ë™ì•ˆ ìŠ¤ëƒ…ìƒ·ìœ¼ë¡œ SKUÃ—ì„¼í„°ë³„ ì¼ì¼ ì†Œì§„ëŸ‰(ê°œ/ì¼)ì„ ì¶”ì •.
    íšŒê·€ ê¸°ìš¸ê¸°ì™€ ê°ì†Œë¶„ í‰ê· ì˜ maxë¥¼ ì‚¬ìš©í•˜ì—¬ 'ì…ê³  í›„ í‰í‰' êµ¬ê°„ì—ì„œë„ ì•ˆì •ì  ì¶”ì •.
    """
    snap = snap_long.rename(columns={"snapshot_date":"date"}).copy()
    snap["date"] = pd.to_datetime(snap["date"], errors="coerce").dt.normalize()
    start = (asof_dt - pd.Timedelta(days=int(lookback_days)-1)).normalize()

    hist = snap[(snap["date"] >= start) & (snap["date"] <= asof_dt) &
                (snap["center"].isin(centers_sel)) &
                (snap["resource_code"].isin(skus_sel))]

    rates = {}
    if hist.empty: 
        return rates

    for (ct, sku), g in hist.groupby(["center","resource_code"]):
        ts = (g.sort_values("date")
                .set_index("date")["stock_qty"]
                .asfreq("D").ffill())
        # ê´€ì¸¡ ìµœì†Œ ë³´ì¥
        if ts.dropna().shape[0] < max(7, lookback_days//2):
            continue
        
        x = np.arange(len(ts), dtype=float)
        y = ts.values.astype(float)
        
        # ë°©ë²•1: íšŒê·€ ê¸°ìš¸ê¸°(ê°ì†Œë§Œ)
        rate_reg = max(0.0, -np.polyfit(x, y, 1)[0])
        
        # ë°©ë²•2: ì¼/ì¼ ê°ì†Œë¶„ í‰ê· 
        dec = (-np.diff(y)).clip(min=0)
        rate_dec = dec.mean() if len(dec) else 0.0
        
        # ë‘ ë°©ë²•ì˜ max ì‚¬ìš©
        rate = max(rate_reg, rate_dec)
        if rate > 0:
            rates[(ct, sku)] = float(rate)
    
    return rates

@st.cache_data(ttl=1800)  # 30ë¶„ ìºì‹œ
def apply_consumption_with_events(
    timeline: pd.DataFrame,
    snap_long: pd.DataFrame,
    centers_sel: List[str], skus_sel: List[str],
    start_dt: pd.Timestamp, end_dt: pd.Timestamp,
    lookback_days: int = 28,
    events: Optional[List[Dict]] = None
) -> pd.DataFrame:
    out = timeline.copy()
    if out.empty:
        return out
    out["date"] = pd.to_datetime(out["date"]).dt.normalize()

    # ë‚ ì§œ ì»¬ëŸ¼ ìë™ ê°ì§€ (date / snapshot_date ëª¨ë‘ ì§€ì›)
    snap_cols = {c.lower(): c for c in snap_long.columns}
    date_col = snap_cols.get("date") or snap_cols.get("snapshot_date")
    if date_col is None:
        raise KeyError("snap_longì—ëŠ” 'date' ë˜ëŠ” 'snapshot_date' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    latest_snap = pd.to_datetime(snap_long[date_col]).max().normalize()
    cons_start = max(latest_snap + pd.Timedelta(days=1), start_dt)
    if cons_start > end_dt:
        return out

    # ì´ë²¤íŠ¸ ê³„ìˆ˜ (ì—†ìœ¼ë©´ 1.0)
    idx = pd.date_range(cons_start, end_dt, freq="D")
    uplift = pd.Series(1.0, index=idx)
    if events:
        for e in events:
            s = pd.to_datetime(e.get("start"), errors="coerce")
            t = pd.to_datetime(e.get("end"), errors="coerce")
            u = min(3.0, max(-1.0, float(e.get("uplift", 0.0))))  # -100% ~ +300% ë°©ì–´
            if pd.notna(s) and pd.notna(t):
                s = s.normalize(); t = t.normalize()
                s = max(s, idx[0]); t = min(t, idx[-1])
                if s <= t:
                    uplift.loc[s:t] = uplift.loc[s:t] * (1.0 + u)

    # ì¼ì¼ ì†Œì§„ëŸ‰ ì¶”ì • (ë³´ê°• ë²„ì „: íšŒê·€ vs ê°ì†Œë¶„ í‰ê· ì˜ max)
    rates = estimate_daily_consumption(snap_long, centers_sel, skus_sel, latest_snap, int(lookback_days))

    chunks: list[pd.DataFrame] = []  # â† ë°˜ë“œì‹œ ë£¨í”„ ë°–ì—ì„œ ì´ˆê¸°í™”
    for (ct, sku), g in out.groupby(["center","resource_code"]):
        # ê°€ìƒ ë¼ì¸ì€ ì†Œì§„ ë¯¸ì ìš©
        if ct in ("In-Transit", "WIP"):
            chunks.append(g)
            continue

        rate = float(rates.get((ct, sku), 0.0))
        if rate <= 0:
            chunks.append(g)
            continue

        g = g.sort_values("date").copy()
        mask = g["date"] >= cons_start
        if not mask.any():
            chunks.append(g)
            continue

        daily = g.loc[mask, "date"].map(uplift).fillna(1.0).values * rate
        stk = g.loc[mask, "stock_qty"].astype(float).values
        # ëˆ„ì  ì°¨ê°(í•˜í•œ 0)
        for i in range(len(stk)):
            dec = daily[i]
            stk[i:] = np.maximum(0.0, stk[i:] - dec)
        g.loc[mask, "stock_qty"] = stk
        chunks.append(g)

    # ê·¸ë£¹ì´ ì—†ê±°ë‚˜ ì „ë¶€ ì œì™¸ë˜ì—ˆë‹¤ë©´ ì›ë³¸ ë°˜í™˜
    if not chunks:
        return out

    out = pd.concat(chunks, ignore_index=True)
    # ì†Œìˆ˜ ì •ë¦¬(í‘œì‹œ/ì¼ê´€ì„±)
    out["stock_qty"] = (
        pd.to_numeric(out["stock_qty"], errors="coerce")
        .round()
        .clip(lower=0)
        .astype(int)
    )
    return out




# @st.cache_data(ttl=1800)  # 30ë¶„ ìºì‹œ - ì„ì‹œ ë¹„í™œì„±í™”
def build_timeline(snap_long: pd.DataFrame, moves: pd.DataFrame, 
                   centers_sel: List[str], skus_sel: List[str],
                   start_dt: pd.Timestamp, end_dt: pd.Timestamp, 
                   horizon_days: int = 0, today: pd.Timestamp = None) -> pd.DataFrame:
    if today is None:
        today = pd.Timestamp.today().normalize()
    horizon_end = end_dt + pd.Timedelta(days=horizon_days)
    full_dates = pd.date_range(start_dt, horizon_end, freq="D")

    base = (snap_long[snap_long["center"].isin(centers_sel) & snap_long["resource_code"].isin(skus_sel)]
            .copy().rename(columns={"snapshot_date":"date"}))
    if base.empty:
        return pd.DataFrame(columns=["date","center","resource_code","stock_qty"])
    base = base[(base["date"] >= start_dt) & (base["date"] <= end_dt)]

    # ë‚ ì§œ ì¼ë‹¨ìœ„ ì •ê·œí™”
    mv = moves.copy()
    for c in ["onboard_date","arrival_date","inbound_date","event_date","real_departure"]:
        if c in mv.columns:
            mv[c] = pd.to_datetime(mv[c], errors="coerce").dt.normalize()

    lines = []

    # 1) ì‹¤ì œ ì„¼í„° ë¼ì¸
    for (ct, sku), grp in base.groupby(["center","resource_code"]):
        grp = grp.sort_values("date")
        last_dt = grp["date"].max()

        if horizon_days > 0:
            proj_dates = pd.date_range(last_dt + pd.Timedelta(days=1), horizon_end, freq="D")
            proj_df = pd.DataFrame({"date": proj_dates, "center": ct, "resource_code": sku, "stock_qty": np.nan})
            ts = pd.concat([grp[["date","center","resource_code","stock_qty"]], proj_df], ignore_index=True)
        else:
            ts = grp[["date","center","resource_code","stock_qty"]].copy()

        ts = ts.sort_values("date"); ts["stock_qty"] = ts["stock_qty"].ffill()

        mv_sku = mv[mv["resource_code"] == sku]

        eff_minus = (
            mv_sku[(mv_sku["from_center"].astype(str) == str(ct)) &
                   (mv_sku["onboard_date"].notna()) &
                   (mv_sku["onboard_date"] > last_dt)]
            .groupby("onboard_date", as_index=False)["qty_ea"].sum()
            .rename(columns={"onboard_date":"date","qty_ea":"delta"})
        )
        eff_minus["delta"] *= -1

        # âœ… WIP ì œì™¸ (WIPëŠ” ì „ìš© ë¸”ë¡ì—ì„œë§Œ ë”í•¨)
        eff_plus = (
            mv_sku[(mv_sku["to_center"].astype(str) == str(ct)) &
                   (mv_sku["event_date"].notna()) &
                   (mv_sku["event_date"] > last_dt) &
                   (~mv_sku["carrier_mode"].astype(str).str.upper().eq("WIP"))]
            .groupby("event_date", as_index=False)["qty_ea"].sum()
            .rename(columns={"event_date":"date","qty_ea":"delta"})
        )

        eff_all = pd.concat([eff_minus, eff_plus], ignore_index=True).sort_values("date")
        for d, delta in zip(eff_all["date"], eff_all["delta"]):
            ts.loc[ts["date"] >= d, "stock_qty"] = ts.loc[ts["date"] >= d, "stock_qty"] + delta

        # (ì˜µì…˜) WIP ì™„ë£Œë¶„ì„ ì´ ì„¼í„°ì— ê°€ì‚°
        wip_complete = mv_sku[(mv_sku["carrier_mode"].astype(str).str.upper()=="WIP") &
                              (mv_sku["to_center"].astype(str) == str(ct)) &
                              (mv_sku["event_date"].notna())]
        if not wip_complete.empty:
            wip_add = (wip_complete.groupby("event_date", as_index=False)["qty_ea"].sum()
                       .rename(columns={"event_date":"date","qty_ea":"delta"}))
            for d, delta in zip(wip_add["date"], wip_add["delta"]):
                ts.loc[ts["date"] >= d, "stock_qty"] = ts.loc[ts["date"] >= d, "stock_qty"] + delta

        ts["stock_qty"] = ts["stock_qty"].clip(lower=0)
        lines.append(ts)

    # 2) In-Transit ê°€ìƒ ë¼ì¸ (Nonâ€‘WIP / WIP)
    mv_sel = mv[mv["resource_code"].isin(skus_sel) &
                (mv["from_center"].isin(centers_sel) | mv["to_center"].isin(centers_sel) |
                 mv["carrier_mode"].astype(str).str.upper().eq("WIP"))]

    for sku, g in mv_sel.groupby("resource_code"):
        # Nonâ€‘WIP In-Transit
        g_nonwip = g[~g["carrier_mode"].astype(str).str.upper().eq("WIP")]
        if not g_nonwip.empty:
            g_selected = g_nonwip[g_nonwip["to_center"].isin(centers_sel)]
            if not g_selected.empty:
                s = pd.Series(0, index=pd.to_datetime(full_dates))
                for _, row in g_selected.iterrows():
                    ob = row["onboard_date"]; ev = row["event_date"]; qty = row["qty_ea"]
                    if pd.notna(ob):
                        if pd.notna(ev):
                            mask = (s.index >= ob) & (s.index < ev)
                        else:
                            mask = (s.index >= ob) & (s.index <= min(today, full_dates[-1]))
                        s.loc[mask] = s.loc[mask] + qty
                s = s[s > 0]
                if not s.empty:
                    lines.append(pd.DataFrame({"date": s.index, "center": "In-Transit", "resource_code": sku, "stock_qty": s.values}))

        # WIP ë¼ì¸
        g_wip = g[g["carrier_mode"].astype(str).str.upper().eq("WIP")]
        if not g_wip.empty:
            add_onboard = (g_wip[g_wip["onboard_date"].notna()]
                           .groupby("onboard_date", as_index=False)["qty_ea"].sum()
                           .rename(columns={"onboard_date":"date","qty_ea":"delta"}))
            add_event = (g_wip[g_wip["event_date"].notna()]
                         .groupby("event_date", as_index=False)["qty_ea"].sum()
                         .rename(columns={"event_date":"date","qty_ea":"delta"}))
            add_event["delta"] *= -1
            deltas = pd.concat([add_onboard, add_event], ignore_index=True)
            if not deltas.empty:
                s = pd.Series(0, index=pd.to_datetime(full_dates))
                for d, v in deltas.groupby("date")["delta"].sum().items():
                    if d < full_dates[0]:
                        s.iloc[:] = s.iloc[:] + v
                    else:
                        s.loc[s.index >= d] = s.loc[s.index >= d] + v
                lines.append(pd.DataFrame({"date": s.index, "center": "WIP", "resource_code": sku, "stock_qty": s.values.clip(min=0)}))

    if not lines:
        return pd.DataFrame(columns=["date","center","resource_code","stock_qty"])
    out = pd.concat(lines, ignore_index=True)
    return out[(out["date"] >= start_dt) & (out["date"] <= horizon_end)]

# -------------------- Tabs for inputs --------------------
tab1, tab2, tab3 = st.tabs(["ì—‘ì…€ ì—…ë¡œë“œ", "CSV ìˆ˜ë™ ì—…ë¡œë“œ", "Google Sheets"])

with tab1:
    xfile = st.file_uploader("ì—‘ì…€ ì—…ë¡œë“œ (.xlsx)", type=["xlsx"], key="excel")
    if xfile is not None:
        df_move, df_ref, df_incoming = load_from_excel(xfile)
        moves_raw = normalize_moves(df_move)
        snap_long = normalize_refined_snapshot(df_ref)


        # WIP ë¶ˆëŸ¬ì˜¤ê¸° ë° ë³‘í•©
        try:
            wip_df = load_wip_from_incoming(df_incoming)
            moves = merge_wip_as_moves(moves_raw, wip_df)
            st.success(f"WIP {len(wip_df)}ê±´ ë°˜ì˜ ì™„ë£Œ" if wip_df is not None and not wip_df.empty else "WIP ì—†ìŒ")
        except Exception as e:
            moves = moves_raw
            st.warning(f"WIP ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")

with tab2:
    cs_snap = st.file_uploader("ì •ì œ ìŠ¤ëƒ…ìƒ· CSV ì—…ë¡œë“œ (snap_ì •ì œ: date,center,resource_code,stock_qty)", type=["csv"], key="snapcsv")
    cs_move = st.file_uploader("SCM_í†µí•©.csv ì—…ë¡œë“œ", type=["csv"], key="movecsv")
    if cs_snap is not None and cs_move is not None:
        df_ref = pd.read_csv(cs_snap)
        move_raw = pd.read_csv(cs_move)

        # ì´ë™ ë°ì´í„° ì •ê·œí™”
        moves = normalize_moves(move_raw)

        # ì •ì œ ìŠ¤ëƒ…ìƒ· ì •ê·œí™” (date/center/resource_code/stock_qtyë¡œ í†µì¼)
        snap_cols = {c.strip().lower(): c for c in df_ref.columns}

        # ìœ ì—° ë§¤í•‘
        col_date = snap_cols.get("date") or snap_cols.get("snapshot_date") or snap_cols.get("ìŠ¤ëƒ…ìƒ· ì¼ì")
        col_center = snap_cols.get("center") or snap_cols.get("ì°½ê³ ëª…")
        col_sku = snap_cols.get("resource_code") or snap_cols.get("sku") or snap_cols.get("ìƒí’ˆì½”ë“œ")
        col_qty = (snap_cols.get("stock_qty") or snap_cols.get("qty") or
                   snap_cols.get("quantity") or snap_cols.get("ìˆ˜ëŸ‰"))

        if not all([col_date, col_center, col_sku, col_qty]):
            st.error("ì •ì œ ìŠ¤ëƒ…ìƒ· CSVì— 'date, center, resource_code, stock_qty' ì»¬ëŸ¼(ë˜ëŠ” ë™ì˜ì–´)ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            st.stop()

        sr = df_ref.rename(columns={
            col_date: "date",
            col_center: "center",
            col_sku: "resource_code",
            col_qty: "stock_qty",
        }).copy()

        sr["date"] = pd.to_datetime(sr["date"], errors="coerce").dt.normalize()
        sr["center"] = sr["center"].astype(str)
        sr["resource_code"] = sr["resource_code"].astype(str)
        sr["stock_qty"] = pd.to_numeric(sr["stock_qty"], errors="coerce").fillna(0).astype(int)

        snap_long = sr[["date","center","resource_code","stock_qty"]].dropna()

with tab3:
    st.subheader("Google Sheets ì—°ë™")
    st.info("Google Sheetsì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    
    # ë°ì´í„° ë¡œë” ì •ë³´ í‘œì‹œ
    show_data_loader_info()
    
    if st.button("Google Sheetsì—ì„œ ë°ì´í„° ë¡œë“œ", type="primary"):
        try:
            with st.spinner("Google Sheetsì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
                df_move, df_ref, df_incoming = load_from_gsheet()
                moves_raw = normalize_moves(df_move)
                snap_long = normalize_refined_snapshot(df_ref)
                
                # WIP ë¶ˆëŸ¬ì˜¤ê¸° ë° ë³‘í•©
                try:
                    wip_df = load_wip_from_incoming(df_incoming)
                    moves = merge_wip_as_moves(moves_raw, wip_df)
                    st.success(f"Google Sheets ë¡œë“œ ì™„ë£Œ! WIP {len(wip_df)}ê±´ ë°˜ì˜" if wip_df is not None and not wip_df.empty else "Google Sheets ë¡œë“œ ì™„ë£Œ! WIP ì—†ìŒ")
                except Exception as e:
                    moves = moves_raw
                    st.warning(f"WIP ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                    
        except Exception as e:
            st.error(f"Google Sheets ë¡œë“œ ì‹¤íŒ¨: {e}")
            st.caption("Google Sheets IDì™€ ì‹œíŠ¸ëª…ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

# ì•± ì‹œì‘ ì‹œ Google Sheetsì—ì„œ ìë™ ë¡œë“œ ì‹œë„
if "snap_long" not in locals():
    try:
        with st.spinner("Google Sheetsì—ì„œ ë°ì´í„°ë¥¼ ìë™ ë¡œë“œí•˜ëŠ” ì¤‘..."):
            df_move, df_ref, df_incoming = load_from_gsheet()
            moves_raw = normalize_moves(df_move)
            snap_long = normalize_refined_snapshot(df_ref)
            
            # WIP ë¶ˆëŸ¬ì˜¤ê¸° ë° ë³‘í•©
            try:
                wip_df = load_wip_from_incoming(df_incoming)
                moves = merge_wip_as_moves(moves_raw, wip_df)
                st.success(f"âœ… Google Sheets ìë™ ë¡œë“œ ì™„ë£Œ! WIP {len(wip_df)}ê±´ ë°˜ì˜" if wip_df is not None and not wip_df.empty else "âœ… Google Sheets ìë™ ë¡œë“œ ì™„ë£Œ! WIP ì—†ìŒ")
            except Exception as e:
                moves = moves_raw
                st.warning(f"âš ï¸ WIP ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                
    except Exception as e:
        st.error(f"âŒ Google Sheets ìë™ ë¡œë“œ ì‹¤íŒ¨: {e}")
        st.info("ì—‘ì…€, CSV ë˜ëŠ” Google Sheetsì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ë©´ í•„í„°/ì°¨íŠ¸ê°€ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.")
        st.stop()

# -------------------- ë””ë²„ê·¸ íŒ¨ë„ --------------------
with st.expander("ğŸ”§ ë””ë²„ê·¸(ë°ì´í„° ì ê²€)", expanded=False):
    st.write("**ê¸°ë³¸ ë°ì´í„° ìƒíƒœ:**")
    st.write(f"- moves: {moves.shape if 'moves' in locals() else 'N/A'}")
    st.write(f"- snap_long: {snap_long.shape if 'snap_long' in locals() else 'N/A'}")
    
    if 'snap_long' in locals() and not snap_long.empty:
        latest_dt = snap_long["date"].max()
        st.write(f"- ìµœì‹  ìŠ¤ëƒ…ìƒ·: {latest_dt}")
        st.write(f"- ìŠ¤ëƒ…ìƒ· ì„¼í„°: {sorted(snap_long['center'].unique())}")
        st.write(f"- ìŠ¤ëƒ…ìƒ· SKU: {len(snap_long['resource_code'].unique())}ê°œ")
    
    if 'moves' in locals() and not moves.empty:
        st.write("**ì´ë™ ë°ì´í„° ìƒíƒœ:**")
        st.write(f"- ì´ ì´ë™ ê±´ìˆ˜: {len(moves)}")
        st.write(f"- ì´ë™ ì„¼í„°: {sorted(moves['from_center'].unique())} â†’ {sorted(moves['to_center'].unique())}")
        st.write(f"- ì´ë™ ëª¨ë“œ: {sorted(moves['carrier_mode'].unique())}")
        
        # WIP ë°ì´í„° í™•ì¸
        wip_data = moves[moves["carrier_mode"] == "WIP"]
        if not wip_data.empty:
            st.write("**WIP ë°ì´í„° ìƒ˜í”Œ:**")
            st.write(wip_data[["resource_code", "qty_ea", "onboard_date", "event_date", "to_center"]].head(5))
        else:
            st.write("**WIP ë°ì´í„°: ì—†ìŒ**")
        
        # ì»¬ëŸ¼ í™•ì¸
        st.write("**moves ì»¬ëŸ¼:**", list(moves.columns))
        
        # eta_date ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€
        if "eta_date" in moves.columns:
            st.write("âœ… eta_date ì»¬ëŸ¼ ì¡´ì¬")
        else:
            st.write("âŒ eta_date ì»¬ëŸ¼ ì—†ìŒ")
            st.write("ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ì»¬ëŸ¼:", [c for c in moves.columns if 'date' in c.lower()])

# -------------------- Filters --------------------
# ì„¼í„° ëª©ë¡: ìŠ¤ëƒ…ìƒ· + ì´ë™ ë°ì´í„°ì—ì„œ ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì œê±°)
centers_snap = set(snap_long["center"].dropna().astype(str).unique().tolist())
centers_moves = set(moves["from_center"].dropna().astype(str).unique().tolist() + 
                   moves["to_center"].dropna().astype(str).unique().tolist())

# ì„¼í„°ëª… í†µì¼ í•¨ìˆ˜
def normalize_center_name(center):
    """ì„¼í„°ëª…ì„ í†µì¼ëœ í˜•íƒœë¡œ ë³€í™˜"""
    if center in ["", "nan", "None", "WIP", "In-Transit"]:
        return None
    # AcrossBUSì™€ ì–´í¬ë¡œìŠ¤ë¹„USë¥¼ ì–´í¬ë¡œìŠ¤ë¹„USë¡œ í†µì¼
    if center in ["AcrossBUS", "ì–´í¬ë¡œìŠ¤ë¹„US"]:
        return "ì–´í¬ë¡œìŠ¤ë¹„US"
    return center

# ëª¨ë“  ì„¼í„°ë¥¼ í†µì¼ëœ í˜•íƒœë¡œ ë³€í™˜
all_centers = set()
for center in centers_snap:
    normalized = normalize_center_name(center)
    if normalized:
        all_centers.add(normalized)

for center in centers_moves:
    normalized = normalize_center_name(center)
    if normalized:
        all_centers.add(normalized)

centers = sorted(list(all_centers))

skus = sorted(snap_long["resource_code"].dropna().astype(str).unique().tolist())
min_date = snap_long["date"].min()
max_date = snap_long["date"].max()

st.sidebar.header("í•„í„°")
centers_sel = st.sidebar.multiselect("ì„¼í„° ì„ íƒ", centers, default=(["íƒœê´‘KR"] if "íƒœê´‘KR" in centers else centers[:1]))
skus_sel = st.sidebar.multiselect("SKU ì„ íƒ", skus, default=([s for s in ["BA00022","BA00021"] if s in skus] or skus[:2]))


# === ê¸°ê°„ ì œì–´: ê³¼ê±° 42ì¼, ë¯¸ë˜ 60ì¼ ===
today = pd.Timestamp.today().normalize()
PAST_DAYS = 42  # ê³¼ê±° 42ì¼
FUTURE_DAYS = 60  # ë¯¸ë˜ 60ì¼

# ìŠ¤ëƒ…ìƒ· ë²”ìœ„ì™€ êµì°¨(ìŠ¤ëƒ…ìƒ·ì´ ë” ì§§ì•„ë„ ì•ˆì „)
snap_min = pd.to_datetime(snap_long["date"]).min().normalize()
snap_max = pd.to_datetime(snap_long["date"]).max().normalize()

bound_min = max(today - pd.Timedelta(days=PAST_DAYS), snap_min)
bound_max = min(today + pd.Timedelta(days=FUTURE_DAYS), snap_max + pd.Timedelta(days=60))  # +60ì€ ì•½ê°„ì˜ ì „ë§ ì—¬ì§€

def _init_range():
    if "date_range" not in st.session_state:
        st.session_state.date_range = (max(today - pd.Timedelta(days=20), bound_min),
                                       min(today + pd.Timedelta(days=20), bound_max))
    if "horizon_days" not in st.session_state:
        st.session_state.horizon_days = 20

def _apply_horizon_to_range():
    h = int(st.session_state.horizon_days)
    h = max(0, min(h, FUTURE_DAYS))   # â† ì…ë ¥ ìµœëŒ€ë¥¼ 60ì¼ë¡œ í´ë¨í”„
    st.session_state.horizon_days = h
    start = max(today - pd.Timedelta(days=h), bound_min)
    end   = min(today + pd.Timedelta(days=h), bound_max)
    st.session_state.date_range = (start, end)

def _clamp_range(r):
    s, e = pd.Timestamp(r[0]).normalize(), pd.Timestamp(r[1]).normalize()
    s = max(min(s, bound_max), bound_min)
    e = max(min(e, bound_max), bound_min)
    if e < s:
        e = s
    return (s, e)

_init_range()

st.sidebar.subheader("ê¸°ê°„ ì„¤ì •")
st.sidebar.number_input(
    "ë¯¸ë˜ ì „ë§ ì¼ìˆ˜",
    min_value=0, max_value=FUTURE_DAYS, step=1,
    key="horizon_days", on_change=_apply_horizon_to_range
)

# ìŠ¬ë¼ì´ë”: ë²”ìœ„ë¥¼ Â±6ì£¼ ê²½ê³„ë¡œ ì œí•œ
date_range = st.sidebar.slider(
    "ê¸°ê°„",
    min_value=bound_min.to_pydatetime(),
    max_value=bound_max.to_pydatetime(),
    value=tuple(d.to_pydatetime() for d in _clamp_range(st.session_state.date_range)),
    format="YYYY-MM-DD"
)

# ë‚´ë¶€ ì‚¬ìš© ê¸°ê°„(ìŠ¬ë¼ì´ë” ìš°ì„ )
start_dt = pd.Timestamp(date_range[0]).normalize()
end_dt   = pd.Timestamp(date_range[1]).normalize()

# ì „ë§ì¼(ë¹Œë“œìš©): ìµœì‹  ìŠ¤ëƒ…ìƒ· ì´í›„ë§Œ ê³„ì‚°
_latest_snap = snap_long["date"].max()
proj_days_for_build = max(0, int((end_dt - _latest_snap).days))



st.sidebar.header("í‘œì‹œ ì˜µì…˜")

# í‘œì‹œ í† ê¸€ (ìˆœì„œ/ë¬¸êµ¬ ì •ë¦¬)
show_prod = st.sidebar.checkbox("ìƒì‚°ì¤‘(ë¯¸ì™„ë£Œ) í‘œì‹œ", value=True)
show_transit = st.sidebar.checkbox("ì´ë™ì¤‘ í‘œì‹œ", value=True)

# ì¶”ì„¸ ê¸°ë°˜ ì¬ê³  ì˜ˆì¸¡
use_cons_forecast = st.sidebar.checkbox("ì¶”ì„¸ ê¸°ë°˜ ì¬ê³  ì˜ˆì¸¡", value=True)
lookback_days = st.sidebar.number_input("ì¶”ì„¸ ê³„ì‚° ê¸°ê°„(ì¼)", min_value=7, max_value=56, value=28, step=7)

# í”„ë¡œëª¨ì…˜ ê°€ì¤‘ì¹˜ (ë‹¨ì¼)
with st.sidebar.expander("í”„ë¡œëª¨ì…˜ ê°€ì¤‘ì¹˜(+%)", expanded=False):
    enable_event = st.checkbox("ê°€ì¤‘ì¹˜ ì ìš©", value=False)
    ev_start = st.date_input("ì‹œì‘ì¼")
    ev_end   = st.date_input("ì¢…ë£Œì¼")
    ev_pct   = st.number_input("ê°€ì¤‘ì¹˜(%)", min_value=-100.0, max_value=300.0, value=30.0, step=5.0)
events = [{"start": pd.Timestamp(ev_start).strftime("%Y-%m-%d"),
           "end":   pd.Timestamp(ev_end).strftime("%Y-%m-%d"),
           "uplift": ev_pct/100.0}] if enable_event else []



# -------------------- KPIs --------------------
st.subheader("ìš”ì•½ KPI")

# ìŠ¤ëƒ…ìƒ· ê¸°ì¤€ ìµœì‹ ì¼
latest_dt = snap_long["date"].max()
latest_dt_str = pd.to_datetime(latest_dt).strftime("%Y-%m-%d")

# SKUë³„ í˜„ì¬ ì¬ê³ (ì„ íƒ ì„¼í„° ê¸°ì¤€, ìµœì‹  ìŠ¤ëƒ…ìƒ·)
latest_rows = snap_long[(snap_long["date"] == latest_dt) & (snap_long["center"].isin(centers_sel))]
if latest_rows.empty:
    st.warning("ì„ íƒëœ ì„¼í„°ì— í•´ë‹¹í•˜ëŠ” ìµœì‹  ìŠ¤ëƒ…ìƒ· ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    sku_totals = {sku: 0 for sku in skus_sel}
else:
    sku_totals = {sku: int(latest_rows[latest_rows["resource_code"]==sku]["stock_qty"].sum()) for sku in skus_sel}

# In-Transit (WIP ì œì™¸, ì…/ì¶œê³  ëª¨ë‘ í¬í•¨)
today = pd.Timestamp.today().normalize()
# normalize_movesì—ì„œ ì´ë¯¸ astype(str) ì²˜ë¦¬ë¨, carrier_modeë§Œ upper() ì ìš©
moves_typed = moves.copy()
moves_typed["carrier_mode"] = moves_typed["carrier_mode"].str.upper()

in_transit_mask = (
    (moves_typed["onboard_date"].notna()) &
    (moves_typed["onboard_date"] <= today) &
    (moves_typed["inbound_date"].isna()) &
    ((moves_typed["arrival_date"].isna()) | (moves_typed["arrival_date"] > today)) &
    (moves_typed["to_center"].isin(centers_sel)) &  # ì„ íƒëœ ì„¼í„°ë¡œ ì´ë™ì¤‘ì¸ ì¬ê³ ë§Œ
    (moves_typed["resource_code"].isin(skus_sel)) &
    (moves_typed["carrier_mode"] != "WIP")
)
in_transit_total = int(moves_typed[in_transit_mask]["qty_ea"].sum())

# WIP(ì˜¤ëŠ˜ ê¸°ì¤€ ì”ëŸ‰, ì„ íƒ ì„¼í„°/SKU ë²”ìœ„)
wip_moves = moves_typed[
    (moves_typed["carrier_mode"] == "WIP") &
    (moves_typed["to_center"].isin(centers_sel)) &
    (moves_typed["resource_code"].isin(skus_sel))
]
if not wip_moves.empty:
    on = (wip_moves.dropna(subset=["onboard_date"]).groupby("onboard_date", as_index=True)["qty_ea"].sum())
    ev = (wip_moves.dropna(subset=["event_date"]).groupby("event_date", as_index=True)["qty_ea"].sum() * -1)
    wip_flow = pd.concat([on, ev]).groupby(level=0).sum().sort_index()
    wip_cum = wip_flow[wip_flow.index <= today].cumsum()
    wip_today = int(wip_cum.iloc[-1]) if not wip_cum.empty else 0
else:
    wip_today = 0

# SKUë³„ í˜„ì¬ ì¬ê³  ì¹´ë“œ
def _chunk(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i+n]

for group in _chunk(skus_sel, 4):
    cols = st.columns(len(group))
    for i, sku in enumerate(group):
        cols[i].metric(f"{sku} í˜„ì¬ ì¬ê³ (ìŠ¤ëƒ…ìƒ· {latest_dt_str})", f"{sku_totals.get(sku, 0):,}")

# í†µí•© KPI
k_it, k_wip = st.columns(2)
k_it.metric("ì´ë™ ì¤‘ ì¬ê³ ", f"{in_transit_total:,}")
k_wip.metric("ìƒì‚°ì¤‘(ë¯¸ì™„ë£Œ)", f"{wip_today:,}")

st.divider()

# -------------------- Step Chart (Plotly) --------------------
st.subheader("ê³„ë‹¨ì‹ ì¬ê³  íë¦„")
timeline = build_timeline(snap_long, moves, centers_sel, skus_sel,
                          start_dt, end_dt, horizon_days=proj_days_for_build, today=today)

# (A) ì†Œì§„ ì¶”ì„¸ + ì´ë²¤íŠ¸ ê°€ì¤‘ì¹˜ ì ìš©
if use_cons_forecast and not timeline.empty:
    timeline = apply_consumption_with_events(
        timeline, snap_long, centers_sel, skus_sel,
        start_dt, end_dt,
        lookback_days=int(lookback_days),
        events=events
    )

if timeline.empty:
    st.info("ì„ íƒ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” íƒ€ì„ë¼ì¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    vis_df = timeline[(timeline["date"] >= start_dt) & (timeline["date"] <= end_dt)].copy()

    # (B) ì„¼í„° í‘œì‹œ ëª…ì¹­ í•œê¸€í™” + í† ê¸€
    # In-Transit â†’ ì´ë™ì¤‘ (í†µí•©)
    vis_df["center"] = vis_df["center"].str.replace(r"^In-Transit.*$", "ì´ë™ì¤‘", regex=True)
    # WIP â†’ ìƒì‚°ì¤‘
    vis_df.loc[vis_df["center"] == "WIP", "center"] = "ìƒì‚°ì¤‘"

    # âœ… íƒœê´‘KR ë¯¸ì„ íƒ ì‹œ 'ìƒì‚°ì¤‘' ë¼ì¸ ìˆ¨ê¹€
    if "íƒœê´‘KR" not in centers_sel:
        vis_df = vis_df[vis_df["center"] != "ìƒì‚°ì¤‘"]
    
    if not show_prod:
        vis_df = vis_df[vis_df["center"] != "ìƒì‚°ì¤‘"]
    if not show_transit:
        vis_df = vis_df[~vis_df["center"].str.startswith("ì´ë™ì¤‘")]
    
    # 0 ê°’ ë°ì´í„° í•„í„°ë§ (ì°¨íŠ¸ í‘œì‹œ ë¬¸ì œ í•´ê²°)
    vis_df = vis_df[vis_df["stock_qty"] > 0]

    # ë¼ë²¨
    vis_df["label"] = vis_df["resource_code"] + " @ " + vis_df["center"]

    # (C) ê·¸ë¦¬ê¸°
    fig = px.line(
        vis_df, x="date", y="stock_qty", color="label",
        line_shape="hv",
        title="ì„ íƒí•œ SKU Ã— ì„¼í„°(ë° ì´ë™ì¤‘/ìƒì‚°ì¤‘) ê³„ë‹¨ì‹ ì¬ê³  íë¦„",
        render_mode="svg"
    )
    fig.update_layout(
        hovermode="x unified",
        xaxis_title="ë‚ ì§œ",
        yaxis_title="ì¬ê³ ëŸ‰(EA)",
        legend_title_text="SKU @ Center / ì´ë™ì¤‘(ì ì„ ) / ìƒì‚°ì¤‘(ì ì„ )",
        margin=dict(l=20, r=20, t=60, b=20)
    )

    # === ì˜¤ëŠ˜ ê¸°ì¤€ì„  í‘œì‹œ ===
    today = pd.Timestamp.today().normalize()
    if start_dt <= today <= end_dt:
        fig.add_vline(
            x=today,
            line_width=1,
            line_dash="solid",  # ì‹¤ì„ ìœ¼ë¡œ ë³€ê²½
            line_color="rgba(255, 0, 0, 0.4)",  # ë” í¬ë¯¸í•œ ë°˜íˆ¬ëª… ë¹¨ê°„ìƒ‰
        )
        fig.add_annotation(
            x=today,
            y=1.02,
            xref="x",
            yref="paper",
            text="ì˜¤ëŠ˜",
            showarrow=False,
            font=dict(size=12, color="#555"),
            align="center",
            yanchor="bottom",
        )


    
    # Yì¶• ëˆˆê¸ˆ ì •ìˆ˜ë¡œ
    fig.update_yaxes(tickformat=",.0f")
    # í˜¸ë²„ë„ ì •ìˆ˜ ì²œë‹¨ìœ„
    fig.update_traces(hovertemplate="ë‚ ì§œ: %{x|%Y-%m-%d}<br>ì¬ê³ : %{y:,.0f} EA<br>%{fullData.name}<extra></extra>")


    # (D) ìƒ‰ìƒ/ì„  ìŠ¤íƒ€ì¼ - ê° ë¼ì¸ë§ˆë‹¤ ë‹¤ë¥¸ ìƒ‰ìƒ ì‚¬ìš©
    PALETTE = [
        "#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd",
        "#8c564b", "#e377c2", "#7f7f7f", "#bcbd22", "#17becf",
        "#aec7e8", "#ffbb78", "#98df8a", "#ff9896", "#c5b0d5",
        "#c49c94", "#f7b6d3", "#c7c7c7", "#dbdb8d", "#9edae5",
        "#4e79a7", "#f28e2b", "#e15759", "#76b7b2", "#59a14f",
        "#edc948", "#b07aa1", "#ff9da7", "#9c755f", "#bab0ac"
    ]
    
    # ê° ë¼ì¸ë§ˆë‹¤ ê³ ìœ í•œ ìƒ‰ìƒ í• ë‹¹
    line_colors = {}
    color_idx = 0
    for tr in fig.data:
        name = tr.name or ""
        if " @ " in name:
            if name not in line_colors:
                line_colors[name] = PALETTE[color_idx % len(PALETTE)]
                color_idx += 1

    for i, tr in enumerate(fig.data):
        name = tr.name or ""
        if " @ " not in name:
            continue
        sku, kind = name.split(" @ ", 1)
        line_color = line_colors.get(name, PALETTE[0])

        if kind == "ì´ë™ì¤‘":
            # ì ì„  + ë‘ê»˜ 1.2
            fig.data[i].update(line=dict(color=line_color, dash="dot", width=1.2), opacity=0.9)
            fig.data[i].legendgroup = f"{sku} (ì´ë™ì¤‘)"
            fig.data[i].legendrank = 20
        elif kind == "ìƒì‚°ì¤‘":
            # íŒŒì„  + ë‘ê»˜ 1.0
            fig.data[i].update(line=dict(color=line_color, dash="dash", width=1.0), opacity=0.8)
            fig.data[i].legendgroup = f"{sku} (ìƒì‚°ì¤‘)"
            fig.data[i].legendrank = 30
        else:
            # ì‹¤ì„  + ë‘ê»˜ 1.5
            fig.data[i].update(line=dict(color=line_color, dash="solid", width=1.5), opacity=1.0)
            fig.data[i].legendgroup = f"{sku} (ì„¼í„°)"
            fig.data[i].legendrank = 10


    chart_key = (
        f"stepchart|centers={','.join(centers_sel)}|skus={','.join(skus_sel)}|"
        f"{start_dt:%Y%m%d}-{end_dt:%Y%m%d}"
        f"|h{int(st.session_state.horizon_days)}"
        f"|prod{int(show_prod)}|tran{int(show_transit)}"
    )

    st.plotly_chart(fig, use_container_width=True, config={"displaylogo": False}, key=chart_key)



# -------------------- Upcoming Arrivals --------------------
st.subheader("ì…ê³  ì˜ˆì • ë‚´ì—­ (ì„ íƒ ì„¼í„°/SKU)")

today = pd.Timestamp.today().normalize()
window_start = start_dt  # âœ… ì„ íƒëœ ê¸°ê°„ ì „ì²´ í¬í•¨ (ê³¼ê±°ë„ í¬í•¨)
window_end   = end_dt

# (A) ìš´ì†¡(ë¹„ WIP) - ì„ íƒëœ ì„¼í„°ë¡œ ì´ë™í•˜ëŠ” ëª¨ë“  ê±´
# event_dateê°€ ì—†ìœ¼ë©´ arrival_dateë‚˜ eta_date ì‚¬ìš©
arr_transport_conditions = [
    (moves_typed["carrier_mode"] != "WIP"),
    (moves_typed["to_center"].isin(centers_sel)),
    (moves_typed["resource_code"].isin(skus_sel))
]

# ë‚ ì§œ ì¡°ê±´ ì¶”ê°€ (ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸)
date_conditions = []
# event_dateê°€ ìˆê³  ê¸°ê°„ ë‚´ì¸ ê²½ìš°
date_conditions.append(
    (moves_typed["event_date"].notna()) & 
    (moves_typed["event_date"] >= window_start) & 
    (moves_typed["event_date"] <= window_end)
)

# event_dateê°€ ì—†ì§€ë§Œ arrival_dateê°€ ìˆê³  ê¸°ê°„ ë‚´ì¸ ê²½ìš°
if "arrival_date" in moves_typed.columns:
    date_conditions.append(
        (moves_typed["event_date"].isna()) & 
        (moves_typed["arrival_date"].notna()) & 
        (moves_typed["arrival_date"] >= window_start) & 
        (moves_typed["arrival_date"] <= window_end)
    )

# event_date, arrival_dateê°€ ì—†ì§€ë§Œ eta_dateê°€ ìˆê³  ê¸°ê°„ ë‚´ì¸ ê²½ìš°
if "eta_date" in moves_typed.columns:
    arrival_isna = moves_typed["arrival_date"].isna() if "arrival_date" in moves_typed.columns else True
    eta_notna = moves_typed["eta_date"].notna()
    eta_in_range = (moves_typed["eta_date"] >= window_start) & (moves_typed["eta_date"] <= window_end)
    date_conditions.append(
        (moves_typed["event_date"].isna()) & 
        arrival_isna & 
        eta_notna & 
        eta_in_range
    )

# ëª¨ë“  ì¡°ê±´ ê²°í•©
if date_conditions:
    date_condition = date_conditions[0]
    for cond in date_conditions[1:]:
        date_condition = date_condition | cond
    arr_transport_conditions.append(date_condition)

arr_transport = moves_typed[arr_transport_conditions[0]]
for cond in arr_transport_conditions[1:]:
    arr_transport = arr_transport[cond]

# (B) WIP - íƒœê´‘KR ì„ íƒ ì‹œ
arr_wip = pd.DataFrame()
if "íƒœê´‘KR" in centers_sel:
    arr_wip = moves_typed[
        (moves_typed["event_date"].notna()) &
        (moves_typed["event_date"] >= window_start) & (moves_typed["event_date"] <= window_end) &
        (moves_typed["carrier_mode"] == "WIP") &
        (moves_typed["to_center"] == "íƒœê´‘KR") &
        (moves_typed["resource_code"].isin(skus_sel))
    ]

upcoming = pd.concat([arr_transport, arr_wip], ignore_index=True)

# ì…ê³  ì˜ˆì • ë‚´ì—­ ìš”ì•½
if not upcoming.empty:
    st.info(f"ğŸ“Š ì´ {len(upcoming)}ê±´ì˜ ì…ê³  ì˜ˆì • ë‚´ì—­ì´ ìˆìŠµë‹ˆë‹¤.")
else:
    st.info("ğŸ“Š ì„ íƒëœ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ì…ê³  ì˜ˆì • ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤.")

if upcoming.empty:
    st.caption("ë„ì°© ì˜ˆì • ì—†ìŒ (ì˜¤ëŠ˜ ì´í›„ / ì„ íƒ ê¸°ê°„)")
else:
    # âœ… days_to_arrivalëŠ” í•­ìƒ 0 ì´ìƒ
    # event_dateê°€ ì—†ìœ¼ë©´ arrival_dateë‚˜ eta_date ì‚¬ìš©
    display_date = upcoming["event_date"]
    if "arrival_date" in upcoming.columns:
        display_date = display_date.fillna(upcoming["arrival_date"])
    if "eta_date" in upcoming.columns:
        display_date = display_date.fillna(upcoming["eta_date"])
    upcoming["display_date"] = display_date
    upcoming["days_to_arrival"] = (upcoming["display_date"] - today).dt.days
    upcoming = upcoming.sort_values(["display_date","to_center","resource_code","qty_ea"], 
                                   ascending=[True,True,True,False])
    cols = ["display_date","days_to_arrival","to_center","resource_code","qty_ea","carrier_mode","onboard_date","lot"]
    cols = [c for c in cols if c in upcoming.columns]
    st.dataframe(upcoming[cols].head(1000), use_container_width=True, height=300)


# -------------------- Simulation --------------------
st.subheader("ì¶œê³  ê°€ëŠ¥ ì‹œë®¬ë ˆì´ì…˜")
sim_c1, sim_c2, sim_c3, sim_c4 = st.columns(4)
with sim_c1:
    sim_center = st.selectbox("ì„¼í„°", centers, index=max(0, centers.index("íƒœê´‘KR") if "íƒœê´‘KR" in centers else 0))
with sim_c2:
    default_skus = [s for s in ["BA00022","BA00021"] if s in skus] or skus
    sim_sku = st.selectbox("SKU", skus, index=max(0, skus.index(default_skus[0])))
with sim_c3:
    sim_days = st.number_input(f"ë©°ì¹  ë’¤ (ê¸°ì¤€ì¼: {today.strftime('%Y-%m-%d')})", min_value=0, max_value=60, value=20, step=1)
    st.caption(f"â†’ ëª©í‘œì¼: {(today + pd.Timedelta(days=int(sim_days))).strftime('%Y-%m-%d')}")
with sim_c4:
    sim_qty = st.number_input("í•„ìš” ìˆ˜ëŸ‰", min_value=0, step=1000, value=20000)

# === ì¶œê³  ê°€ëŠ¥ ì‹œë®¬ë ˆì´ì…˜ (ì†Œì§„/ì´ë²¤íŠ¸ ì ìš© í¬í•¨) ===
sim_target_dt = (today + pd.Timedelta(days=int(sim_days))).normalize()

# 1) íƒ€ì„ë¼ì¸ ìƒì„± (ì‹œë®¬ ìœˆë„ìš°ë§Œ) - ë” ê¸´ ê¸°ê°„ìœ¼ë¡œ í™•ì¥
sim_start_dt = min(today, snap_long["date"].min())
sim_tl = build_timeline(
    snap_long, moves,
    centers_sel=[sim_center], skus_sel=[sim_sku],
    start_dt=sim_start_dt, end_dt=sim_target_dt,
    horizon_days=max(0, (sim_target_dt - snap_long["date"].max()).days),
    today=today
)


# 2) ì†Œì§„ + ì´ë²¤íŠ¸ ê°€ì¤‘ì¹˜ ë™ì¼í•˜ê²Œ ì ìš© (ë³¸ë¬¸ê³¼ ì™„ì „ ë™ì¼)
if use_cons_forecast and not sim_tl.empty:
    sim_tl = apply_consumption_with_events(
        sim_tl, snap_long,
        centers_sel=[sim_center], skus_sel=[sim_sku],
        start_dt=today, end_dt=sim_target_dt,
        lookback_days=int(lookback_days),
        events=events  # ì‚¬ì´ë“œë°” ë‹¨ì¼ ì´ë²¤íŠ¸ ì„¸íŒ… ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš©
    )

if sim_tl.empty:
    st.info("í•´ë‹¹ ì¡°í•©ì˜ íƒ€ì„ë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤.")
else:
    # 3) ëª©í‘œì¼ ì¬ê³ (ì‹¤ì œ ì„¼í„° ë¼ì¸ë§Œ: ì´ë™ì¤‘/WIP ì œì™¸)
    real_mask = (
        (sim_tl["center"] == sim_center) &
        (~sim_tl["center"].isin(["WIP"])) &
        (~sim_tl["center"].str.startswith("In-Transit", na=False))
    )
    sim_stock = int(pd.to_numeric(
        sim_tl.loc[(sim_tl["date"] == sim_target_dt) & real_mask, "stock_qty"],
        errors="coerce"
    ).fillna(0).sum().round())

    # 4) ê²°ê³¼ í‘œì‹œ (ë³¸ë¬¸ê³¼ ì¼ì¹˜)
    ok = sim_stock >= sim_qty
    st.metric(
        f"{int(sim_days)}ì¼ ë’¤({sim_target_dt:%Y-%m-%d}) '{sim_center}'ì˜ '{sim_sku}' ì˜ˆìƒ ì¬ê³ ",
        f"{sim_stock:,}",
        delta=f"í•„ìš” {sim_qty:,}"
    )
    if ok:
        st.success("ì¶œê³  ê°€ëŠ¥")
    else:
        st.error("ì¶œê³  ë¶ˆê°€")

    # (ì„ íƒ) ë””ë²„ê·¸ ë°°ì§€: í˜„ì¬ ì†Œì§„ ì¶”ì •ì¹˜
    if use_cons_forecast:
        rates_dbg = estimate_daily_consumption(
            snap_long, [sim_center], [sim_sku],
            asof_dt=pd.to_datetime(snap_long["date"]).max().normalize(),
            lookback_days=int(lookback_days)
        )
        r0 = float(next(iter(rates_dbg.values()), 0.0))
        st.caption(f"ì†Œì§„ ì¶”ì •(ìµœê·¼ {int(lookback_days)}ì¼): {sim_center} / {sim_sku} â‰ˆ {int(round(r0))} EA/ì¼"
                   + (" Â· ì´ë²¤íŠ¸ ê°€ì¤‘ì¹˜ ì ìš©" if events else ""))

# ==================== ì„ íƒ ì„¼í„° í˜„ì¬ ì¬ê³  (ì „ì²´ SKU) ====================
st.subheader(f"ì„ íƒ ì„¼í„° í˜„ì¬ ì¬ê³  (ìŠ¤ëƒ…ìƒ· {latest_dt_str} / ì „ì²´ SKU)")

# 1) ìµœì‹  ìŠ¤ëƒ…ìƒ·ì—ì„œ ì„ íƒ ì„¼í„°ë§Œ ì¶”ì¶œ
cur = snap_long[
    (snap_long["date"] == latest_dt) &
    (snap_long["center"].isin(centers_sel))
].copy()

# 2) SKUÃ—ì„¼í„° í”¼ë²— (ë¹ˆ ê°’ì€ 0)
pivot = (
    cur.groupby(["resource_code", "center"], as_index=False)["stock_qty"].sum()
       .pivot(index="resource_code", columns="center", values="stock_qty")
       .fillna(0)
       .astype(int)
)

# 3) ì´í•© ì»¬ëŸ¼ ì¶”ê°€
pivot["ì´í•©"] = pivot.sum(axis=1)

# 4) UX: í•„í„°/ì •ë ¬ ì˜µì…˜
# UI ê°œì„ : ì²´í¬ë°•ìŠ¤ë¥¼ ìƒë‹¨ìœ¼ë¡œ ì´ë™
col1, col2 = st.columns([2, 1])
with col1:
    q = st.text_input("SKU í•„í„°(í¬í•¨ ê²€ìƒ‰)", "", key="sku_filter_text")
with col2:
    sort_by = st.selectbox("ì •ë ¬ ê¸°ì¤€", ["ì´í•©"] + list(pivot.columns.drop("ì´í•©")), index=0)

# ì²´í¬ë°•ìŠ¤ë“¤ì„ ë³„ë„ í–‰ì— ë°°ì¹˜
col1, col2 = st.columns(2)
with col1:
    hide_zero = st.checkbox("ì´í•©=0 ìˆ¨ê¸°ê¸°", value=True)
with col2:
    show_cost = st.checkbox("ì¬ê³ ìì‚°(ì œì¡°ì›ê°€) í‘œì‹œ", value=False)

# 5) í•„í„° ì ìš©
view = pivot.copy()
if q.strip():
    view = view[view.index.str.contains(q.strip(), case=False, regex=False)]
if hide_zero:
    view = view[view["ì´í•©"] > 0]

# 6) ì •ë ¬(ë‚´ë¦¼ì°¨ìˆœ)  
view = view.sort_values(by=sort_by, ascending=False)

# 7) ë¹„ìš© í†µí•© ë¡œì§
if show_cost:
    snap_raw_df = load_snapshot_raw()
    cost_pivot = pivot_inventory_cost_from_raw(snap_raw_df, latest_dt, centers_sel)  # SKU Ã— ì„¼í„°ë¹„ìš©
    # ì´ë¹„ìš© ì»¬ëŸ¼
    if not cost_pivot.empty:
        cost_cols = [c for c in cost_pivot.columns if c.endswith("_ì¬ê³ ìì‚°")]
        cost_pivot["ì´ ì¬ê³ ìì‚°"] = cost_pivot[cost_cols].sum(axis=1).astype(int)
        # ìˆ˜ëŸ‰ view(í”¼ë²—)ê³¼ ë³‘í•©
        merged = view.reset_index().rename(columns={"resource_code":"SKU"}) \
                     .merge(cost_pivot.rename(columns={"resource_code":"SKU"}), on="SKU", how="left")
        # ê²°ì¸¡ 0
        cost_cols2 = [c for c in merged.columns if c.endswith("_ì¬ê³ ìì‚°")] + (["ì´ ì¬ê³ ìì‚°"] if "ì´ ì¬ê³ ìì‚°" in merged.columns else [])
        if cost_cols2:
            merged[cost_cols2] = merged[cost_cols2].fillna(0).astype(int)
            # ì¬ê³ ìì‚° ì»¬ëŸ¼ í¬ë§·íŒ… (ì²œ ë‹¨ìœ„ êµ¬ë¶„ì + ì›)
            for col in cost_cols2:
                merged[col] = merged[col].apply(lambda x: f"{x:,}ì›" if pd.notna(x) else "0ì›")
        # ì»¬ëŸ¼ ìˆœì„œ: [SKU] + (ì„¼í„° ìˆ˜ëŸ‰ë“¤) + [ì´í•©] + (ì„¼í„°ì¬ê³ ìì‚°ë“¤) + [ì´ ì¬ê³ ìì‚°]
        qty_center_cols = [c for c in merged.columns if c not in ["SKU","ì´í•©"] + cost_cols2]
        ordered = ["SKU"] + qty_center_cols + (["ì´í•©"] if "ì´í•©" in merged.columns else []) + cost_cols2
        merged = merged[ordered]
        show_df = merged
    else:
        show_df = view.reset_index().rename(columns={"resource_code":"SKU"})
else:
    show_df = view.reset_index().rename(columns={"resource_code":"SKU"})

# 8) ìˆ˜ëŸ‰ ì»¬ëŸ¼ í¬ë§·íŒ… (ì²œ ë‹¨ìœ„ êµ¬ë¶„ì)
qty_columns = [col for col in show_df.columns if col not in ["SKU"] and not col.endswith("_ì¬ê³ ìì‚°") and col != "ì´ ì¬ê³ ìì‚°"]
for col in qty_columns:
    if col in show_df.columns:
        show_df[col] = show_df[col].apply(lambda x: f"{x:,}" if pd.notna(x) and isinstance(x, (int, float)) else x)

# 9) ë³´ì—¬ì£¼ê¸°
st.dataframe(show_df, use_container_width=True, height=380)

# 10) CSV ë‹¤ìš´ë¡œë“œ
csv_bytes = show_df.to_csv(index=False).encode("utf-8-sig")
st.download_button(
    "í˜„ì¬ í‘œ CSV ë‹¤ìš´ë¡œë“œ",
    data=csv_bytes,
    file_name=f"centers_{'-'.join(centers_sel)}_snapshot_{latest_dt_str}.csv",
    mime="text/csv"
)

st.caption("â€» ì´ í‘œëŠ” **ì„ íƒëœ ì„¼í„° ì „ì²´ SKU**ì˜ ìµœì‹  ìŠ¤ëƒ…ìƒ· ì¬ê³ ì…ë‹ˆë‹¤. ìƒë‹¨ 'SKU ì„ íƒ'ê³¼ ë¬´ê´€í•˜ê²Œ ëª¨ë“  SKUê°€ í¬í•¨ë©ë‹ˆë‹¤.")

# === ë¡œíŠ¸ ìƒì„¸ ìë™ í‘œì‹œ (ì„ íƒ SKUê°€ 1ê°œì¼ ë•Œ) ===
# â€» show_dfëŠ” ìœ„ì—ì„œ ì‹¤ì œë¡œ st.dataframeì— í‘œì‹œí•œ í…Œì´ë¸”
#    (cost ë¯¸í‘œì‹œ: view.reset_index().rename(columns={"resource_code":"SKU"})
#     cost í‘œì‹œ:   merged â†’ show_df) ë¡œ êµ¬ì„±ë¨.

# 1) í˜„ì¬ í™”ë©´ì— ë³´ì´ëŠ” í‘œì—ì„œ SKU ëª©ë¡ ì¶”ì¶œ
if 'show_df' in locals() and "SKU" in show_df.columns:
    filtered_df = show_df
else:
    # í˜¹ì‹œë¼ë„ show_dfê°€ ì—†ìœ¼ë©´ viewë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒì„±
    filtered_df = view.reset_index().rename(columns={"resource_code": "SKU"})

visible_skus = (
    filtered_df["SKU"].dropna().astype(str).unique().tolist()
)

# 2) SKUê°€ 1ê°œì¼ ë•Œë§Œ ë¡œíŠ¸ ìƒì„¸ í‘œì‹œ
if len(visible_skus) == 1:
    lot_sku = visible_skus[0]
    snap_raw_df = load_snapshot_raw()
    lot_tbl = build_lot_detail(snap_raw_df, latest_dt, lot_sku, centers_sel)

    st.markdown(f"### ë¡œíŠ¸ ìƒì„¸ (ìŠ¤ëƒ…ìƒ· {latest_dt_str} / **{', '.join(centers_sel)}** / **{lot_sku}**)")
    if lot_tbl.empty:
        st.caption("í•´ë‹¹ ì¡°ê±´ì˜ ë¡œíŠ¸ ìƒì„¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.dataframe(lot_tbl, use_container_width=True, height=320)
        st.download_button(
            "ë¡œíŠ¸ ìƒì„¸ CSV ë‹¤ìš´ë¡œë“œ",
            data=lot_tbl.to_csv(index=False).encode("utf-8-sig"),
            file_name=f"lot_detail_{lot_sku}_{latest_dt:%Y%m%d}.csv",
            mime="text/csv"
        )
else:
    st.caption("â€» íŠ¹ì • SKU í•œ ê°œë§Œ í•„í„°ë§í•˜ë©´ ë¡œíŠ¸ ìƒì„¸ê°€ ìë™ í‘œì‹œë©ë‹ˆë‹¤.")