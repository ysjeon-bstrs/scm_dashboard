
import pandas as pd
import numpy as np
import streamlit as st
import plotly.express as px
import re
from datetime import datetime, timedelta
from io import BytesIO
from typing import Dict, List, Optional, Tuple
import json
import requests
import os
import time

# === Apps Script Web App í”„ë¡ì‹œ ì„¤ì • ===
def _get_proxy_cfg():
    """WebApp í”„ë¡ì‹œ ì„¤ì • ë¡œë“œ"""
    if "gs_proxy" in st.secrets:
        cfg = st.secrets["gs_proxy"]
        return dict(
            webapp_url = cfg.get("webapp_url", "").strip(),
            sheet_id   = cfg.get("sheet_id", "").strip(),
            token      = cfg.get("token", "").strip(),
            timeout_s  = int(cfg.get("timeout_s", 15)),
        )
    # ì‹œí¬ë¦¿ì´ ì—†ë‹¤ë©´ í™˜ê²½ë³€ìˆ˜ë¡œë„ ì§€ì›
    return dict(
        webapp_url = os.getenv("GS_WEBAPP_URL","").strip(),
        sheet_id   = os.getenv("GS_SHEET_ID","").strip(),
        token      = os.getenv("GS_TOKEN","").strip(),
        timeout_s  = int(os.getenv("GS_TIMEOUT","15")),
    )

def _fetch_sheet_via_webapp(tab_name: str) -> pd.DataFrame:
    """Apps Script Web Appì„ í†µí•´ ì‹œíŠ¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    cfg = _get_proxy_cfg()
    if not (cfg["webapp_url"] and cfg["sheet_id"] and cfg["token"]):
        st.error("WebApp í”„ë¡ì‹œ ì„¤ì •ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. (webapp_url/sheet_id/token)")
        return pd.DataFrame()

    params = {
        "id": cfg["sheet_id"],
        "sheet": tab_name,
        "token": cfg["token"],
        "_": str(int(time.time()))  # ìºì‹œ ë¬´ë ¥í™”
    }

    # ê°„ë‹¨ ë°±ì˜¤í”„ ì¬ì‹œë„
    last_err = None
    for i in range(3):
        try:
            r = requests.get(cfg["webapp_url"], params=params, timeout=cfg["timeout_s"])
            r.raise_for_status()
            data = r.json()
            if not data.get("ok"):
                raise RuntimeError(f"Proxy error: {data.get('error')}")
            rows = data.get("rows", [])
            return pd.DataFrame(rows)
        except Exception as e:
            last_err = e
            time.sleep(0.6 * (i+1))
    st.error(f"í”„ë¡ì‹œ í˜¸ì¶œ ì‹¤íŒ¨: {last_err}")
    return pd.DataFrame()

def gs_csv(sheet_name: str) -> pd.DataFrame:
    """Apps Script Web Appì„ ì‚¬ìš©í•˜ì—¬ ì‹œíŠ¸ ë°ì´í„° ë¡œë“œ"""
    return _fetch_sheet_via_webapp(sheet_name)

# ì„¼í„°ë³„ ì›ë³¸ ì»¬ëŸ¼ ë§¤í•‘
CENTER_COL = {
    "íƒœê´‘KR": "stock2",
    "AMZUS": "fba_available_stock",
    "í’ˆê³ KR": "poomgo_v2_available_stock",
    "SBSPH": "shopee_ph_available_stock",
    "SBSSG": "shopee_sg_available_stock",
    "SBSMY": "shopee_my_available_stock",
    "AcrossBUS": "acrossb_available_stock",
}

@st.cache_data(ttl=300)
def load_from_gsheet():
    df_move = gs_csv("SCM_í†µí•©")             # ì´ë™ ì›ì¥
    df_ref  = gs_csv("snap_ì •ì œ")           # ì •ì œ ìŠ¤ëƒ…ìƒ· (long: date|center|resource_code|stock_qty)
    # ì…ê³  ì˜ˆì •ì€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë¹ˆ DF
    try:
        df_incoming = gs_csv("ì…ê³ ì˜ˆì •ë‚´ì—­")
    except Exception:
        df_incoming = pd.DataFrame()
    return df_move, df_ref, df_incoming

@st.cache_data(ttl=300)
def load_snapshot_raw():
    # ë¡œíŠ¸ ìƒì„¸ìš© on-demand
    try:
        return gs_csv("snapshot_raw")
    except Exception:
        return pd.DataFrame()

def build_lot_detail(snap_raw: pd.DataFrame, date_latest: pd.Timestamp, sku: str, centers: list[str]) -> pd.DataFrame:
    if snap_raw is None or snap_raw.empty:
        return pd.DataFrame(columns=["lot"] + centers + ["í•©ê³„"])

    sr = snap_raw.copy()
    # ë‚ ì§œ/í—¤ë” ìœ ì—° ì¸ì‹
    cols = {c.strip().lower(): c for c in sr.columns}
    col_date = cols.get("snapshot_date") or cols.get("date")
    col_sku  = cols.get("resource_code") or cols.get("sku") or cols.get("ìƒí’ˆì½”ë“œ")
    col_lot  = cols.get("lot")

    if not all([col_date, col_sku, col_lot]):
        return pd.DataFrame(columns=["lot"] + centers + ["í•©ê³„"])

    # ë‚ ì§œ ì •ê·œí™”
    sr[col_date] = pd.to_datetime(sr[col_date], errors="coerce")
    # í•„í„°: ìµœì‹ ì¼ + SKU
    sub = sr[(sr[col_date].dt.normalize()==date_latest.normalize()) & (sr[col_sku].astype(str)==str(sku))].copy()

    if sub.empty:
        return pd.DataFrame(columns=["lot"] + centers + ["í•©ê³„"])

    # ì„¼í„°ë³„ ìˆ˜ëŸ‰ ì»¬ëŸ¼(ì—†ìœ¼ë©´ ê±´ë„ˆëœ€)
    used_centers = []
    for ct in centers:
        if CENTER_COL.get(ct) in sr.columns:
            used_centers.append(ct)
    if not used_centers:
        return pd.DataFrame(columns=["lot"] + centers + ["í•©ê³„"])

    # ìˆ«ìí™” + ìŒìˆ˜ 0 í´ë¦½
    for ct in used_centers:
        c = CENTER_COL[ct]
        sub[c] = pd.to_numeric(sub[c], errors="coerce").fillna(0).clip(lower=0)

    # lotë³„ í•©ê³„
    out = pd.DataFrame({"lot": sub[col_lot].astype(str).fillna("(no lot)")})
    for ct in used_centers:
        out[ct] = sub.groupby(col_lot)[CENTER_COL[ct]].transform("sum")
    out = out.drop_duplicates()
    out["í•©ê³„"] = out[used_centers].sum(axis=1)
    # ì„ íƒ ì„¼í„° ì™¸ëŠ” 0 ì—´ ì¶”ê°€(ë³´ê¸°ìš©)
    for ct in centers:
        if ct not in used_centers:
            out[ct] = 0
    # ìˆ˜ëŸ‰ 0ì¸ ë¡œíŠ¸ ì œê±°
    out = out[out["í•©ê³„"] > 0]
    
    # ì •ë ¬
    return out[["lot"] + centers + ["í•©ê³„"]].sort_values("í•©ê³„", ascending=False).reset_index(drop=True)

def pivot_inventory_cost_from_raw(snap_raw: pd.DataFrame,
                                  latest_dt: pd.Timestamp,
                                  centers: list[str]) -> pd.DataFrame:
    """
    snapshot_rawì˜ ìµœì‹ ì¼(latest_dt)ì—ì„œ lotë³„ COGS Ã— ê° ì„¼í„° ìˆ˜ëŸ‰ì„ í•©ì‚°í•´
    SKUÃ—ì„¼í„° ë¹„ìš© í”¼ë²—ì„ ë°˜í™˜. (ë‹¨ìœ„: ì›ê°€ ê¸°ì¤€ ê¸ˆì•¡)
    """
    if snap_raw is None or snap_raw.empty:
        return pd.DataFrame(columns=["resource_code"] + [f"{c}_ë¹„ìš©" for c in centers])

    df = snap_raw.copy()
    cols = {str(c).strip().lower(): c for c in df.columns}

    col_date = cols.get("snapshot_date") or cols.get("date")
    col_sku  = cols.get("resource_code") or cols.get("sku") or cols.get("ìƒí’ˆì½”ë“œ")
    col_cogs = cols.get("cogs")  # ì œì¡°ì›ê°€
    if not all([col_date, col_sku, col_cogs]):
        return pd.DataFrame(columns=["resource_code"] + [f"{c}_ë¹„ìš©" for c in centers])

    # íƒ€ì… ì •ê·œí™”
    df[col_date] = pd.to_datetime(df[col_date], errors="coerce").dt.normalize()
    df[col_sku]  = df[col_sku].astype(str)
    df[col_cogs] = pd.to_numeric(df[col_cogs], errors="coerce").fillna(0).clip(lower=0)

    # ìµœì‹  ìŠ¤ëƒ…ìƒ·ë§Œ
    sub = df[df[col_date] == latest_dt.normalize()].copy()
    if sub.empty:
        return pd.DataFrame(columns=["resource_code"] + [f"{c}_ë¹„ìš©" for c in centers])

    # ì„¼í„°ë³„ ë¹„ìš© ê³„ì‚°: sum_lot( cogs Ã— qty_center )
    cost_cols = {}
    for ct in centers:
        src_col = CENTER_COL.get(ct)
        if not src_col or src_col not in sub.columns:
            continue
        qty = pd.to_numeric(sub[src_col], errors="coerce").fillna(0).clip(lower=0)
        cost = (sub[col_cogs] * qty)  # ë‹¨ìˆœ COGS Ã— ìˆ˜ëŸ‰
        # SKUë³„ í•©ê³„
        g = sub[[col_sku]].copy()
        g[f"{ct}_ì¬ê³ ìì‚°"] = cost
        cost_cols[ct] = g.groupby(col_sku, as_index=False)[f"{ct}_ì¬ê³ ìì‚°"].sum()

    if not cost_cols:
        return pd.DataFrame(columns=["resource_code"] + [f"{c}_ì¬ê³ ìì‚°" for c in centers])

    # ë³‘í•©: SKU ê¸°ì¤€ìœ¼ë¡œ ëª¨ë“  ì„¼í„° ë¹„ìš© ë¶™ì´ê¸°
    base = pd.DataFrame({"resource_code": pd.unique(sub[col_sku])})
    for ct, g in cost_cols.items():
        base = base.merge(g.rename(columns={col_sku: "resource_code"}), on="resource_code", how="left")
    # ëˆ„ë½ 0, ì •ìˆ˜ ë°˜ì˜¬ë¦¼
    num_cols = [c for c in base.columns if c.endswith("_ì¬ê³ ìì‚°")]
    base[num_cols] = base[num_cols].fillna(0).round().astype(int)
    return base

st.set_page_config(page_title="ê¸€ë¡œë²Œ ëŒ€ì‹œë³´ë“œ â€” v4", layout="wide")

st.title("ğŸ“¦ SCM ì¬ê³  íë¦„ ëŒ€ì‹œë³´ë“œ â€” v4")

st.caption("í˜„ì¬ ì¬ê³ ëŠ” í•­ìƒ **ìŠ¤ëƒ…ìƒ· ê¸°ì¤€(snap_ì •ì œ)**ì…ë‹ˆë‹¤. ì´ë™ì¤‘ / ìƒì‚°ì¤‘ ë¼ì¸ì€ ì˜ˆì¸¡ìš© ê°€ìƒ ë¼ì¸ì…ë‹ˆë‹¤. â€˜ìƒì‚°ì¤‘(ë¯¸ì™„ë£Œ)â€™ ê·¸ë˜í”„ëŠ” **íƒœê´‘KR ì„¼í„° ì„ íƒ ì‹œì—ë§Œ** í‘œì‹œë©ë‹ˆë‹¤.")

# -------------------- Helpers --------------------
def _coalesce_columns(df: pd.DataFrame, candidates: List, parse_date: bool = False) -> pd.Series:
    all_names = []
    for item in candidates:
        if isinstance(item, (list, tuple, set)):
            all_names.extend([str(x).strip() for x in item])
        else:
            all_names.append(str(item).strip())
    
    # 1ë‹¨ê³„: ì •í™•í•œ ë§¤ì¹­ ì‹œë„
    cols = [c for c in df.columns if str(c).strip() in all_names]
    
    # 2ë‹¨ê³„: ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ë§¤ì¹­ ì‹œë„
    if not cols:
        cols = [c for c in df.columns if any(name.lower() in str(c).lower() for name in all_names)]
    
    # 3ë‹¨ê³„: ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­ ì‹œë„
    if not cols:
        cols = [c for c in df.columns if any(name.lower() in str(c).lower() or str(c).lower() in name.lower() for name in all_names)]
    
    if not cols:
        return pd.Series(pd.NaT if parse_date else np.nan, index=df.index)
    sub = df[cols].copy()
    if parse_date:
        for c in cols:
            sub[c] = pd.to_datetime(sub[c], errors="coerce")
    out = sub.bfill(axis=1).iloc[:, 0]
    return out

@st.cache_data(ttl=300)
def load_from_excel(file):
    data = file.read() if hasattr(file, "read") else file
    bio = BytesIO(data) if isinstance(data, (bytes, bytearray)) else file

    xl = pd.ExcelFile(bio, engine="openpyxl")
    # í•„ìˆ˜: ì´ë™ë°ì´í„° ì‹œíŠ¸(ì´ë¦„ ìœ ì§€), ì •ì œ ìŠ¤ëƒ…ìƒ· ì‹œíŠ¸(ì—¬ëŸ¬ í›„ë³´ëª… ì§€ì›), ì…ê³ ì˜ˆì •(ì„ íƒ)
    need = {"SCM_í†µí•©": None}
    for s in xl.sheet_names:
        if s == "SCM_í†µí•©":
            need["SCM_í†µí•©"] = s

    # ì •ì œ ìŠ¤ëƒ…ìƒ· í›„ë³´ëª…
    refined_name = next((s for s in xl.sheet_names if s in ["snap_ì •ì œ","snap_refined","snap_refine","snap_ref"]), None)
    if refined_name is None:
        st.error("ì—‘ì…€ì— ì •ì œ ìŠ¤ëƒ…ìƒ· ì‹œíŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤. (ì‹œíŠ¸ëª…: 'snap_ì •ì œ' ë˜ëŠ” 'snap_refined')")
        st.stop()

    df_move = pd.read_excel(bio, sheet_name=need["SCM_í†µí•©"], engine="openpyxl")
    bio.seek(0)
    df_ref = pd.read_excel(bio, sheet_name=refined_name, engine="openpyxl")
    bio.seek(0)

    wip_df = None
    if "ì…ê³ ì˜ˆì •ë‚´ì—­" in xl.sheet_names:
        wip_df = pd.read_excel(bio, sheet_name="ì…ê³ ì˜ˆì •ë‚´ì—­", engine="openpyxl")
    return df_move, df_ref, wip_df

def normalize_refined_snapshot(df_ref: pd.DataFrame) -> pd.DataFrame:
    """ì •ì œ ìŠ¤ëƒ…ìƒ· ì‹œíŠ¸ â†’ í•„ìš”í•œ íƒ€ì… ë³´ì •"""
    # ì»¬ëŸ¼ëª… ë§¤í•‘ (ë³µì‚¬ ì—†ì´)
    cols = {c.strip(): c for c in df_ref.columns}
    req = ["date","center","resource_code","stock_qty"]
    miss = [c for c in req if c not in cols]
    if miss:
        st.error(f"'snap_ì •ì œ' ì‹œíŠ¸ì— ëˆ„ë½ëœ ì»¬ëŸ¼: {miss}")
        st.stop()
    # ì´ë¦„ ì •ê·œí™” ë° ë°ì´í„° íƒ€ì… ë³€í™˜ (í•œ ë²ˆì— ì²˜ë¦¬)
    result = df_ref.rename(columns={cols["date"]:"date",
                                   cols["center"]:"center",
                                   cols["resource_code"]:"resource_code",
                                   cols["stock_qty"]:"stock_qty"})
    
    # ë²¡í„°í™”ëœ ì—°ì‚°ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
    result["date"] = pd.to_datetime(result["date"], errors="coerce").dt.normalize()
    result["center"] = result["center"].astype(str)
    result["resource_code"] = result["resource_code"].astype(str)
    result["stock_qty"] = pd.to_numeric(result["stock_qty"], errors="coerce").fillna(0).astype(int)
    
    return result.dropna(subset=["date","center","resource_code"])


def normalize_moves(df: pd.DataFrame) -> pd.DataFrame:
    # ì»¬ëŸ¼ëª… ì •ê·œí™” (in-place)
    df.columns = [str(c).strip() for c in df.columns]

    resource_code = _coalesce_columns(df, [["resource_code","ìƒí’ˆì½”ë“œ","RESOURCE_CODE","sku","SKU"]])
    qty_ea       = _coalesce_columns(df, [
        ["qty_ea","QTY_EA","ìˆ˜ëŸ‰(EA)","qty","QTY","quantity","Quantity","ìˆ˜ëŸ‰","EA","ea"]
    ])
    carrier_mode = _coalesce_columns(df, [["carrier_mode","ìš´ì†¡ë°©ë²•","carrier mode","ìš´ì†¡ìˆ˜ë‹¨"]])
    from_center  = _coalesce_columns(df, [["from_center","ì¶œë°œì°½ê³ ","from center"]])
    to_center    = _coalesce_columns(df, [["to_center","ë„ì°©ì°½ê³ ","to center"]])
    onboard_date = _coalesce_columns(df, [["onboard_date","ë°°ì •ì¼","ì¶œë°œì¼","H","onboard","depart_date"]], parse_date=True)
    arrival_date = _coalesce_columns(df, [["arrival_date","ë„ì°©ì¼","eta_date","ETA","arrival"]], parse_date=True)
    inbound_date = _coalesce_columns(df, [["inbound_date","ì…ê³ ì¼","ì…ê³ ì™„ë£Œì¼"]], parse_date=True)
    real_depart  = _coalesce_columns(df, [["ì‹¤ì œ ì„ ì ì¼","real_departure","AI"]], parse_date=True)

    out = pd.DataFrame({
        "resource_code": resource_code.astype(str).str.strip(),
        "qty_ea": pd.to_numeric(qty_ea.astype(str).str.replace(',', ''), errors="coerce").fillna(0).astype(int),
        "carrier_mode": carrier_mode.astype(str).str.strip(),
        "from_center": from_center.astype(str).str.strip(),
        "to_center": to_center.astype(str).str.strip(),
        "onboard_date": onboard_date,
        "arrival_date": arrival_date,
        "inbound_date": inbound_date,
        "real_departure": real_depart,
    })
    out["event_date"] = out["inbound_date"].where(out["inbound_date"].notna(), out["arrival_date"])
    return out


def _parse_po_date(po_str: str) -> pd.Timestamp:
    """
    ì˜ˆ: 'T250812-0001' -> 2025-08-12
    ê·œì¹™: ì˜ë¬¸ 1ì + YYMMDD + '-' ...
    """
    if not isinstance(po_str, str):
        return pd.NaT
    m = re.search(r"[A-Za-z](\d{2})(\d{2})(\d{2})", po_str)
    if not m:
        return pd.NaT
    yy, mm, dd = m.groups()
    year = 2000 + int(yy)  # 20xx ê°€ì •
    try:
        return pd.Timestamp(datetime(year, int(mm), int(dd)))
    except Exception:
        return pd.NaT

def load_wip_from_incoming(df_incoming: Optional[pd.DataFrame], default_center: str = "íƒœê´‘KR") -> pd.DataFrame:
    """
    'ì…ê³ ì˜ˆì •ë‚´ì—­' ì‹œíŠ¸ ì •ê·œí™”:
      - wip_start = po_no(ë°œì£¼ë²ˆí˜¸)ì—ì„œ íŒŒì‹±í•œ ë‚ ì§œ(ì—†ìœ¼ë©´ wip_ready-10ì¼)
      - wip_ready = intended_push_date
      - qty_ea    = quantity / total_quantity
      - to_center = íƒœê´‘KR (ê³ ì •)
      - lot       = ì œì¡°ë²ˆí˜¸(ì„ íƒ)
    """
    if df_incoming is None or df_incoming.empty:
        return pd.DataFrame()

    df = df_incoming.copy()
    df.columns = [str(c).strip() for c in df.columns]
    low = {c.lower(): c for c in df.columns}

    # --- robust column pickers ---
    po_col   = next((low[k] for k in low if k in ["po_no","ponumber","po"]), None)
    sku_col  = next((low[k] for k in low if k in ["product_code","resource_code","ìƒí’ˆì½”ë“œ"]), None)

    # ìˆ˜ëŸ‰: quantity/qty/ìˆ˜ëŸ‰/total_quantity ä¸­ "ìˆ«ìë¡œ ì˜ íŒŒì‹±ë˜ëŠ”" ì»¬ëŸ¼ë§Œ ì±„íƒ
    qty_col = None
    for key in ["quantity","qty","ìˆ˜ëŸ‰","total_quantity","ì…ê³ ìˆ˜ëŸ‰"]:
        if key in low:
            s = pd.to_numeric(df[low[key]].astype(str).str.replace(",",""), errors="coerce")
            if s.notna().sum() >= max(1, int(len(s)*0.6)):
                qty_col = low[key]; break

    # ë‚ ì§œ: 'ì…ê³ ì˜ˆì •', 'ì˜ˆì •ì¼', 'ready', 'push', 'intended_push' í¬í•¨ &
    #      "ìˆ˜ëŸ‰/qty" ê°™ì€ ë‹¨ì–´ í¬í•¨ ì»¬ëŸ¼ì€ ì œì™¸ &
    #      "ë‚ ì§œë¡œ 60% ì´ìƒ íŒŒì‹±"ë˜ëŠ” ì»¬ëŸ¼ë§Œ ì±„íƒ
    date_col = None
    
    # ë””ë²„ê¹…: ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë“¤ í™•ì¸
    st.write("ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë“¤:", df.columns.tolist())
    
    for p in ["ì…ê³ ì˜ˆì •", "ì˜ˆì •ì¼", "ready", "push", "intended_push", "ì…ê³ ì¼", "eta"]:
        cands = [c for c in df.columns
                 if (p.lower() in c.lower())
                 and not any(x in c.lower() for x in ["ìˆ˜ëŸ‰","qty","quantity"])]
        st.write(f"ğŸ” '{p}' íŒ¨í„´ ë§¤ì¹­ ì»¬ëŸ¼ë“¤:", cands)
        
        for c in cands:
            s = pd.to_datetime(df[c], errors="coerce")
            success_rate = s.notna().sum() / len(s) if len(s) > 0 else 0
            st.write(f"ğŸ” ì»¬ëŸ¼ '{c}' íŒŒì‹± ì„±ê³µë¥ : {success_rate:.2%} ({s.notna().sum()}/{len(s)})")
            st.write(f"ğŸ” ì»¬ëŸ¼ '{c}' ìƒ˜í”Œ ê°’ë“¤:", df[c].head().tolist())
            
            if s.notna().sum() >= max(1, int(len(s)*0.6)):
                date_col = c
                st.write(f"âœ… ë‚ ì§œ ì»¬ëŸ¼ ì„ íƒ: '{c}'")
                break
        if date_col: break
    
    # ë””ë²„ê¹…: ìµœì¢… ì„ íƒëœ ì»¬ëŸ¼ë“¤ í™•ì¸
    st.write(f"ğŸ” ìµœì¢… ì„ íƒëœ ì»¬ëŸ¼ë“¤ - date_col: '{date_col}', sku_col: '{sku_col}', qty_col: '{qty_col}'")
    
    if date_col:
        st.write(f"ğŸ” ì„ íƒëœ ë‚ ì§œ ì»¬ëŸ¼ '{date_col}'ì˜ ìƒ˜í”Œ ê°’ë“¤:", df[date_col].head().tolist())
        st.write(f"ğŸ” ì„ íƒëœ ë‚ ì§œ ì»¬ëŸ¼ '{date_col}'ì˜ íŒŒì‹± ê²°ê³¼:", pd.to_datetime(df[date_col], errors="coerce").head().tolist())
    if not (date_col and sku_col and qty_col):
        return pd.DataFrame()

    out = pd.DataFrame({
        "resource_code": df[sku_col].astype(str).str.strip(),
        "to_center": default_center,
        "wip_ready": pd.to_datetime(df[date_col], errors="coerce").dt.normalize(),
        "qty_ea": pd.to_numeric(df[qty_col].astype(str).str.replace(',',''), errors="coerce").fillna(0).astype(int),
        "lot": df[low.get("lot")] if "lot" in low else ""
    })

    # wip_start: POì—ì„œ íŒŒì‹± or wip_ready-10ì¼
    def _parse_po_date(po):
        m = re.search(r"[A-Za-z](\d{2})(\d{2})(\d{2})", str(po))
        return (pd.Timestamp(2000+int(m.group(1)), int(m.group(2)), int(m.group(3)))
                if m else pd.NaT)
    out["wip_start"] = df[po_col].map(_parse_po_date) if po_col else pd.NaT
    # ë°œì£¼ì¼ì´ ëª» ì½í˜”ìœ¼ë©´(ì˜ˆì™¸ ì¼€ì´ìŠ¤) ìµœì†Œí•œ wip_ready - 30ì¼ë¡œ ë³´ì •
    mask_na = out["wip_start"].isna() & out["wip_ready"].notna()
    out.loc[mask_na, "wip_start"] = out.loc[mask_na, "wip_ready"] - pd.to_timedelta(30, unit="D")

    # ìœ íš¨ê°’ë§Œ
    out = out.dropna(subset=["resource_code","wip_ready","wip_start"]).reset_index(drop=True)
    return out[["resource_code","to_center","wip_start","wip_ready","qty_ea","lot"]]

def merge_wip_as_moves(moves_df: pd.DataFrame, wip_df: Optional[pd.DataFrame]) -> pd.DataFrame:
    if wip_df is None or wip_df.empty:
        return moves_df
    # ë””ë²„ê¹…: wip_df ìƒíƒœ í™•ì¸
    st.write(f"ğŸ” merge_wip_as_moves - wip_df ìƒíƒœ: {len(wip_df)}ê±´")
    if not wip_df.empty:
        st.write("wip_df ìƒ˜í”Œ:", wip_df[["resource_code", "wip_start", "wip_ready", "qty_ea"]].head())
        st.write("wip_ready ìƒíƒœ:", wip_df["wip_ready"].isna().sum(), "ê°œê°€ NaT")
        st.write("wip_ready ê°’ë“¤:", wip_df["wip_ready"].tolist())
    
    # ë””ë²„ê¹…: wip_dfì˜ wip_ready ìƒíƒœ í™•ì¸
    st.write(f"ğŸ” merge_wip_as_moves - wip_ready ìƒíƒœ: {wip_df['wip_ready'].isna().sum()}ê°œê°€ NaT")
    st.write("wip_ready ìƒ˜í”Œ:", wip_df["wip_ready"].head().tolist())
    
    # ë””ë²„ê¹…: wip_dfì˜ BA00021 ë°ì´í„° í™•ì¸
    ba00021_wip_df = wip_df[wip_df["resource_code"] == "BA00021"]
    st.write(f"ğŸ” merge_wip_as_moves - BA00021 wip_df: {len(ba00021_wip_df)}ê±´")
    if not ba00021_wip_df.empty:
        st.write("BA00021 wip_df ìƒ˜í”Œ:", ba00021_wip_df[["resource_code", "wip_start", "wip_ready", "qty_ea"]].head())
        st.write("BA00021 wip_ready ìƒíƒœ:", ba00021_wip_df["wip_ready"].isna().sum(), "ê°œê°€ NaT")
        st.write("BA00021 wip_ready ê°’ë“¤:", ba00021_wip_df["wip_ready"].tolist())
    
    wip_moves = pd.DataFrame({
        "resource_code": wip_df["resource_code"],
        "qty_ea": wip_df["qty_ea"].astype(int),
        "carrier_mode": "WIP",
        "from_center": "WIP",
        "to_center": wip_df["to_center"],
        "onboard_date": wip_df["wip_start"].dt.normalize(),
        "arrival_date": wip_df["wip_ready"].dt.normalize(),
        "inbound_date": pd.NaT,
        "real_departure": pd.NaT,
        "event_date": wip_df["wip_ready"].dt.normalize(),
        "lot": wip_df.get("lot", "")
    })
    
    # ë””ë²„ê¹…: wip_movesì˜ event_date ìƒíƒœ í™•ì¸
    st.write(f"ğŸ” merge_wip_as_moves - wip_moves event_date ìƒíƒœ: {wip_moves['event_date'].isna().sum()}ê°œê°€ NaT")
    st.write("wip_moves event_date ìƒ˜í”Œ:", wip_moves["event_date"].head().tolist())
    
    # ë‚ ì§œ ì •ê·œí™” (ì¼ ë‹¨ìœ„ë¡œ í†µì¼)
    wip_moves["onboard_date"] = pd.to_datetime(wip_moves["onboard_date"]).dt.normalize()
    wip_moves["arrival_date"] = pd.to_datetime(wip_moves["arrival_date"]).dt.normalize()
    wip_moves["event_date"] = pd.to_datetime(wip_moves["event_date"]).dt.normalize()
    
    # ë””ë²„ê¹…: moves_dfì™€ wip_movesì˜ ì»¬ëŸ¼ êµ¬ì¡° í™•ì¸
    st.write(f"ğŸ” merge_wip_as_moves - moves_df ì»¬ëŸ¼ë“¤: {moves_df.columns.tolist()}")
    st.write(f"ğŸ” merge_wip_as_moves - wip_moves ì»¬ëŸ¼ë“¤: {wip_moves.columns.tolist()}")
    
    # ë””ë²„ê¹…: moves_dfì˜ event_date ìƒíƒœ í™•ì¸
    if "event_date" in moves_df.columns:
        st.write(f"ğŸ” merge_wip_as_moves - moves_df event_date ìƒíƒœ: {moves_df['event_date'].isna().sum()}ê°œê°€ NaT")
    else:
        st.write("ğŸ” merge_wip_as_moves - moves_dfì— event_date ì»¬ëŸ¼ì´ ì—†ìŒ!")
    
    # ë””ë²„ê¹…: wip_movesì˜ event_date ìƒíƒœ í™•ì¸
    st.write(f"ğŸ” merge_wip_as_moves - wip_moves event_date ìƒíƒœ: {wip_moves['event_date'].isna().sum()}ê°œê°€ NaT")
    
    result = pd.concat([moves_df, wip_moves], ignore_index=True)
    
    # ë””ë²„ê¹…: ë³‘í•© ê²°ê³¼ì˜ event_date ìƒíƒœ í™•ì¸
    st.write(f"ğŸ” merge_wip_as_moves - ë³‘í•© ê²°ê³¼ event_date ìƒíƒœ: {result['event_date'].isna().sum()}ê°œê°€ NaT")
    if "BA00021" in result["resource_code"].values:
        ba00021_result = result[result["resource_code"] == "BA00021"]
        st.write(f"ğŸ” merge_wip_as_moves - BA00021 ë³‘í•© ê²°ê³¼: {len(ba00021_result)}ê±´")
        st.write("BA00021 ë³‘í•© ê²°ê³¼ event_date ìƒíƒœ:", ba00021_result["event_date"].isna().sum(), "ê°œê°€ NaT")
        st.write("BA00021 ë³‘í•© ê²°ê³¼ event_date ê°’ë“¤:", ba00021_result["event_date"].head().tolist())
    
    return result

# ===== ì†Œë¹„(ì†Œì§„) ì¶”ì„¸ + ì´ë²¤íŠ¸ ê°€ì¤‘ì¹˜ =====

@st.cache_data(ttl=3600)  # 1ì‹œê°„ ìºì‹œ (í•˜ë£¨ 1íšŒ ê°±ì‹ ì— ì í•©)
def estimate_daily_consumption(snap_long: pd.DataFrame,
                               centers_sel: List[str], skus_sel: List[str],
                               asof_dt: pd.Timestamp,
                               lookback_days: int = 28) -> Dict[Tuple[str, str], float]:
    """
    ìµœê·¼ lookback_days ë™ì•ˆ ìŠ¤ëƒ…ìƒ·ìœ¼ë¡œ SKUÃ—ì„¼í„°ë³„ ì¼ì¼ ì†Œì§„ëŸ‰(ê°œ/ì¼)ì„ ì¶”ì •.
    íšŒê·€ ê¸°ìš¸ê¸°ì™€ ê°ì†Œë¶„ í‰ê· ì˜ maxë¥¼ ì‚¬ìš©í•˜ì—¬ 'ì…ê³  í›„ í‰í‰' êµ¬ê°„ì—ì„œë„ ì•ˆì •ì  ì¶”ì •.
    """
    snap = snap_long.rename(columns={"snapshot_date":"date"}).copy()
    snap["date"] = pd.to_datetime(snap["date"], errors="coerce").dt.normalize()
    start = (asof_dt - pd.Timedelta(days=int(lookback_days)-1)).normalize()

    hist = snap[(snap["date"] >= start) & (snap["date"] <= asof_dt) &
                (snap["center"].isin(centers_sel)) &
                (snap["resource_code"].isin(skus_sel))]

    rates = {}
    if hist.empty: 
        return rates

    for (ct, sku), g in hist.groupby(["center","resource_code"]):
        ts = (g.sort_values("date")
                .set_index("date")["stock_qty"]
                .asfreq("D").ffill())
        # ê´€ì¸¡ ìµœì†Œ ë³´ì¥
        if ts.dropna().shape[0] < max(7, lookback_days//2):
            continue
        
        x = np.arange(len(ts), dtype=float)
        y = ts.values.astype(float)
        
        # ë°©ë²•1: íšŒê·€ ê¸°ìš¸ê¸°(ê°ì†Œë§Œ)
        rate_reg = max(0.0, -np.polyfit(x, y, 1)[0])
        
        # ë°©ë²•2: ì¼/ì¼ ê°ì†Œë¶„ í‰ê· 
        dec = (-np.diff(y)).clip(min=0)
        rate_dec = dec.mean() if len(dec) else 0.0
        
        # ë‘ ë°©ë²•ì˜ max ì‚¬ìš©
        rate = max(rate_reg, rate_dec)
        if rate > 0:
            rates[(ct, sku)] = float(rate)
    
    return rates

@st.cache_data(ttl=1800)  # 30ë¶„ ìºì‹œ
def apply_consumption_with_events(
    timeline: pd.DataFrame,
    snap_long: pd.DataFrame,
    centers_sel: List[str], skus_sel: List[str],
    start_dt: pd.Timestamp, end_dt: pd.Timestamp,
    lookback_days: int = 28,
    events: Optional[List[Dict]] = None
) -> pd.DataFrame:
    out = timeline.copy()
    if out.empty:
        return out
    out["date"] = pd.to_datetime(out["date"]).dt.normalize()

    # ë‚ ì§œ ì»¬ëŸ¼ ìë™ ê°ì§€ (date / snapshot_date ëª¨ë‘ ì§€ì›)
    snap_cols = {c.lower(): c for c in snap_long.columns}
    date_col = snap_cols.get("date") or snap_cols.get("snapshot_date")
    if date_col is None:
        raise KeyError("snap_longì—ëŠ” 'date' ë˜ëŠ” 'snapshot_date' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    latest_snap = pd.to_datetime(snap_long[date_col]).max().normalize()
    cons_start = max(latest_snap + pd.Timedelta(days=1), start_dt)
    if cons_start > end_dt:
        return out

    # ì´ë²¤íŠ¸ ê³„ìˆ˜ (ì—†ìœ¼ë©´ 1.0)
    idx = pd.date_range(cons_start, end_dt, freq="D")
    uplift = pd.Series(1.0, index=idx)
    if events:
        for e in events:
            s = pd.to_datetime(e.get("start"), errors="coerce")
            t = pd.to_datetime(e.get("end"), errors="coerce")
            u = min(3.0, max(-1.0, float(e.get("uplift", 0.0))))  # -100% ~ +300% ë°©ì–´
            if pd.notna(s) and pd.notna(t):
                s = s.normalize(); t = t.normalize()
                s = max(s, idx[0]); t = min(t, idx[-1])
                if s <= t:
                    uplift.loc[s:t] = uplift.loc[s:t] * (1.0 + u)

    # ì¼ì¼ ì†Œì§„ëŸ‰ ì¶”ì • (ë³´ê°• ë²„ì „: íšŒê·€ vs ê°ì†Œë¶„ í‰ê· ì˜ max)
    rates = estimate_daily_consumption(snap_long, centers_sel, skus_sel, latest_snap, int(lookback_days))

    chunks: list[pd.DataFrame] = []  # â† ë°˜ë“œì‹œ ë£¨í”„ ë°–ì—ì„œ ì´ˆê¸°í™”
    for (ct, sku), g in out.groupby(["center","resource_code"]):
        # ê°€ìƒ ë¼ì¸ì€ ì†Œì§„ ë¯¸ì ìš©
        if ct in ("In-Transit", "WIP"):
            chunks.append(g)
            continue

        rate = float(rates.get((ct, sku), 0.0))
        if rate <= 0:
            chunks.append(g)
            continue

        g = g.sort_values("date").copy()
        mask = g["date"] >= cons_start
        if not mask.any():
            chunks.append(g)
            continue

        daily = g.loc[mask, "date"].map(uplift).fillna(1.0).values * rate
        stk = g.loc[mask, "stock_qty"].astype(float).values
        # ëˆ„ì  ì°¨ê°(í•˜í•œ 0)
        for i in range(len(stk)):
            dec = daily[i]
            stk[i:] = np.maximum(0.0, stk[i:] - dec)
        g.loc[mask, "stock_qty"] = stk
        chunks.append(g)

    # ê·¸ë£¹ì´ ì—†ê±°ë‚˜ ì „ë¶€ ì œì™¸ë˜ì—ˆë‹¤ë©´ ì›ë³¸ ë°˜í™˜
    if not chunks:
        return out

    out = pd.concat(chunks, ignore_index=True)
    # ì†Œìˆ˜ ì •ë¦¬(í‘œì‹œ/ì¼ê´€ì„±)
    out["stock_qty"] = (
        pd.to_numeric(out["stock_qty"], errors="coerce")
        .round()
        .clip(lower=0)
        .astype(int)
    )
    return out




@st.cache_data(ttl=1800)  # 30ë¶„ ìºì‹œ
def build_timeline_new(snap_long: pd.DataFrame, moves: pd.DataFrame, 
                       centers_sel: List[str], skus_sel: List[str],
                       start_dt: pd.Timestamp, end_dt: pd.Timestamp, 
                       horizon_days: int = 0) -> pd.DataFrame:
    """
    ë°˜í™˜: date, center, resource_code, stock_qty
    - ì‹¤ì œ ì„¼í„° ë¼ì¸: ìŠ¤ëƒ…ìƒ· + (ì¶œë°œ:onboard -, ë„ì°©:event +)
    - In-Transit ê°€ìƒ ë¼ì¸: onboard +, event -
    """
    horizon_end = end_dt + pd.Timedelta(days=horizon_days)
    full_dates = pd.date_range(start_dt, horizon_end, freq="D")

    base = snap_long[
        snap_long["center"].isin(centers_sel) &
        snap_long["resource_code"].isin(skus_sel)
    ].copy().rename(columns={"snapshot_date":"date"})
    if base.empty:
        return pd.DataFrame(columns=["date","center","resource_code","stock_qty"])
    base = base[(base["date"] >= start_dt) & (base["date"] <= end_dt)]

    lines = []

    # 1) ì‹¤ì œ ì„¼í„° ë¼ì¸
    for (ct, sku), grp in base.groupby(["center","resource_code"]):
        grp = grp.sort_values("date")
        last_dt = grp["date"].max()

        if horizon_days > 0:
            proj_dates = pd.date_range(last_dt + pd.Timedelta(days=1), horizon_end, freq="D")
            proj_df = pd.DataFrame({"date": proj_dates, "center": ct,
                                    "resource_code": sku, "stock_qty": np.nan})
            ts = pd.concat([grp[["date","center","resource_code","stock_qty"]], proj_df], ignore_index=True)
        else:
            ts = grp[["date","center","resource_code","stock_qty"]].copy()

        ts = ts.sort_values("date")
        ts["stock_qty"] = ts["stock_qty"].ffill()

        mv = moves[moves["resource_code"] == sku].copy()

        eff_minus = (
            mv[(mv["from_center"].astype(str) == str(ct)) &
               (mv["onboard_date"].notna()) &
               (mv["onboard_date"] > last_dt)]
            .groupby("onboard_date", as_index=False)["qty_ea"].sum()
            .rename(columns={"onboard_date":"date","qty_ea":"delta"})
        )
        eff_minus["delta"] *= -1

        eff_plus = (
            mv[(mv["to_center"].astype(str) == str(ct)) &
               (mv["event_date"].notna()) &
               (mv["event_date"] > last_dt)]
            .groupby("event_date", as_index=False)["qty_ea"].sum()
            .rename(columns={"event_date":"date","qty_ea":"delta"})
        )

        eff_all = pd.concat([eff_minus, eff_plus], ignore_index=True).sort_values("date")
        for d, delta in zip(eff_all["date"], eff_all["delta"]):
            ts.loc[ts["date"] >= d, "stock_qty"] = ts.loc[ts["date"] >= d, "stock_qty"] + delta

        ts["stock_qty"] = ts["stock_qty"].clip(lower=0)
        lines.append(ts)

    # 2) In-Transit ê°€ìƒ ë¼ì¸ (Non-WIP / WIP ë¶„ë¦¬)
    # ë¯¸ë¦¬ íƒ€ì… ë³€í™˜í•˜ì—¬ ë°˜ë³µ ì—°ì‚° ìµœì í™”
    moves_str = moves.copy()
    
    # ë””ë²„ê¹…: moves ì›ë³¸ ë°ì´í„°ì˜ event_date ìƒíƒœ í™•ì¸
    if "BA00021" in skus_sel:
        ba00021_moves_orig = moves[moves["resource_code"] == "BA00021"]
        st.write(f"ğŸ” build_timeline - moves ì›ë³¸ BA00021: {len(ba00021_moves_orig)}ê±´")
        if not ba00021_moves_orig.empty:
            st.write("moves ì›ë³¸ ìƒ˜í”Œ:", ba00021_moves_orig[["resource_code", "carrier_mode", "event_date"]].head())
            st.write("moves ì›ë³¸ event_date ìƒíƒœ:", ba00021_moves_orig["event_date"].isna().sum(), "ê°œê°€ NaT")
            st.write("moves ì›ë³¸ event_date ê°’ë“¤:", ba00021_moves_orig["event_date"].head().tolist())
    
    moves_str["from_center"] = moves_str["from_center"].astype(str)
    moves_str["to_center"] = moves_str["to_center"].astype(str)
    moves_str["carrier_mode"] = moves_str["carrier_mode"].astype(str).str.upper()
    
    # ë””ë²„ê¹…: moves ë°ì´í„° ì²˜ë¦¬ ì „ ìƒíƒœ í™•ì¸
    if "BA00021" in skus_sel:
        ba00021_moves = moves_str[moves_str["resource_code"] == "BA00021"]
        st.write(f"ğŸ” build_timeline - moves ì²˜ë¦¬ ì „ BA00021: {len(ba00021_moves)}ê±´")
        if not ba00021_moves.empty:
            st.write("moves ì²˜ë¦¬ ì „ ìƒ˜í”Œ:", ba00021_moves[["resource_code", "carrier_mode", "event_date"]].head())
            st.write("event_date ìƒíƒœ:", ba00021_moves["event_date"].isna().sum(), "ê°œê°€ NaT")
    
    # ë””ë²„ê¹…: moves_str ìƒíƒœ í™•ì¸
    if "BA00021" in skus_sel:
        ba00021_moves = moves_str[moves_str["resource_code"] == "BA00021"]
        st.write(f"ğŸ” build_timeline - BA00021 moves_str: {len(ba00021_moves)}ê±´")
        if not ba00021_moves.empty:
            st.write("moves_str ìƒ˜í”Œ:", ba00021_moves[["resource_code", "carrier_mode", "event_date"]].head())
            st.write("event_date ìƒíƒœ:", ba00021_moves["event_date"].isna().sum(), "ê°œê°€ NaT")
        
        # WIP ë°ì´í„° í™•ì¸
        wip_moves = moves_str[moves_str["carrier_mode"] == "WIP"]
        st.write(f"ğŸ” build_timeline - WIP moves: {len(wip_moves)}ê±´")
        if not wip_moves.empty:
            st.write("WIP moves ìƒ˜í”Œ:", wip_moves[["resource_code", "carrier_mode", "event_date"]].head())
    
    mv_sel = moves_str[
        moves_str["resource_code"].isin(skus_sel) &
        (moves_str["from_center"].isin(centers_sel) | 
         moves_str["to_center"].isin(centers_sel) | 
         (moves_str["carrier_mode"] == "WIP"))
    ]
    
    # ë””ë²„ê¹…: mv_sel í•„í„°ë§ ê²°ê³¼ í™•ì¸
    if "BA00021" in skus_sel:
        ba00021_mv_sel = mv_sel[mv_sel["resource_code"] == "BA00021"]
        st.write(f"ğŸ” build_timeline - mv_sel BA00021: {len(ba00021_mv_sel)}ê±´")
        if not ba00021_mv_sel.empty:
            st.write("mv_sel ìƒ˜í”Œ:", ba00021_mv_sel[["resource_code", "carrier_mode", "event_date"]].head())
            st.write("event_date ìƒíƒœ:", ba00021_mv_sel["event_date"].isna().sum(), "ê°œê°€ NaT")

    for sku, g in mv_sel.groupby("resource_code"):
        # Non-WIP In-Transit (ì„ íƒëœ ì„¼í„°ë¡œ ì´ë™ì¤‘ì¸ ì¬ê³ ë§Œ)
        g_nonwip = g[g["carrier_mode"] != "WIP"]
        if not g_nonwip.empty:
            # ì„ íƒëœ ì„¼í„°ë¡œ ì´ë™ì¤‘ì¸ ì¬ê³ ë§Œ í•„í„°ë§
            g_selected = g_nonwip[g_nonwip["to_center"].isin(centers_sel)]
            if not g_selected.empty:
                add_onboard = (
                    g_selected[g_selected["onboard_date"].notna()]
                    .groupby("onboard_date", as_index=False)["qty_ea"].sum()
                    .rename(columns={"onboard_date":"date","qty_ea":"delta"})
                )
                add_event = (
                    g_selected[g_selected["event_date"].notna()]
                    .groupby("event_date", as_index=False)["qty_ea"].sum()
                    .rename(columns={"event_date":"date","qty_ea":"delta"})
                )
                add_event["delta"] *= -1
                deltas = pd.concat([add_onboard, add_event], ignore_index=True)
                if not deltas.empty:
                    s = pd.Series(0, index=pd.to_datetime(full_dates))
                    for d, v in deltas.groupby("date")["delta"].sum().items():
                        d = pd.to_datetime(d)
                        if d < full_dates[0]:
                            s.iloc[:] = s.iloc[:] + v
                        else:
                            s.loc[s.index >= d] = s.loc[s.index >= d] + v
                    vdf = pd.DataFrame({
                        "date": s.index,
                        "center": "In-Transit",
                        "resource_code": sku,
                        "stock_qty": s.values.clip(min=0)
                    })
                    lines.append(vdf)

        # --- WIP ë³„ë„ ë¼ì¸ (ìƒì‚°ì¤‘) ---
        g_wip = g[g["carrier_mode"].astype(str).str.upper() == "WIP"]
        
        # ë””ë²„ê¹…: BA00021 WIP ì›ë³¸ ë°ì´í„° í™•ì¸
        if sku == "BA00021":
            st.write(f"ğŸ” BA00021 WIP ì›ë³¸ ë°ì´í„°: {len(g_wip)}ê±´")
            if not g_wip.empty:
                st.write("WIP ì›ë³¸ ìƒ˜í”Œ:", g_wip[["resource_code", "onboard_date", "event_date", "qty_ea"]].head())
                st.write("event_date ìƒíƒœ:", g_wip["event_date"].isna().sum(), "ê°œê°€ NaT")
                st.write("event_date ê°’ë“¤:", g_wip["event_date"].tolist())
        
        if not g_wip.empty:
            add_onboard = (
                g_wip[g_wip["onboard_date"].notna()]
                .groupby("onboard_date", as_index=False)["qty_ea"].sum()
                .rename(columns={"onboard_date":"date","qty_ea":"delta"})
            )
            add_event = (
                g_wip[g_wip["event_date"].notna()]
                .groupby("event_date", as_index=False)["qty_ea"].sum()
                .rename(columns={"event_date":"date","qty_ea":"delta"})
            )
            # âœ… ì™„ë£Œì¼ì€ ìŒìˆ˜(ê°ì†Œ)
            add_event["delta"] *= -1
            
            # ë””ë²„ê¹…: BA00021 WIP ë°ì´í„° í™•ì¸
            if sku == "BA00021":
                st.write(f"ğŸ” BA00021 WIP ì‹œì‘ ë°ì´í„°: {len(add_onboard)}ê±´")
                st.write("WIP ì‹œì‘ ìƒ˜í”Œ:", add_onboard.head())
                st.write(f"ğŸ” BA00021 WIP ì™„ë£Œ ë°ì´í„°: {len(add_event)}ê±´")
                st.write("WIP ì™„ë£Œ ìƒ˜í”Œ:", add_event.head())

            # ë‚ ì§œë¥¼ ì¼ ë‹¨ìœ„ë¡œ ì •ê·œí™”
            add_onboard["date"] = pd.to_datetime(add_onboard["date"]).dt.normalize()
            add_event["date"]   = pd.to_datetime(add_event["date"]).dt.normalize()
            deltas = pd.concat([add_onboard, add_event], ignore_index=True)
            if not deltas.empty:
                # ë‚ ì§œ ë²”ìœ„ ì „ì²´ 0ìœ¼ë¡œ ì‹œì‘í•´ ê³„ë‹¨ ëˆ„ì 
                s = pd.Series(0, index=pd.to_datetime(full_dates))
                for d, v in deltas.groupby("date")["delta"].sum().items():
                    d = pd.to_datetime(d)
                    if d < full_dates[0]:
                        s.iloc[:] = s.iloc[:] + v
                    else:
                        s.loc[s.index >= d] = s.loc[s.index >= d] + v

                vdf = pd.DataFrame({
                    "date": s.index,
                    "center": "WIP",
                    "resource_code": sku,
                    "stock_qty": s.values.clip(min=0)
                })
                lines.append(vdf)
                
                # --- WIP ì™„ë£Œ ë¬¼ëŸ‰ì„ ì„¼í„° ë¼ì¸ì— ëˆ„ì (+1) ë°˜ì˜ ---
                wip_done = g_wip[g_wip["event_date"].notna()].copy()
                wip_done["event_date"] = pd.to_datetime(wip_done["event_date"]).dt.normalize()
                if not wip_done.empty:
                    # ë””ë²„ê¹…: WIP ì™„ë£Œ ë°ì´í„° í™•ì¸
                    if sku == "BA00021":
                        st.write(f"ğŸ” BA00021 WIP ì™„ë£Œ ë°ì´í„°: {len(wip_done)}ê±´")
                        st.write("WIP ì™„ë£Œ ìƒ˜í”Œ:", wip_done[["resource_code", "to_center", "event_date", "qty_ea"]].head())
                    
                    add_to_center = (
                        wip_done.groupby(["to_center","event_date"], as_index=False)["qty_ea"].sum()
                        .rename(columns={"to_center":"center","event_date":"date","qty_ea":"delta"})
                    )
                    
                    # ë””ë²„ê¹…: ì„¼í„° ì…ê³  ë°ì´í„° í™•ì¸
                    if sku == "BA00021":
                        st.write(f"ğŸ” BA00021 ì„¼í„° ì…ê³  ë°ì´í„°: {len(add_to_center)}ê±´")
                        st.write("ì„¼í„° ì…ê³  ìƒ˜í”Œ:", add_to_center.head())
                    # lines ì•ˆì˜ ê° 'ì„¼í„° ë¼ì¸(ts_line)'ì— ë‚ ì§œ>=d ì§€ì ë¶€í„° ëˆ„ì  ê°€ì‚°
                    for idx_line, ts_line in enumerate(lines):
                        if ts_line.empty: 
                            continue
                        if ts_line["resource_code"].iloc[0] != sku:
                            continue
                        ct_name = ts_line["center"].iloc[0]
                        if ct_name in ("In-Transit", "WIP"):    # ê°€ìƒ ë¼ì¸ ì œì™¸
                            continue

                        inc = add_to_center[add_to_center["center"].astype(str) == str(ct_name)]
                        if inc.empty:
                            continue

                        ts = ts_line.sort_values("date").copy()
                        for d, q in zip(pd.to_datetime(inc["date"]), inc["delta"]):
                            ts.loc[ts["date"] >= d, "stock_qty"] = ts.loc[ts["date"] >= d, "stock_qty"] + int(q)
                        ts["stock_qty"] = ts["stock_qty"].clip(lower=0)
                        lines[idx_line] = ts

    if not lines:
        return pd.DataFrame(columns=["date","center","resource_code","stock_qty"])

    out = pd.concat(lines, ignore_index=True)
    out = out[(out["date"] >= start_dt) & (out["date"] <= horizon_end)]
    return out

# -------------------- Tabs for inputs --------------------
tab1, tab2, tab3 = st.tabs(["ì—‘ì…€ ì—…ë¡œë“œ", "CSV ìˆ˜ë™ ì—…ë¡œë“œ", "Google Sheets"])

with tab1:
    xfile = st.file_uploader("ì—‘ì…€ ì—…ë¡œë“œ (.xlsx)", type=["xlsx"], key="excel")
    if xfile is not None:
        df_move, df_ref, df_incoming = load_from_excel(xfile)
        moves_raw = normalize_moves(df_move)
        snap_long = normalize_refined_snapshot(df_ref)


        # WIP ë¶ˆëŸ¬ì˜¤ê¸° ë° ë³‘í•©
        try:
            wip_df = load_wip_from_incoming(df_incoming)
            st.write(f"ğŸ” WIP ë°ì´í„° ë¡œë“œë¨: {len(wip_df)}ê±´")
            if not wip_df.empty:
                st.write("WIP ìƒ˜í”Œ:", wip_df.head())
                # BA00021, BA00022 WIP íŒŒì‹± ê²°ê³¼ í™•ì¸
                ba00021_wip = wip_df[wip_df["resource_code"] == "BA00021"]
                ba00022_wip = wip_df[wip_df["resource_code"] == "BA00022"]
                if not ba00021_wip.empty:
                    st.caption("ğŸ” BA00021 WIP íŒŒì‹± í™•ì¸")
                    st.dataframe(ba00021_wip)
                    st.write("WIP ìƒ˜í”Œ(BA00021):", 
                             wip_df.loc[wip_df["resource_code"]=="BA00021",
                                        ["resource_code","wip_start","wip_ready","qty_ea"]].head(10))
                if not ba00022_wip.empty:
                    st.caption("ğŸ” BA00022 WIP íŒŒì‹± í™•ì¸")
                    st.dataframe(ba00022_wip)
            moves = merge_wip_as_moves(moves_raw, wip_df)
            st.write(f"ğŸ” ìµœì¢… moves ë°ì´í„°: {len(moves)}ê±´")
            if not moves.empty:
                wip_moves = moves[moves["carrier_mode"] == "WIP"]
                st.write(f"ğŸ” WIP moves: {len(wip_moves)}ê±´")
                if not wip_moves.empty:
                    st.write("WIP moves ìƒ˜í”Œ:", wip_moves[["resource_code", "onboard_date", "event_date", "qty_ea"]].head())
                    # BA00021 WIP moves í™•ì¸
                    ba00021_moves = wip_moves[wip_moves["resource_code"] == "BA00021"]
                    if not ba00021_moves.empty:
                        st.write("ğŸ” BA00021 WIP moves:", ba00021_moves[["resource_code", "onboard_date", "event_date", "qty_ea"]].head())
                        st.write("ğŸ” BA00021 event_date ìƒíƒœ:", ba00021_moves["event_date"].isna().sum(), "ê°œê°€ NaT")
                        st.write("ğŸ” BA00021 event_date ê°’ë“¤:", ba00021_moves["event_date"].tolist())
                
                # ë””ë²„ê¹…: moves ë°ì´í„° ì „ì²´ ìƒíƒœ í™•ì¸
                st.write(f"ğŸ” moves ì „ì²´ carrier_mode ë¶„í¬:")
                carrier_counts = moves["carrier_mode"].value_counts()
                st.write(carrier_counts)
                
                # BA00021 moves ì „ì²´ í™•ì¸
                ba00021_all = moves[moves["resource_code"] == "BA00021"]
                st.write(f"ğŸ” BA00021 moves ì „ì²´: {len(ba00021_all)}ê±´")
                if not ba00021_all.empty:
                    st.write("BA00021 moves ì „ì²´ ìƒ˜í”Œ:", ba00021_all[["resource_code", "carrier_mode", "event_date"]].head())
            st.success(f"WIP {len(wip_df)}ê±´ ë°˜ì˜ ì™„ë£Œ" if wip_df is not None and not wip_df.empty else "WIP ì—†ìŒ")
        except Exception as e:
            moves = moves_raw
            st.warning(f"WIP ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")

with tab2:
    cs_snap = st.file_uploader("ì •ì œ ìŠ¤ëƒ…ìƒ· CSV ì—…ë¡œë“œ (snap_ì •ì œ: date,center,resource_code,stock_qty)", type=["csv"], key="snapcsv")
    cs_move = st.file_uploader("SCM_í†µí•©.csv ì—…ë¡œë“œ", type=["csv"], key="movecsv")
    if cs_snap is not None and cs_move is not None:
        df_ref = pd.read_csv(cs_snap)
        move_raw = pd.read_csv(cs_move)

        # ì´ë™ ë°ì´í„° ì •ê·œí™”
        moves = normalize_moves(move_raw)

        # ì •ì œ ìŠ¤ëƒ…ìƒ· ì •ê·œí™” (date/center/resource_code/stock_qtyë¡œ í†µì¼)
        snap_cols = {c.strip().lower(): c for c in df_ref.columns}

        # ìœ ì—° ë§¤í•‘
        col_date = snap_cols.get("date") or snap_cols.get("snapshot_date") or snap_cols.get("ìŠ¤ëƒ…ìƒ· ì¼ì")
        col_center = snap_cols.get("center") or snap_cols.get("ì°½ê³ ëª…")
        col_sku = snap_cols.get("resource_code") or snap_cols.get("sku") or snap_cols.get("ìƒí’ˆì½”ë“œ")
        col_qty = (snap_cols.get("stock_qty") or snap_cols.get("qty") or
                   snap_cols.get("quantity") or snap_cols.get("ìˆ˜ëŸ‰"))

        if not all([col_date, col_center, col_sku, col_qty]):
            st.error("ì •ì œ ìŠ¤ëƒ…ìƒ· CSVì— 'date, center, resource_code, stock_qty' ì»¬ëŸ¼(ë˜ëŠ” ë™ì˜ì–´)ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            st.stop()

        sr = df_ref.rename(columns={
            col_date: "date",
            col_center: "center",
            col_sku: "resource_code",
            col_qty: "stock_qty",
        }).copy()

        sr["date"] = pd.to_datetime(sr["date"], errors="coerce").dt.normalize()
        sr["center"] = sr["center"].astype(str)
        sr["resource_code"] = sr["resource_code"].astype(str)
        sr["stock_qty"] = pd.to_numeric(sr["stock_qty"], errors="coerce").fillna(0).astype(int)

        snap_long = sr[["date","center","resource_code","stock_qty"]].dropna()

with tab3:
    st.subheader("Google Sheets ì—°ë™ (Apps Script Web App)")
    st.info("Google Apps Script Web Appì„ í†µí•´ Google Sheetsì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    
    # ì„¤ì • ìƒíƒœ í‘œì‹œ
    cfg = _get_proxy_cfg()
    if cfg["webapp_url"] and cfg["sheet_id"] and cfg["token"]:
        st.success("âœ… WebApp í”„ë¡ì‹œ ì„¤ì • ì™„ë£Œ")
        st.caption(f"ì‹œíŠ¸ ID: {cfg['sheet_id']}")
    else:
        st.error("âŒ WebApp í”„ë¡ì‹œ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        st.caption("Streamlit Secretsì— gs_proxy ì„¤ì •ì„ ì¶”ê°€í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    
    if st.button("Google Sheetsì—ì„œ ë°ì´í„° ë¡œë“œ", type="primary"):
        try:
            with st.spinner("Google Sheetsì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
                df_move, df_ref, df_incoming = load_from_gsheet()
                moves_raw = normalize_moves(df_move)
                snap_long = normalize_refined_snapshot(df_ref)
                
                # WIP ë¶ˆëŸ¬ì˜¤ê¸° ë° ë³‘í•©
                try:
                    wip_df = load_wip_from_incoming(df_incoming)
                    moves = merge_wip_as_moves(moves_raw, wip_df)
                    st.success(f"Google Sheets ë¡œë“œ ì™„ë£Œ! WIP {len(wip_df)}ê±´ ë°˜ì˜" if wip_df is not None and not wip_df.empty else "Google Sheets ë¡œë“œ ì™„ë£Œ! WIP ì—†ìŒ")
                except Exception as e:
                    moves = moves_raw
                    st.warning(f"WIP ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                    
        except Exception as e:
            st.error(f"Google Sheets ë¡œë“œ ì‹¤íŒ¨: {e}")
            st.caption("WebApp URL, ì‹œíŠ¸ ID, í† í°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

# ì•± ì‹œì‘ ì‹œ Google Sheetsì—ì„œ ìë™ ë¡œë“œ ì‹œë„
if "snap_long" not in locals():
    try:
        with st.spinner("Google Sheetsì—ì„œ ë°ì´í„°ë¥¼ ìë™ ë¡œë“œí•˜ëŠ” ì¤‘..."):
            df_move, df_ref, df_incoming = load_from_gsheet()
            moves_raw = normalize_moves(df_move)
            snap_long = normalize_refined_snapshot(df_ref)
            
            # WIP ë¶ˆëŸ¬ì˜¤ê¸° ë° ë³‘í•©
            try:
                wip_df = load_wip_from_incoming(df_incoming)
                st.write(f"ğŸ” WIP ë°ì´í„° ë¡œë“œë¨: {len(wip_df)}ê±´")
                if not wip_df.empty:
                    st.write("WIP ìƒ˜í”Œ:", wip_df.head())
                    # BA00021, BA00022 WIP íŒŒì‹± ê²°ê³¼ í™•ì¸
                    ba00021_wip = wip_df[wip_df["resource_code"] == "BA00021"]
                    ba00022_wip = wip_df[wip_df["resource_code"] == "BA00022"]
                    if not ba00021_wip.empty:
                        st.caption("ğŸ” BA00021 WIP íŒŒì‹± í™•ì¸")
                        st.dataframe(ba00021_wip)
                        st.write("WIP ìƒ˜í”Œ(BA00021):", 
                                 wip_df.loc[wip_df["resource_code"]=="BA00021",
                                            ["resource_code","wip_start","wip_ready","qty_ea"]].head(10))
                    if not ba00022_wip.empty:
                        st.caption("ğŸ” BA00022 WIP íŒŒì‹± í™•ì¸")
                        st.dataframe(ba00022_wip)
                moves = merge_wip_as_moves(moves_raw, wip_df)
                st.write(f"ğŸ” ìµœì¢… moves ë°ì´í„°: {len(moves)}ê±´")
                if not moves.empty:
                    wip_moves = moves[moves["carrier_mode"] == "WIP"]
                    st.write(f"ğŸ” WIP moves: {len(wip_moves)}ê±´")
                    if not wip_moves.empty:
                        st.write("WIP moves ìƒ˜í”Œ:", wip_moves[["resource_code", "onboard_date", "event_date", "qty_ea"]].head())
                        # BA00021 WIP moves í™•ì¸
                        ba00021_moves = wip_moves[wip_moves["resource_code"] == "BA00021"]
                        if not ba00021_moves.empty:
                            st.write("ğŸ” BA00021 WIP moves:", ba00021_moves[["resource_code", "onboard_date", "event_date", "qty_ea"]].head())
                            st.write("ğŸ” BA00021 event_date ìƒíƒœ:", ba00021_moves["event_date"].isna().sum(), "ê°œê°€ NaT")
                            st.write("ğŸ” BA00021 event_date ê°’ë“¤:", ba00021_moves["event_date"].tolist())
                st.success(f"âœ… Google Sheets ìë™ ë¡œë“œ ì™„ë£Œ! WIP {len(wip_df)}ê±´ ë°˜ì˜" if wip_df is not None and not wip_df.empty else "âœ… Google Sheets ìë™ ë¡œë“œ ì™„ë£Œ! WIP ì—†ìŒ")
            except Exception as e:
                moves = moves_raw
                st.warning(f"âš ï¸ WIP ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                
    except Exception as e:
        st.error(f"âŒ Google Sheets ìë™ ë¡œë“œ ì‹¤íŒ¨: {e}")
        st.info("ì—‘ì…€, CSV ë˜ëŠ” Google Sheetsì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ë©´ í•„í„°/ì°¨íŠ¸ê°€ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.")
        st.caption("WebApp í”„ë¡ì‹œ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. Streamlit Secretsì— gs_proxy ì„¤ì •ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
        st.stop()



# -------------------- Filters --------------------
# ì„¼í„° ëª©ë¡: ìŠ¤ëƒ…ìƒ· + ì´ë™ ë°ì´í„°ì—ì„œ ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì œê±°)
centers_snap = set(snap_long["center"].dropna().astype(str).unique().tolist())
centers_moves = set(moves["from_center"].dropna().astype(str).unique().tolist() + 
                   moves["to_center"].dropna().astype(str).unique().tolist())

# ì„¼í„°ëª… í†µì¼ (AcrossBUS = ì–´í¬ë¡œìŠ¤ë¹„US) + ìœ íš¨í•˜ì§€ ì•Šì€ ì„¼í„° ì œê±°
centers_moves_unified = set()
for center in centers_moves:
    if center == "AcrossBUS":
        centers_moves_unified.add("ì–´í¬ë¡œìŠ¤ë¹„US")
    elif center not in ["WIP", "In-Transit", "", "nan", "None"]:  # ìœ íš¨í•˜ì§€ ì•Šì€ ì„¼í„° ì œê±°
        centers_moves_unified.add(center)

# ìŠ¤ëƒ…ìƒ· ì„¼í„°ë„ ìœ íš¨í•˜ì§€ ì•Šì€ ì„¼í„° ì œê±°
centers_snap_clean = set()
for center in centers_snap:
    if center not in ["WIP", "In-Transit", "", "nan", "None"]:  # ìœ íš¨í•˜ì§€ ì•Šì€ ì„¼í„° ì œê±°
        centers_snap_clean.add(center)

centers = sorted(list(centers_snap_clean | centers_moves_unified))

skus = sorted(snap_long["resource_code"].dropna().astype(str).unique().tolist())
min_date = snap_long["date"].min()
max_date = snap_long["date"].max()

st.sidebar.header("í•„í„°")
centers_sel = st.sidebar.multiselect("ì„¼í„° ì„ íƒ", centers, default=(["íƒœê´‘KR"] if "íƒœê´‘KR" in centers else centers[:1]))
skus_sel = st.sidebar.multiselect("SKU ì„ íƒ", skus, default=([s for s in ["BA00022","BA00021"] if s in skus] or skus[:2]))


# === ê¸°ê°„ ì œì–´: ê³¼ê±° 42ì¼, ë¯¸ë˜ 60ì¼ ===
today = pd.Timestamp.today().normalize()
# íƒ€ì„ì¡´ ë¬¸ì œ í•´ê²°: tz-naiveë¡œ í†µì¼
today = today.tz_localize(None) if today.tz else today
PAST_DAYS = 42  # ê³¼ê±° 42ì¼
FUTURE_DAYS = 60  # ë¯¸ë˜ 60ì¼

# ìŠ¤ëƒ…ìƒ· ë²”ìœ„ì™€ êµì°¨(ìŠ¤ëƒ…ìƒ·ì´ ë” ì§§ì•„ë„ ì•ˆì „)
snap_min = pd.to_datetime(snap_long["date"]).min().normalize()
snap_max = pd.to_datetime(snap_long["date"]).max().normalize()

# íƒ€ì„ì¡´ ë¬¸ì œ í•´ê²°: ëª¨ë“  ë‚ ì§œë¥¼ tz-naiveë¡œ í†µì¼
today = today.tz_localize(None) if today.tz else today
snap_min = snap_min.tz_localize(None) if snap_min.tz else snap_min
snap_max = snap_max.tz_localize(None) if snap_max.tz else snap_max

# snap_longì˜ date ì»¬ëŸ¼ë„ tz-naiveë¡œ í†µì¼
snap_long["date"] = pd.to_datetime(snap_long["date"]).dt.tz_localize(None)

bound_min = max(today - pd.Timedelta(days=PAST_DAYS), snap_min)
bound_max = min(today + pd.Timedelta(days=FUTURE_DAYS), snap_max + pd.Timedelta(days=60))  # +60ì€ ì•½ê°„ì˜ ì „ë§ ì—¬ì§€

def _init_range():
    if "date_range" not in st.session_state:
        st.session_state.date_range = (max(today - pd.Timedelta(days=20), bound_min),
                                       min(today + pd.Timedelta(days=20), bound_max))
    if "horizon_days" not in st.session_state:
        st.session_state.horizon_days = 20

def _apply_horizon_to_range():
    h = int(st.session_state.horizon_days)
    h = max(0, min(h, FUTURE_DAYS))   # â† ì…ë ¥ ìµœëŒ€ë¥¼ 60ì¼ë¡œ í´ë¨í”„
    st.session_state.horizon_days = h
    start = max(today - pd.Timedelta(days=h), bound_min)
    end   = min(today + pd.Timedelta(days=h), bound_max)
    st.session_state.date_range = (start, end)

def _clamp_range(r):
    s, e = pd.Timestamp(r[0]).normalize(), pd.Timestamp(r[1]).normalize()
    s = max(min(s, bound_max), bound_min)
    e = max(min(e, bound_max), bound_min)
    if e < s:
        e = s
    return (s, e)

_init_range()

st.sidebar.subheader("ê¸°ê°„ ì„¤ì •")
st.sidebar.number_input(
    "ë¯¸ë˜ ì „ë§ ì¼ìˆ˜",
    min_value=0, max_value=FUTURE_DAYS, step=1,
    key="horizon_days", on_change=_apply_horizon_to_range
)

# ìŠ¬ë¼ì´ë”: ë²”ìœ„ë¥¼ Â±6ì£¼ ê²½ê³„ë¡œ ì œí•œ
date_range = st.sidebar.slider(
    "ê¸°ê°„",
    min_value=bound_min.to_pydatetime(),
    max_value=bound_max.to_pydatetime(),
    value=tuple(d.to_pydatetime() for d in _clamp_range(st.session_state.date_range)),
    format="YYYY-MM-DD"
)

# ë‚´ë¶€ ì‚¬ìš© ê¸°ê°„(ìŠ¬ë¼ì´ë” ìš°ì„ )
start_dt = pd.Timestamp(date_range[0]).normalize()
end_dt   = pd.Timestamp(date_range[1]).normalize()
# íƒ€ì„ì¡´ ë¬¸ì œ í•´ê²°: ëª¨ë“  ë‚ ì§œë¥¼ tz-naiveë¡œ í†µì¼
start_dt = start_dt.tz_localize(None) if start_dt.tz else start_dt
end_dt = end_dt.tz_localize(None) if end_dt.tz else end_dt

# ì „ë§ì¼(ë¹Œë“œìš©): ìµœì‹  ìŠ¤ëƒ…ìƒ· ì´í›„ë§Œ ê³„ì‚°
_latest_snap = snap_long["date"].max()
# íƒ€ì„ì¡´ ë¬¸ì œ í•´ê²°: ëª¨ë“  ë‚ ì§œë¥¼ tz-naiveë¡œ í†µì¼
_latest_snap = _latest_snap.tz_localize(None) if _latest_snap.tz else _latest_snap
end_dt = end_dt.tz_localize(None) if end_dt.tz else end_dt
proj_days_for_build = max(0, int((end_dt - _latest_snap).days))



st.sidebar.header("í‘œì‹œ ì˜µì…˜")

# í‘œì‹œ í† ê¸€ (ìˆœì„œ/ë¬¸êµ¬ ì •ë¦¬)
show_prod = st.sidebar.checkbox("ìƒì‚°ì¤‘(ë¯¸ì™„ë£Œ) í‘œì‹œ", value=True)
show_transit = st.sidebar.checkbox("ì´ë™ì¤‘ í‘œì‹œ", value=True)

# ì¶”ì„¸ ê¸°ë°˜ ì¬ê³  ì˜ˆì¸¡
use_cons_forecast = st.sidebar.checkbox("ì¶”ì„¸ ê¸°ë°˜ ì¬ê³  ì˜ˆì¸¡", value=True)
lookback_days = st.sidebar.number_input("ì¶”ì„¸ ê³„ì‚° ê¸°ê°„(ì¼)", min_value=7, max_value=56, value=28, step=7)

# í”„ë¡œëª¨ì…˜ ê°€ì¤‘ì¹˜ (ë‹¨ì¼)
with st.sidebar.expander("í”„ë¡œëª¨ì…˜ ê°€ì¤‘ì¹˜(+%)", expanded=False):
    enable_event = st.checkbox("ê°€ì¤‘ì¹˜ ì ìš©", value=False)
    ev_start = st.date_input("ì‹œì‘ì¼")
    ev_end   = st.date_input("ì¢…ë£Œì¼")
    ev_pct   = st.number_input("ê°€ì¤‘ì¹˜(%)", min_value=-100.0, max_value=300.0, value=30.0, step=5.0)
events = [{"start": pd.Timestamp(ev_start).strftime("%Y-%m-%d"),
           "end":   pd.Timestamp(ev_end).strftime("%Y-%m-%d"),
           "uplift": ev_pct/100.0}] if enable_event else []



# -------------------- KPIs --------------------
st.subheader("ìš”ì•½ KPI")

# ìŠ¤ëƒ…ìƒ· ê¸°ì¤€ ìµœì‹ ì¼
latest_dt = snap_long["date"].max()
latest_dt_str = pd.to_datetime(latest_dt).strftime("%Y-%m-%d")

# SKUë³„ í˜„ì¬ ì¬ê³ (ì„ íƒ ì„¼í„° ê¸°ì¤€, ìµœì‹  ìŠ¤ëƒ…ìƒ·)
latest_rows = snap_long[(snap_long["date"] == latest_dt) & (snap_long["center"].isin(centers_sel))]
if latest_rows.empty:
    st.warning("ì„ íƒëœ ì„¼í„°ì— í•´ë‹¹í•˜ëŠ” ìµœì‹  ìŠ¤ëƒ…ìƒ· ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    sku_totals = {sku: 0 for sku in skus_sel}
else:
    sku_totals = {sku: int(latest_rows[latest_rows["resource_code"]==sku]["stock_qty"].sum()) for sku in skus_sel}

# In-Transit (WIP ì œì™¸, ì…/ì¶œê³  ëª¨ë‘ í¬í•¨)
today = pd.Timestamp.today().normalize()
# íƒ€ì„ì¡´ ë¬¸ì œ í•´ê²°: tz-naiveë¡œ í†µì¼
today = today.tz_localize(None) if today.tz else today
# normalize_movesì—ì„œ ì´ë¯¸ astype(str) ì²˜ë¦¬ë¨, carrier_modeë§Œ upper() ì ìš©
moves_typed = moves.copy()
moves_typed["carrier_mode"] = moves_typed["carrier_mode"].str.upper()

# íƒ€ì„ì¡´ ë¬¸ì œ í•´ê²°: ëª¨ë“  ë‚ ì§œ ì»¬ëŸ¼ì„ tz-naiveë¡œ í†µì¼
date_columns = ["onboard_date", "arrival_date", "inbound_date", "real_departure", "event_date"]
for col in date_columns:
    if col in moves_typed.columns:
        moves_typed[col] = pd.to_datetime(moves_typed[col], errors="coerce")
        moves_typed[col] = moves_typed[col].dt.tz_localize(None) if moves_typed[col].dt.tz is not None else moves_typed[col]

# moves ì›ë³¸ ë°ì´í„°ë„ íƒ€ì„ì¡´ í†µì¼
for col in date_columns:
    if col in moves.columns:
        moves[col] = pd.to_datetime(moves[col], errors="coerce")
        moves[col] = moves[col].dt.tz_localize(None) if moves[col].dt.tz is not None else moves[col]

in_transit_mask = (
    (moves_typed["onboard_date"].notna()) &
    (moves_typed["onboard_date"] <= today) &
    (moves_typed["inbound_date"].isna()) &
    ((moves_typed["arrival_date"].isna()) | (moves_typed["arrival_date"] > today)) &
    (moves_typed["to_center"].isin(centers_sel)) &  # ì„ íƒëœ ì„¼í„°ë¡œ ì´ë™ì¤‘ì¸ ì¬ê³ ë§Œ
    (moves_typed["resource_code"].isin(skus_sel)) &
    (moves_typed["carrier_mode"] != "WIP")
)
in_transit_total = int(moves_typed[in_transit_mask]["qty_ea"].sum())

# WIP(ì˜¤ëŠ˜ ê¸°ì¤€ ì”ëŸ‰, ì„ íƒ ì„¼í„°/SKU ë²”ìœ„)
wip_moves = moves_typed[
    (moves_typed["carrier_mode"] == "WIP") &
    (moves_typed["to_center"].isin(centers_sel)) &
    (moves_typed["resource_code"].isin(skus_sel))
]
if not wip_moves.empty:
    on = (wip_moves.dropna(subset=["onboard_date"]).groupby("onboard_date", as_index=True)["qty_ea"].sum())
    ev = (wip_moves.dropna(subset=["event_date"]).groupby("event_date", as_index=True)["qty_ea"].sum() * -1)
    wip_flow = pd.concat([on, ev]).groupby(level=0).sum().sort_index()
    wip_cum = wip_flow[wip_flow.index <= today].cumsum()
    wip_today = int(wip_cum.iloc[-1]) if not wip_cum.empty else 0
else:
    wip_today = 0

# SKUë³„ í˜„ì¬ ì¬ê³  ì¹´ë“œ
def _chunk(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i+n]

for group in _chunk(skus_sel, 4):
    cols = st.columns(len(group))
    for i, sku in enumerate(group):
        cols[i].metric(f"{sku} í˜„ì¬ ì¬ê³ (ìŠ¤ëƒ…ìƒ· {latest_dt_str})", f"{sku_totals.get(sku, 0):,}")

# í†µí•© KPI
k_it, k_wip = st.columns(2)
k_it.metric("ì´ë™ ì¤‘ ì¬ê³ ", f"{in_transit_total:,}")
k_wip.metric("ìƒì‚°ì¤‘(ë¯¸ì™„ë£Œ)", f"{wip_today:,}")

st.divider()

# -------------------- Step Chart (Plotly) --------------------
st.subheader("ê³„ë‹¨ì‹ ì¬ê³  íë¦„")

# ë””ë²„ê¹…: build_timeline í˜¸ì¶œ ì „ moves ë°ì´í„° ìƒíƒœ í™•ì¸
if "BA00021" in skus_sel:
    ba00021_moves = moves[moves["resource_code"] == "BA00021"]
    st.write(f"ğŸ” build_timeline í˜¸ì¶œ ì „ - BA00021 moves: {len(ba00021_moves)}ê±´")
    if not ba00021_moves.empty:
        st.write("moves ìƒ˜í”Œ:", ba00021_moves[["resource_code", "carrier_mode", "event_date"]].head())
        st.write("event_date ìƒíƒœ:", ba00021_moves["event_date"].isna().sum(), "ê°œê°€ NaT")

timeline = build_timeline(snap_long, moves, centers_sel, skus_sel,
                          start_dt, end_dt, horizon_days=proj_days_for_build)

# (A) ì†Œì§„ ì¶”ì„¸ + ì´ë²¤íŠ¸ ê°€ì¤‘ì¹˜ ì ìš©
if use_cons_forecast and not timeline.empty:
    timeline = apply_consumption_with_events(
        timeline, snap_long, centers_sel, skus_sel,
        start_dt, end_dt,
        lookback_days=int(lookback_days),
        events=events
    )

if timeline.empty:
    st.info("ì„ íƒ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” íƒ€ì„ë¼ì¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    vis_df = timeline[(timeline["date"] >= start_dt) & (timeline["date"] <= end_dt)].copy()

    # (B) ì„¼í„° í‘œì‹œ ëª…ì¹­ í•œê¸€í™” + í† ê¸€
    # In-Transit â†’ ì´ë™ì¤‘ (í†µí•©)
    vis_df["center"] = vis_df["center"].str.replace(r"^In-Transit.*$", "ì´ë™ì¤‘", regex=True)
    # WIP â†’ ìƒì‚°ì¤‘
    vis_df.loc[vis_df["center"] == "WIP", "center"] = "ìƒì‚°ì¤‘"

    # âœ… ìƒì‚°ì¤‘(WIP)ì€ ì˜¤ëŠ˜ ì´í›„ë§Œ í‘œì‹œ (ê³¼ê±° êµ¬ê°„ ìˆ¨ê¹€)
    today = pd.Timestamp.today().normalize()
    vis_df = vis_df[~((vis_df["center"] == "ìƒì‚°ì¤‘") & (vis_df["date"] < today))]

    # âœ… íƒœê´‘KR ë¯¸ì„ íƒ ì‹œ 'ìƒì‚°ì¤‘' ë¼ì¸ ìˆ¨ê¹€
    if "íƒœê´‘KR" not in centers_sel:
        vis_df = vis_df[vis_df["center"] != "ìƒì‚°ì¤‘"]
    
    if not show_prod:
        vis_df = vis_df[vis_df["center"] != "ìƒì‚°ì¤‘"]
    if not show_transit:
        vis_df = vis_df[~vis_df["center"].str.startswith("ì´ë™ì¤‘")]
    
    # 0 ê°’ ë°ì´í„° í•„í„°ë§ (ì°¨íŠ¸ í‘œì‹œ ë¬¸ì œ í•´ê²°)
    vis_df = vis_df[vis_df["stock_qty"] > 0]

    # ë¼ë²¨
    vis_df["label"] = vis_df["resource_code"] + " @ " + vis_df["center"]

    # (C) ê·¸ë¦¬ê¸°
    fig = px.line(
        vis_df, x="date", y="stock_qty", color="label",
        line_shape="hv",
        title="ì„ íƒí•œ SKU Ã— ì„¼í„°(ë° ì´ë™ì¤‘/ìƒì‚°ì¤‘) ê³„ë‹¨ì‹ ì¬ê³  íë¦„",
        render_mode="svg"
    )
    fig.update_layout(
        hovermode="x unified",
        xaxis_title="ë‚ ì§œ",
        yaxis_title="ì¬ê³ ëŸ‰(EA)",
        legend_title_text="SKU @ Center / ì´ë™ì¤‘(ì ì„ ) / ìƒì‚°ì¤‘(ì ì„ )",
        margin=dict(l=20, r=20, t=60, b=20)
    )

    # === ì˜¤ëŠ˜ ê¸°ì¤€ì„  í‘œì‹œ ===
    today = pd.Timestamp.today().normalize()
    # íƒ€ì„ì¡´ ë¬¸ì œ í•´ê²°: tz-naiveë¡œ í†µì¼
    today = today.tz_localize(None) if today.tz else today
    if start_dt <= today <= end_dt:
        fig.add_vline(
            x=today,
            line_width=1,
            line_dash="solid",  # ì‹¤ì„ ìœ¼ë¡œ ë³€ê²½
            line_color="rgba(255, 0, 0, 0.4)",  # ë” í¬ë¯¸í•œ ë°˜íˆ¬ëª… ë¹¨ê°„ìƒ‰
        )
        fig.add_annotation(
            x=today,
            y=1.02,
            xref="x",
            yref="paper",
            text="ì˜¤ëŠ˜",
            showarrow=False,
            font=dict(size=12, color="#555"),
            align="center",
            yanchor="bottom",
        )


    
    # Yì¶• ëˆˆê¸ˆ ì •ìˆ˜ë¡œ
    fig.update_yaxes(tickformat=",.0f")
    # í˜¸ë²„ë„ ì •ìˆ˜ ì²œë‹¨ìœ„
    fig.update_traces(hovertemplate="ë‚ ì§œ: %{x|%Y-%m-%d}<br>ì¬ê³ : %{y:,.0f} EA<br>%{fullData.name}<extra></extra>")


    # (D) ìƒ‰ìƒ/ì„  ìŠ¤íƒ€ì¼ - ê° ë¼ì¸ë§ˆë‹¤ ë‹¤ë¥¸ ìƒ‰ìƒ ì‚¬ìš©
    PALETTE = [
        "#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd",
        "#8c564b", "#e377c2", "#7f7f7f", "#bcbd22", "#17becf",
        "#aec7e8", "#ffbb78", "#98df8a", "#ff9896", "#c5b0d5",
        "#c49c94", "#f7b6d3", "#c7c7c7", "#dbdb8d", "#9edae5",
        "#4e79a7", "#f28e2b", "#e15759", "#76b7b2", "#59a14f",
        "#edc948", "#b07aa1", "#ff9da7", "#9c755f", "#bab0ac"
    ]
    
    # ê° ë¼ì¸ë§ˆë‹¤ ê³ ìœ í•œ ìƒ‰ìƒ í• ë‹¹
    line_colors = {}
    color_idx = 0
    for tr in fig.data:
        name = tr.name or ""
        if " @ " in name:
            if name not in line_colors:
                line_colors[name] = PALETTE[color_idx % len(PALETTE)]
                color_idx += 1

    for i, tr in enumerate(fig.data):
        name = tr.name or ""
        if " @ " not in name:
            continue
        sku, kind = name.split(" @ ", 1)
        line_color = line_colors.get(name, PALETTE[0])

        if kind == "ì´ë™ì¤‘":
            # ì ì„  + ë‘ê»˜ 1.2
            fig.data[i].update(line=dict(color=line_color, dash="dot", width=1.2), opacity=0.9)
            fig.data[i].legendgroup = f"{sku} (ì´ë™ì¤‘)"
            fig.data[i].legendrank = 20
        elif kind == "ìƒì‚°ì¤‘":
            # íŒŒì„  + ë‘ê»˜ 1.0
            fig.data[i].update(line=dict(color=line_color, dash="dash", width=1.0), opacity=0.8)
            fig.data[i].legendgroup = f"{sku} (ìƒì‚°ì¤‘)"
            fig.data[i].legendrank = 30
        else:
            # ì‹¤ì„  + ë‘ê»˜ 1.5
            fig.data[i].update(line=dict(color=line_color, dash="solid", width=1.5), opacity=1.0)
            fig.data[i].legendgroup = f"{sku} (ì„¼í„°)"
            fig.data[i].legendrank = 10


    chart_key = (
        f"stepchart|centers={','.join(centers_sel)}|skus={','.join(skus_sel)}|"
        f"{start_dt:%Y%m%d}-{end_dt:%Y%m%d}"
        f"|h{int(st.session_state.horizon_days)}"
        f"|prod{int(show_prod)}|tran{int(show_transit)}"
    )

    st.plotly_chart(fig, use_container_width=True, config={"displaylogo": False}, key=chart_key)



# -------------------- Upcoming Arrivals --------------------
st.subheader("ì…ê³  ì˜ˆì • ë‚´ì—­ (ì„ íƒ ì„¼í„°/SKU)")

today = pd.Timestamp.today().normalize()
# íƒ€ì„ì¡´ ë¬¸ì œ í•´ê²°: tz-naiveë¡œ í†µì¼
today = today.tz_localize(None) if today.tz else today
window_start = max(start_dt, today)   # âœ… ì˜¤ëŠ˜ ì´í›„ë§Œ
window_end   = end_dt
# íƒ€ì„ì¡´ ë¬¸ì œ í•´ê²°: window_startì™€ window_endë„ tz-naiveë¡œ í†µì¼
window_start = window_start.tz_localize(None) if window_start.tz else window_start
window_end = window_end.tz_localize(None) if window_end.tz else window_end

# (A) ìš´ì†¡(ë¹„ WIP) - ê¸°ì¡´ íƒ€ì… ë³€í™˜ëœ ë°ì´í„° ì¬ì‚¬ìš©
arr_transport = moves_typed[
    (moves_typed["event_date"].notna()) &
    (moves_typed["event_date"] >= window_start) & (moves_typed["event_date"] <= window_end) &
    (moves_typed["carrier_mode"] != "WIP") &
    (moves_typed["to_center"].isin([c for c in centers_sel if c != "íƒœê´‘KR"])) &
    (moves_typed["resource_code"].isin(skus_sel))
]

# (B) WIP - íƒœê´‘KR ì„ íƒ ì‹œ
arr_wip = pd.DataFrame()
if "íƒœê´‘KR" in centers_sel:
    arr_wip = moves_typed[
        (moves_typed["event_date"].notna()) &
        (moves_typed["event_date"] >= window_start) & (moves_typed["event_date"] <= window_end) &
        (moves_typed["carrier_mode"] == "WIP") &
        (moves_typed["to_center"] == "íƒœê´‘KR") &
        (moves_typed["resource_code"].isin(skus_sel))
    ]

upcoming = pd.concat([arr_transport, arr_wip], ignore_index=True)

if upcoming.empty:
    st.caption("ë„ì°© ì˜ˆì • ì—†ìŒ (ì˜¤ëŠ˜ ì´í›„ / ì„ íƒ ê¸°ê°„)")
else:
    # âœ… days_to_arrivalëŠ” í•­ìƒ 0 ì´ìƒ
    upcoming["days_to_arrival"] = (upcoming["event_date"] - today).dt.days
    upcoming = upcoming.sort_values(["event_date","to_center","resource_code","qty_ea"], 
                                   ascending=[True,True,True,False])
    cols = ["event_date","days_to_arrival","to_center","resource_code","qty_ea","carrier_mode","onboard_date","lot"]
    cols = [c for c in cols if c in upcoming.columns]
    st.dataframe(upcoming[cols].head(1000), use_container_width=True, height=300)


# -------------------- Simulation --------------------
st.subheader("ì¶œê³  ê°€ëŠ¥ ì‹œë®¬ë ˆì´ì…˜")
sim_c1, sim_c2, sim_c3, sim_c4 = st.columns(4)
with sim_c1:
    sim_center = st.selectbox("ì„¼í„°", centers, index=max(0, centers.index("íƒœê´‘KR") if "íƒœê´‘KR" in centers else 0))
with sim_c2:
    default_skus = [s for s in ["BA00022","BA00021"] if s in skus] or skus
    sim_sku = st.selectbox("SKU", skus, index=max(0, skus.index(default_skus[0])))
with sim_c3:
    sim_days = st.number_input(f"ë©°ì¹  ë’¤ (ê¸°ì¤€ì¼: {today.strftime('%Y-%m-%d')})", min_value=0, max_value=60, value=20, step=1)
    st.caption(f"â†’ ëª©í‘œì¼: {(today + pd.Timedelta(days=int(sim_days))).strftime('%Y-%m-%d')}")
with sim_c4:
    sim_qty = st.number_input("í•„ìš” ìˆ˜ëŸ‰", min_value=0, step=1000, value=20000)

# === ì¶œê³  ê°€ëŠ¥ ì‹œë®¬ë ˆì´ì…˜ (ì†Œì§„/ì´ë²¤íŠ¸ ì ìš© í¬í•¨) ===
sim_target_dt = (today + pd.Timedelta(days=int(sim_days))).normalize()

# 1) íƒ€ì„ë¼ì¸ ìƒì„± (ì‹œë®¬ ìœˆë„ìš°ë§Œ)
sim_tl = build_timeline(
    snap_long, moves,
    centers_sel=[sim_center], skus_sel=[sim_sku],
    start_dt=today, end_dt=sim_target_dt,
    horizon_days=max(0, (sim_target_dt - snap_long["date"].max()).days)
)

# 2) ì†Œì§„ + ì´ë²¤íŠ¸ ê°€ì¤‘ì¹˜ ë™ì¼í•˜ê²Œ ì ìš© (ë³¸ë¬¸ê³¼ ì™„ì „ ë™ì¼)
if use_cons_forecast and not sim_tl.empty:
    sim_tl = apply_consumption_with_events(
        sim_tl, snap_long,
        centers_sel=[sim_center], skus_sel=[sim_sku],
        start_dt=today, end_dt=sim_target_dt,
        lookback_days=int(lookback_days),
        events=events  # ì‚¬ì´ë“œë°” ë‹¨ì¼ ì´ë²¤íŠ¸ ì„¸íŒ… ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš©
    )

if sim_tl.empty:
    st.info("í•´ë‹¹ ì¡°í•©ì˜ íƒ€ì„ë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤.")
else:
    # 3) ëª©í‘œì¼ ì¬ê³ (ì‹¤ì œ ì„¼í„° ë¼ì¸ë§Œ: ì´ë™ì¤‘/WIP ì œì™¸)
    real_mask = (
        (sim_tl["center"] == sim_center) &
        (~sim_tl["center"].isin(["WIP"])) &
        (~sim_tl["center"].str.startswith("In-Transit", na=False))
    )
    sim_stock = int(pd.to_numeric(
        sim_tl.loc[(sim_tl["date"] == sim_target_dt) & real_mask, "stock_qty"],
        errors="coerce"
    ).fillna(0).sum().round())

    # 4) ê²°ê³¼ í‘œì‹œ (ë³¸ë¬¸ê³¼ ì¼ì¹˜)
    ok = sim_stock >= sim_qty
    st.metric(
        f"{int(sim_days)}ì¼ ë’¤({sim_target_dt:%Y-%m-%d}) '{sim_center}'ì˜ '{sim_sku}' ì˜ˆìƒ ì¬ê³ ",
        f"{sim_stock:,}",
        delta=f"í•„ìš” {sim_qty:,}"
    )
    if ok:
        st.success("ì¶œê³  ê°€ëŠ¥")
    else:
        st.error("ì¶œê³  ë¶ˆê°€")

    # (ì„ íƒ) ë””ë²„ê·¸ ë°°ì§€: í˜„ì¬ ì†Œì§„ ì¶”ì •ì¹˜
    if use_cons_forecast:
        rates_dbg = estimate_daily_consumption(
            snap_long, [sim_center], [sim_sku],
            asof_dt=pd.to_datetime(snap_long["date"]).max().normalize(),
            lookback_days=int(lookback_days)
        )
        r0 = float(next(iter(rates_dbg.values()), 0.0))
        st.caption(f"ì†Œì§„ ì¶”ì •(ìµœê·¼ {int(lookback_days)}ì¼): {sim_center} / {sim_sku} â‰ˆ {int(round(r0))} EA/ì¼"
                   + (" Â· ì´ë²¤íŠ¸ ê°€ì¤‘ì¹˜ ì ìš©" if events else ""))

# ==================== ì„ íƒ ì„¼í„° í˜„ì¬ ì¬ê³  (ì „ì²´ SKU) ====================
st.subheader(f"ì„ íƒ ì„¼í„° í˜„ì¬ ì¬ê³  (ìŠ¤ëƒ…ìƒ· {latest_dt_str} / ì „ì²´ SKU)")

# 1) ìµœì‹  ìŠ¤ëƒ…ìƒ·ì—ì„œ ì„ íƒ ì„¼í„°ë§Œ ì¶”ì¶œ
cur = snap_long[
    (snap_long["date"] == latest_dt) &
    (snap_long["center"].isin(centers_sel))
].copy()

# 2) SKUÃ—ì„¼í„° í”¼ë²— (ë¹ˆ ê°’ì€ 0)
pivot = (
    cur.groupby(["resource_code", "center"], as_index=False)["stock_qty"].sum()
       .pivot(index="resource_code", columns="center", values="stock_qty")
       .fillna(0)
       .astype(int)
)

# 3) ì´í•© ì»¬ëŸ¼ ì¶”ê°€
pivot["ì´í•©"] = pivot.sum(axis=1)

# 4) UX: í•„í„°/ì •ë ¬ ì˜µì…˜
# UI ê°œì„ : ì²´í¬ë°•ìŠ¤ë¥¼ ìƒë‹¨ìœ¼ë¡œ ì´ë™
col1, col2 = st.columns([2, 1])
with col1:
    q = st.text_input("SKU í•„í„°(í¬í•¨ ê²€ìƒ‰)", "", key="sku_filter_text")
with col2:
    sort_by = st.selectbox("ì •ë ¬ ê¸°ì¤€", ["ì´í•©"] + list(pivot.columns.drop("ì´í•©")), index=0)

# ì²´í¬ë°•ìŠ¤ë“¤ì„ ë³„ë„ í–‰ì— ë°°ì¹˜
col1, col2 = st.columns(2)
with col1:
    hide_zero = st.checkbox("ì´í•©=0 ìˆ¨ê¸°ê¸°", value=True)
with col2:
    show_cost = st.checkbox("ì¬ê³ ìì‚°(ì œì¡°ì›ê°€) í‘œì‹œ", value=False)

# 5) í•„í„° ì ìš©
view = pivot.copy()
if q.strip():
    view = view[view.index.str.contains(q.strip(), case=False, regex=False)]
if hide_zero:
    view = view[view["ì´í•©"] > 0]

# 6) ì •ë ¬(ë‚´ë¦¼ì°¨ìˆœ)  
view = view.sort_values(by=sort_by, ascending=False)

# 7) ë¹„ìš© í†µí•© ë¡œì§
if show_cost:
    snap_raw_df = load_snapshot_raw()
    cost_pivot = pivot_inventory_cost_from_raw(snap_raw_df, latest_dt, centers_sel)  # SKU Ã— ì„¼í„°ë¹„ìš©
    # ì´ë¹„ìš© ì»¬ëŸ¼
    if not cost_pivot.empty:
        cost_cols = [c for c in cost_pivot.columns if c.endswith("_ì¬ê³ ìì‚°")]
        cost_pivot["ì´ ì¬ê³ ìì‚°"] = cost_pivot[cost_cols].sum(axis=1).astype(int)
        # ìˆ˜ëŸ‰ view(í”¼ë²—)ê³¼ ë³‘í•©
        merged = view.reset_index().rename(columns={"resource_code":"SKU"}) \
                     .merge(cost_pivot.rename(columns={"resource_code":"SKU"}), on="SKU", how="left")
        # ê²°ì¸¡ 0
        cost_cols2 = [c for c in merged.columns if c.endswith("_ì¬ê³ ìì‚°")] + (["ì´ ì¬ê³ ìì‚°"] if "ì´ ì¬ê³ ìì‚°" in merged.columns else [])
        if cost_cols2:
            merged[cost_cols2] = merged[cost_cols2].fillna(0).astype(int)
            # ì¬ê³ ìì‚° ì»¬ëŸ¼ í¬ë§·íŒ… (ì²œ ë‹¨ìœ„ êµ¬ë¶„ì + ì›)
            for col in cost_cols2:
                merged[col] = merged[col].apply(lambda x: f"{x:,}ì›" if pd.notna(x) else "0ì›")
        # ì»¬ëŸ¼ ìˆœì„œ: [SKU] + (ì„¼í„° ìˆ˜ëŸ‰ë“¤) + [ì´í•©] + (ì„¼í„°ì¬ê³ ìì‚°ë“¤) + [ì´ ì¬ê³ ìì‚°]
        qty_center_cols = [c for c in merged.columns if c not in ["SKU","ì´í•©"] + cost_cols2]
        ordered = ["SKU"] + qty_center_cols + (["ì´í•©"] if "ì´í•©" in merged.columns else []) + cost_cols2
        merged = merged[ordered]
        show_df = merged
    else:
        show_df = view.reset_index().rename(columns={"resource_code":"SKU"})
else:
    show_df = view.reset_index().rename(columns={"resource_code":"SKU"})

# 8) ìˆ˜ëŸ‰ ì»¬ëŸ¼ í¬ë§·íŒ… (ì²œ ë‹¨ìœ„ êµ¬ë¶„ì)
qty_columns = [col for col in show_df.columns if col not in ["SKU"] and not col.endswith("_ì¬ê³ ìì‚°") and col != "ì´ ì¬ê³ ìì‚°"]
for col in qty_columns:
    if col in show_df.columns:
        show_df[col] = show_df[col].apply(lambda x: f"{x:,}" if pd.notna(x) and isinstance(x, (int, float)) else x)

# 9) ë³´ì—¬ì£¼ê¸°
st.dataframe(show_df, use_container_width=True, height=380)

# 10) CSV ë‹¤ìš´ë¡œë“œ
csv_bytes = show_df.to_csv(index=False).encode("utf-8-sig")
st.download_button(
    "í˜„ì¬ í‘œ CSV ë‹¤ìš´ë¡œë“œ",
    data=csv_bytes,
    file_name=f"centers_{'-'.join(centers_sel)}_snapshot_{latest_dt_str}.csv",
    mime="text/csv"
)

st.caption("â€» ì´ í‘œëŠ” **ì„ íƒëœ ì„¼í„° ì „ì²´ SKU**ì˜ ìµœì‹  ìŠ¤ëƒ…ìƒ· ì¬ê³ ì…ë‹ˆë‹¤. ìƒë‹¨ 'SKU ì„ íƒ'ê³¼ ë¬´ê´€í•˜ê²Œ ëª¨ë“  SKUê°€ í¬í•¨ë©ë‹ˆë‹¤.")

# === ë¡œíŠ¸ ìƒì„¸ ìë™ í‘œì‹œ (ì„ íƒ SKUê°€ 1ê°œì¼ ë•Œ) ===
# â€» show_dfëŠ” ìœ„ì—ì„œ ì‹¤ì œë¡œ st.dataframeì— í‘œì‹œí•œ í…Œì´ë¸”
#    (cost ë¯¸í‘œì‹œ: view.reset_index().rename(columns={"resource_code":"SKU"})
#     cost í‘œì‹œ:   merged â†’ show_df) ë¡œ êµ¬ì„±ë¨.

# 1) í˜„ì¬ í™”ë©´ì— ë³´ì´ëŠ” í‘œì—ì„œ SKU ëª©ë¡ ì¶”ì¶œ
if 'show_df' in locals() and "SKU" in show_df.columns:
    filtered_df = show_df
else:
    # í˜¹ì‹œë¼ë„ show_dfê°€ ì—†ìœ¼ë©´ viewë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒì„±
    filtered_df = view.reset_index().rename(columns={"resource_code": "SKU"})

visible_skus = (
    filtered_df["SKU"].dropna().astype(str).unique().tolist()
)

# 2) SKUê°€ 1ê°œì¼ ë•Œë§Œ ë¡œíŠ¸ ìƒì„¸ í‘œì‹œ
if len(visible_skus) == 1:
    lot_sku = visible_skus[0]
    snap_raw_df = load_snapshot_raw()
    lot_tbl = build_lot_detail(snap_raw_df, latest_dt, lot_sku, centers_sel)

    st.markdown(f"### ë¡œíŠ¸ ìƒì„¸ (ìŠ¤ëƒ…ìƒ· {latest_dt_str} / **{', '.join(centers_sel)}** / **{lot_sku}**)")
    if lot_tbl.empty:
        st.caption("í•´ë‹¹ ì¡°ê±´ì˜ ë¡œíŠ¸ ìƒì„¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.dataframe(lot_tbl, use_container_width=True, height=320)
        st.download_button(
            "ë¡œíŠ¸ ìƒì„¸ CSV ë‹¤ìš´ë¡œë“œ",
            data=lot_tbl.to_csv(index=False).encode("utf-8-sig"),
            file_name=f"lot_detail_{lot_sku}_{latest_dt:%Y%m%d}.csv",
            mime="text/csv"
        )
else:
    st.caption("â€» íŠ¹ì • SKU í•œ ê°œë§Œ í•„í„°ë§í•˜ë©´ ë¡œíŠ¸ ìƒì„¸ê°€ ìë™ í‘œì‹œë©ë‹ˆë‹¤.")