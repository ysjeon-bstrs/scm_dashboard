import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from typing import List, Dict, Tuple, Optional
import os, time, json, requests

# === Apps Script Web App í”„ë¡ì‹œ ì„¤ì • ===
def _get_proxy_cfg():
    if "gs_proxy" in st.secrets:
        cfg = st.secrets["gs_proxy"]
        return dict(
            webapp_url = cfg.get("webapp_url", "").strip(),
            sheet_id   = cfg.get("sheet_id", "").strip(),
            token      = cfg.get("token", "").strip(),
            timeout_s  = int(cfg.get("timeout_s", 15)),
        )
    return dict(
        webapp_url = os.getenv("GS_WEBAPP_URL","").strip(),
        sheet_id   = os.getenv("GS_SHEET_ID","").strip(),
        token      = os.getenv("GS_TOKEN","").strip(),
        timeout_s  = int(os.getenv("GS_TIMEOUT","15")),
    )

def _fetch_sheet_via_webapp(tab_name: str) -> pd.DataFrame:
    cfg = _get_proxy_cfg()
    if not (cfg["webapp_url"] and cfg["sheet_id"] and cfg["token"]):
        st.error("WebApp í”„ë¡ì‹œ ì„¤ì •ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. (webapp_url/sheet_id/token)")
        return pd.DataFrame()

    params = {
        "id": cfg["sheet_id"],
        "sheet": tab_name,
        "token": cfg["token"],
        "_": str(int(time.time()))
    }

    last_err = None
    for i in range(3):
        try:
            r = requests.get(cfg["webapp_url"], params=params, timeout=cfg["timeout_s"])
            r.raise_for_status()
            data = r.json()
            if not data.get("ok"):
                raise RuntimeError(f"Proxy error: {data.get('error')}")
            rows = data.get("rows", [])
            return pd.DataFrame(rows)
        except Exception as e:
            last_err = e
            time.sleep(0.6 * (i+1))
    st.error(f"í”„ë¡ì‹œ í˜¸ì¶œ ì‹¤íŒ¨: {last_err}")
    return pd.DataFrame()

def gs_csv(sheet_name: str) -> pd.DataFrame:
    return _fetch_sheet_via_webapp(sheet_name)

# === ë°ì´í„° ë¡œë”© í•¨ìˆ˜ë“¤ ===
def load_from_excel(uploaded_file) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """Excel íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
    with st.spinner("Excel íŒŒì¼ì„ ì½ëŠ” ì¤‘..."):
        df_move = pd.read_excel(uploaded_file, sheet_name="SCM_í†µí•©")
        df_ref = pd.read_excel(uploaded_file, sheet_name="snap_ì •ì œ")
        df_incoming = pd.read_excel(uploaded_file, sheet_name="ì…ê³ ì˜ˆì •ë‚´ì—­")
    return df_move, df_ref, df_incoming

def load_from_gsheet() -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """Google Sheetsì—ì„œ ë°ì´í„° ë¡œë“œ"""
    with st.spinner("Google Sheetsì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘..."):
        df_move = gs_csv("SCM_í†µí•©")
        df_ref = gs_csv("snap_ì •ì œ")
        df_incoming = gs_csv("ì…ê³ ì˜ˆì •ë‚´ì—­")
    return df_move, df_ref, df_incoming

def normalize_moves(df_move: pd.DataFrame) -> pd.DataFrame:
    """ì´ë™ ë‚´ì—­ ë°ì´í„° ì •ê·œí™”"""
    df = df_move.copy()
    df.columns = [str(c).strip() for c in df.columns]
    
    # ì»¬ëŸ¼ ë§¤í•‘
    col_map = {
        "resource_code": ["resource_code", "ìƒí’ˆì½”ë“œ", "product_code"],
        "qty_ea": ["qty_ea", "ìˆ˜ëŸ‰", "quantity"],
        "carrier_mode": ["carrier_mode", "ìš´ì†¡ìˆ˜ë‹¨", "carrier_name"],
        "from_center": ["from_center", "ì¶œë°œì„¼í„°", "ì¶œë°œì°½ê³ "],
        "to_center": ["to_center", "ë„ì°©ì„¼í„°", "ë„ì°©ì°½ê³ "],
        "onboard_date": ["onboard_date", "ì¶œë°œì¼", "departure_date"],
        "arrival_date": ["arrival_date", "ë„ì°©ì¼", "arrival_date"],
        "inbound_date": ["inbound_date", "ì…ê³ ì¼", "inbound_date"],
        "real_departure": ["real_departure", "ì‹¤ì œì¶œë°œì¼", "actual_departure"],
        "event_date": ["event_date", "ì´ë²¤íŠ¸ì¼", "event_date"]
    }
    
    result = pd.DataFrame()
    for target_col, source_cols in col_map.items():
        for source_col in source_cols:
            if source_col in df.columns:
                result[target_col] = df[source_col]
                break
        if target_col not in result.columns:
            result[target_col] = pd.NaT if "date" in target_col else ""
    
    # ìˆ˜ëŸ‰ ì •ê·œí™” (ì‰¼í‘œ ì œê±°)
    result["qty_ea"] = pd.to_numeric(result["qty_ea"].astype(str).str.replace(',', ''), errors='coerce').fillna(0).astype(int)
    
    # ë‚ ì§œ ì •ê·œí™”
    for date_col in ["onboard_date", "arrival_date", "inbound_date", "real_departure", "event_date"]:
        result[date_col] = pd.to_datetime(result[date_col], errors='coerce')
    
    return result

def normalize_refined_snapshot(df_ref: pd.DataFrame) -> pd.DataFrame:
    """ì •ì œëœ ìŠ¤ëƒ…ìƒ· ë°ì´í„° ì •ê·œí™”"""
    df = df_ref.copy()
    df.columns = [str(c).strip() for c in df.columns]
    
    # ì»¬ëŸ¼ ë§¤í•‘
    col_map = {
        "date": ["date", "ë‚ ì§œ", "snapshot_date"],
        "center": ["center", "ì„¼í„°", "ì°½ê³ "],
        "resource_code": ["resource_code", "ìƒí’ˆì½”ë“œ", "product_code"],
        "stock_qty": ["stock_qty", "ì¬ê³ ìˆ˜ëŸ‰", "quantity"]
    }
    
    result = pd.DataFrame()
    for target_col, source_cols in col_map.items():
        for source_col in source_cols:
            if source_col in df.columns:
                result[target_col] = df[source_col]
                break
        if target_col not in result.columns:
            result[target_col] = pd.NaT if target_col == "date" else 0
    
    # ë‚ ì§œ ì •ê·œí™”
    result["date"] = pd.to_datetime(result["date"], errors='coerce')
    
    # ìˆ˜ëŸ‰ ì •ê·œí™”
    result["stock_qty"] = pd.to_numeric(result["stock_qty"], errors='coerce').fillna(0).astype(int)
    
    return result

def load_wip_from_incoming(df_incoming: pd.DataFrame) -> pd.DataFrame:
    """ì…ê³ ì˜ˆì •ë‚´ì—­ì—ì„œ WIP ë°ì´í„° ë¡œë“œ"""
    df = df_incoming.copy()
    df.columns = [str(c).strip() for c in df.columns]
    
    # ì»¬ëŸ¼ ë§¤í•‘
    sku_col = next((c for c in df.columns if "product_code" in c.lower() or "resource_code" in c.lower()), None)
    qty_col = next((c for c in df.columns if "total_quantity" in c.lower() or "quantity" in c.lower()), None)
    date_col = next((c for c in df.columns if "intended_push_date" in c.lower() or "push" in c.lower()), None)
    
    if not all([sku_col, qty_col, date_col]):
        return pd.DataFrame()
    
    result = pd.DataFrame({
        "resource_code": df[sku_col].astype(str).str.strip(),
        "to_center": "íƒœê´‘KR",  # ê¸°ë³¸ ë„ì°© ì„¼í„°
        "wip_ready": pd.to_datetime(df[date_col], errors='coerce').dt.normalize(),
        "qty_ea": pd.to_numeric(df[qty_col].astype(str).str.replace(',', ''), errors='coerce').fillna(0).astype(int),
        "lot": df.get("lot", "")
    })
    
    # wip_start ê³„ì‚° (wip_ready - 30ì¼)
    result["wip_start"] = result["wip_ready"] - pd.to_timedelta(30, unit="D")
    
    # ìœ íš¨ê°’ë§Œ ë°˜í™˜
    result = result.dropna(subset=["resource_code", "wip_ready", "wip_start"]).reset_index(drop=True)
    return result[["resource_code", "to_center", "wip_start", "wip_ready", "qty_ea", "lot"]]

def merge_wip_as_moves(moves_df: pd.DataFrame, wip_df: Optional[pd.DataFrame]) -> pd.DataFrame:
    """WIP ë°ì´í„°ë¥¼ movesì— ë³‘í•©"""
    if wip_df is None or wip_df.empty:
        return moves_df
    
    wip_moves = pd.DataFrame({
        "resource_code": wip_df["resource_code"],
        "qty_ea": wip_df["qty_ea"].astype(int),
        "carrier_mode": "WIP",
        "from_center": "WIP",
        "to_center": wip_df["to_center"],
        "onboard_date": wip_df["wip_start"].dt.normalize(),
        "arrival_date": wip_df["wip_ready"].dt.normalize(),
        "inbound_date": pd.NaT,
        "real_departure": pd.NaT,
        "event_date": wip_df["wip_ready"].dt.normalize(),
        "lot": wip_df.get("lot", "")
    })
    
    return pd.concat([moves_df, wip_moves], ignore_index=True)

@st.cache_data(ttl=1800)
def build_timeline(snap_long: pd.DataFrame, moves: pd.DataFrame, 
                   centers_sel: List[str], skus_sel: List[str],
                   start_dt: pd.Timestamp, end_dt: pd.Timestamp, 
                   horizon_days: int = 0) -> pd.DataFrame:
    """ìƒˆë¡œìš´ WIP ì²˜ë¦¬ ë¡œì§ - event_date ì†ì‹¤ ë¬¸ì œ í•´ê²°"""
    horizon_end = end_dt + pd.Timedelta(days=horizon_days)
    full_dates = pd.date_range(start_dt, horizon_end, freq="D")

    # 1) ì‹¤ì œ ì„¼í„° ë¼ì¸ (ìŠ¤ëƒ…ìƒ· ê¸°ë°˜)
    base = snap_long[
        snap_long["center"].isin(centers_sel) &
        snap_long["resource_code"].isin(skus_sel)
    ].copy().rename(columns={"snapshot_date":"date"})
    
    if base.empty:
        return pd.DataFrame(columns=["date","center","resource_code","stock_qty"])
    
    base = base[(base["date"] >= start_dt) & (base["date"] <= end_dt)]
    lines = []

    # ì‹¤ì œ ì„¼í„° ë¼ì¸ ìƒì„±
    for (ct, sku), grp in base.groupby(["center","resource_code"]):
        grp = grp.sort_values("date")
        last_dt = grp["date"].max()

        if horizon_days > 0:
            proj_dates = pd.date_range(last_dt + pd.Timedelta(days=1), horizon_end, freq="D")
            proj_df = pd.DataFrame({"date": proj_dates, "center": ct,
                                  "resource_code": sku, "stock_qty": 0})
            grp = pd.concat([grp, proj_df], ignore_index=True)

        ts = grp[["date","stock_qty"]].copy()
        ts["stock_qty"] = ts["stock_qty"].astype(float)
        ts["resource_code"] = sku
        ts["center"] = ct

        # í•´ë‹¹ SKUì˜ ì´ë™ ë‚´ì—­ (WIP ì œì™¸)
        mv = moves[
            (moves["resource_code"] == sku) &
            ((moves["from_center"] == ct) | (moves["to_center"] == ct)) &
            (moves["carrier_mode"] != "WIP")
        ].copy()

        # ì¶œë°œ: onboard_dateì— - (ì„¼í„°ì—ì„œ ë‚˜ê°)
        eff_minus = (
            mv[mv["from_center"] == ct]
            .dropna(subset=["onboard_date"])
            .groupby("onboard_date", as_index=False)["qty_ea"].sum()
            .rename(columns={"onboard_date":"date","qty_ea":"delta"})
        )
        eff_minus["delta"] *= -1

        # ë„ì°©: event_dateì— + (ì„¼í„°ë¡œ ë“¤ì–´ì˜´)
        eff_plus = (
            mv[mv["to_center"] == ct]
            .dropna(subset=["event_date"])
            .groupby("event_date", as_index=False)["qty_ea"].sum()
            .rename(columns={"event_date":"date","qty_ea":"delta"})
        )

        eff_all = pd.concat([eff_minus, eff_plus], ignore_index=True).sort_values("date")
        for d, delta in zip(eff_all["date"], eff_all["delta"]):
            ts.loc[ts["date"] >= d, "stock_qty"] = ts.loc[ts["date"] >= d, "stock_qty"] + delta

        ts["stock_qty"] = ts["stock_qty"].clip(lower=0)
        lines.append(ts)

    # 2) WIP ë¼ì¸ ìƒì„± (ìƒˆë¡œìš´ ë¡œì§)
    wip_moves = moves[moves["carrier_mode"] == "WIP"].copy()
    if not wip_moves.empty:
        for sku in skus_sel:
            sku_wip = wip_moves[wip_moves["resource_code"] == sku]
            if sku_wip.empty:
                continue
                
            # WIP ì‹œì‘ (onboard_dateì— +)
            wip_start = (
                sku_wip[sku_wip["onboard_date"].notna()]
                .groupby("onboard_date", as_index=False)["qty_ea"].sum()
                .rename(columns={"onboard_date":"date","qty_ea":"delta"})
            )
            
            # WIP ì™„ë£Œ (event_dateì— -)
            wip_complete = (
                sku_wip[sku_wip["event_date"].notna()]
                .groupby("event_date", as_index=False)["qty_ea"].sum()
                .rename(columns={"event_date":"date","qty_ea":"delta"})
            )
            wip_complete["delta"] *= -1
            
            # WIP ë¼ì¸ ìƒì„±
            wip_deltas = pd.concat([wip_start, wip_complete], ignore_index=True)
            if not wip_deltas.empty:
                s = pd.Series(0, index=pd.to_datetime(full_dates))
                for d, v in wip_deltas.groupby("date")["delta"].sum().items():
                    d = pd.to_datetime(d)
                    if d < full_dates[0]:
                        s.iloc[:] = s.iloc[:] + v
                    else:
                        s.loc[s.index >= d] = s.loc[s.index >= d] + v

                wip_df = pd.DataFrame({
                    "date": s.index,
                    "center": "ìƒì‚°ì¤‘",
                    "resource_code": sku,
                    "stock_qty": s.values.clip(min=0)
                })
                lines.append(wip_df)
                
                # WIP ì™„ë£Œ ë¬¼ëŸ‰ì„ ì„¼í„° ë¼ì¸ì— ë°˜ì˜
                wip_done = sku_wip[sku_wip["event_date"].notna()].copy()
                if not wip_done.empty:
                    add_to_center = (
                        wip_done.groupby(["to_center","event_date"], as_index=False)["qty_ea"].sum()
                        .rename(columns={"to_center":"center","event_date":"date","qty_ea":"delta"})
                    )
                    
                    # í•´ë‹¹ ì„¼í„° ë¼ì¸ì— WIP ì™„ë£Œ ë¬¼ëŸ‰ ì¶”ê°€
                    for idx_line, ts_line in enumerate(lines):
                        if ts_line.empty or ts_line["resource_code"].iloc[0] != sku:
                            continue
                        ct_name = ts_line["center"].iloc[0]
                        if ct_name in ("ìƒì‚°ì¤‘", "ì´ë™ì¤‘"):
                            continue
                            
                        inc = add_to_center[add_to_center["center"].astype(str) == str(ct_name)]
                        if inc.empty:
                            continue
                            
                        ts = ts_line.sort_values("date").copy()
                        for d, q in zip(pd.to_datetime(inc["date"]), inc["delta"]):
                            ts.loc[ts["date"] >= d, "stock_qty"] = ts.loc[ts["date"] >= d, "stock_qty"] + int(q)
                        ts["stock_qty"] = ts["stock_qty"].clip(lower=0)
                        lines[idx_line] = ts

    if not lines:
        return pd.DataFrame(columns=["date","center","resource_code","stock_qty"])

    result = pd.concat(lines, ignore_index=True)
    result["date"] = pd.to_datetime(result["date"]).dt.normalize()
    result = result[(result["date"] >= start_dt) & (result["date"] <= horizon_end)]
    return result.sort_values(["resource_code","center","date"]).reset_index(drop=True)

# === ë©”ì¸ ì•± ===
st.set_page_config(page_title="SCM ì¬ê³  ëŒ€ì‹œë³´ë“œ", layout="wide")

st.title("ğŸš€ SCM ì¬ê³  ëŒ€ì‹œë³´ë“œ (ìƒˆë¡œìš´ ë²„ì „)")

# íƒ­ ìƒì„±
tab1, tab2, tab3 = st.tabs(["ğŸ“Š Excel ì—…ë¡œë“œ", "ğŸ“ˆ CSV ì—…ë¡œë“œ", "â˜ï¸ Google Sheets"])

# ì•± ì‹œì‘ ì‹œ Google Sheetsì—ì„œ ìë™ ë¡œë“œ ì‹œë„
if "snap_long" not in locals():
    try:
        with st.spinner("Google Sheetsì—ì„œ ë°ì´í„°ë¥¼ ìë™ ë¡œë“œí•˜ëŠ” ì¤‘..."):
            df_move, df_ref, df_incoming = load_from_gsheet()
            moves_raw = normalize_moves(df_move)
            snap_long = normalize_refined_snapshot(df_ref)
            
            # WIP ë¶ˆëŸ¬ì˜¤ê¸° ë° ë³‘í•©
            wip_df = load_wip_from_incoming(df_incoming)
            moves = merge_wip_as_moves(moves_raw, wip_df)
            
            st.success(f"âœ… Google Sheets ìë™ ë¡œë“œ ì™„ë£Œ! WIP {len(wip_df)}ê±´ ë°˜ì˜")
    except Exception as e:
        st.error(f"Google Sheets ìë™ ë¡œë“œ ì‹¤íŒ¨: {e}")
        st.caption("WebApp URL, ì‹œíŠ¸ ID, í† í°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

# ì„¼í„° ëª©ë¡ ìƒì„± (ì •ë¦¬ëœ ë²„ì „)
if "snap_long" in locals():
    centers_snap = set(snap_long["center"].dropna().astype(str).unique().tolist())
    centers_moves = set(moves["from_center"].dropna().astype(str).unique().tolist() + 
                       moves["to_center"].dropna().astype(str).unique().tolist())

    # ì„¼í„°ëª… í†µì¼ + ìœ íš¨í•˜ì§€ ì•Šì€ ì„¼í„° ì œê±°
    centers_moves_unified = set()
    for center in centers_moves:
        if center == "AcrossBUS":
            centers_moves_unified.add("ì–´í¬ë¡œìŠ¤ë¹„US")
        elif center not in ["WIP", "In-Transit", "", "nan", "None"]:
            centers_moves_unified.add(center)

    centers_snap_clean = set()
    for center in centers_snap:
        if center not in ["WIP", "In-Transit", "", "nan", "None"]:
            centers_snap_clean.add(center)

    centers = sorted(list(centers_snap_clean | centers_moves_unified))
    skus = sorted(snap_long["resource_code"].dropna().astype(str).unique().tolist())

    # í•„í„° UI
    st.sidebar.header("í•„í„°")
    centers_sel = st.sidebar.multiselect("ì„¼í„° ì„ íƒ", centers, default=(["íƒœê´‘KR"] if "íƒœê´‘KR" in centers else centers[:1]))
    skus_sel = st.sidebar.multiselect("SKU ì„ íƒ", skus, default=([s for s in ["BA00022","BA00021"] if s in skus] or skus[:2]))

    # ê¸°ê°„ ì„¤ì •
    today = pd.Timestamp.today().normalize()
    start_dt = st.sidebar.date_input("ì‹œì‘ì¼", value=today - pd.Timedelta(days=42))
    end_dt = st.sidebar.date_input("ì¢…ë£Œì¼", value=today + pd.Timedelta(days=60))
    
    start_dt = pd.Timestamp(start_dt).tz_localize(None)
    end_dt = pd.Timestamp(end_dt).tz_localize(None)
    
    # snap_longì˜ date ì»¬ëŸ¼ì„ íƒ€ì„ì¡´ ì—†ëŠ” ê²ƒìœ¼ë¡œ ë³€í™˜
    snap_long["date"] = snap_long["date"].dt.tz_localize(None)
    
    # movesì˜ ëª¨ë“  ë‚ ì§œ ì»¬ëŸ¼ì„ íƒ€ì„ì¡´ ì—†ëŠ” ê²ƒìœ¼ë¡œ ë³€í™˜
    for date_col in ["onboard_date", "arrival_date", "inbound_date", "real_departure", "event_date"]:
        if date_col in moves.columns:
            moves[date_col] = moves[date_col].dt.tz_localize(None)

    # íƒ€ì„ë¼ì¸ ìƒì„±
    if centers_sel and skus_sel:
        timeline = build_timeline(snap_long, moves, centers_sel, skus_sel, start_dt, end_dt, horizon_days=60)
        
        if not timeline.empty:
            # ì°¨íŠ¸ ìƒì„±
            fig = px.line(timeline, x="date", y="stock_qty", color="center", 
                         title="ì¬ê³  ì¶”ì´", labels={"stock_qty": "ì¬ê³ ëŸ‰(EA)"})
            
            # ì˜¤ëŠ˜ í‘œì‹œì„  ì¶”ê°€
            fig.add_vline(x=today, line_dash="solid", line_color="rgba(255, 0, 0, 0.4)", line_width=1)
            
            st.plotly_chart(fig, use_container_width=True)
            
            # ë°ì´í„° í…Œì´ë¸”
            st.subheader("ì¬ê³  ë°ì´í„°")
            st.dataframe(timeline, use_container_width=True)
        else:
            st.warning("ì„ íƒëœ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("ì„¼í„°ì™€ SKUë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
else:
    st.info("ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")
