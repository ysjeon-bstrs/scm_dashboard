"""
AI ì±—ë´‡ Function Calling ë²„ì „: OpenAI GPT-4o Function Calling
- í…ìŠ¤íŠ¸ ìš”ì•½ ì œê±° â†’ ë©”íƒ€ë°ì´í„°ë§Œ ì „ë‹¬ (90% í† í° ì ˆì•½)
- AIê°€ í•„ìš”í•œ í•¨ìˆ˜ë¥¼ ì§ì ‘ ì„ íƒ ë° í˜¸ì¶œ
- ì •í™•í•œ ê³„ì‚°, í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜
"""

import streamlit as st
import pandas as pd
import numpy as np
import openai  # OpenAIë¡œ ë³€ê²½
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import json
import math
from typing import Optional, Dict, List, Any


def safe_float(value) -> Optional[float]:
    """
    NaN, Infë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ì—¬ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ ê°’ìœ¼ë¡œ ë³€í™˜

    Args:
        value: ë³€í™˜í•  ê°’ (pandas/numpy íƒ€ì… í¬í•¨)

    Returns:
        float ë˜ëŠ” None (NaN/Infì¸ ê²½ìš°)
    """
    if pd.isna(value) or (isinstance(value, float) and math.isinf(value)):
        return None
    if isinstance(value, (np.floating, np.integer)):
        return float(value)
    return float(value)


def prepare_minimal_metadata(
    snapshot_df: pd.DataFrame,
    moves_df: Optional[pd.DataFrame] = None,
    timeline_df: Optional[pd.DataFrame] = None
) -> Dict[str, Any]:
    """
    í…ìŠ¤íŠ¸ ìš”ì•½ ëŒ€ì‹  ë©”íƒ€ë°ì´í„°ë§Œ ì¶”ì¶œ (í† í° 90% ì ˆì•½)

    Returns:
        ë©”íƒ€ë°ì´í„° dict (SKU ëª©ë¡, ì„¼í„° ëª©ë¡, ë‚ ì§œ ë²”ìœ„ ë“±)
    """
    # None ì²´í¬ ì¶”ê°€ (Phase 1 Quick Win)
    if snapshot_df is None or snapshot_df.empty:
        return {"status": "empty", "message": "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}

    metadata = {
        "status": "ok",
        "snapshot": {
            "total_rows": len(snapshot_df),
            "centers": sorted(snapshot_df["center"].unique().tolist()) if "center" in snapshot_df.columns else [],
            "skus": sorted(snapshot_df["resource_code"].unique().tolist()[:50]) if "resource_code" in snapshot_df.columns else [],  # ìƒìœ„ 50ê°œë§Œ
            "sku_count": int(snapshot_df["resource_code"].nunique()) if "resource_code" in snapshot_df.columns else 0,
            "date_range": None
        },
        "moves": {
            "available": moves_df is not None and not moves_df.empty,
            "date_range": None
        },
        "timeline": {
            "available": timeline_df is not None and not timeline_df.empty,
            "has_forecast": False,
            "date_range": None
        }
    }

    # ë‚ ì§œ ë²”ìœ„ (copy() ì œê±° - Phase 1 Quick Win)
    if "date" in snapshot_df.columns:
        date_series = pd.to_datetime(snapshot_df["date"], errors="coerce")
        min_date = date_series.min()
        max_date = date_series.max()
        if pd.notna(min_date) and pd.notna(max_date):
            metadata["snapshot"]["date_range"] = {
                "min": min_date.strftime('%Y-%m-%d'),
                "max": max_date.strftime('%Y-%m-%d')
            }

    if moves_df is not None and not moves_df.empty and "date" in moves_df.columns:
        date_series = pd.to_datetime(moves_df["date"], errors="coerce")
        min_date = date_series.min()
        max_date = date_series.max()
        if pd.notna(min_date) and pd.notna(max_date):
            metadata["moves"]["date_range"] = {
                "min": min_date.strftime('%Y-%m-%d'),
                "max": max_date.strftime('%Y-%m-%d')
            }

    if timeline_df is not None and not timeline_df.empty:
        if "is_forecast" in timeline_df.columns:
            metadata["timeline"]["has_forecast"] = timeline_df["is_forecast"].any()

    return metadata


# Gemini Function Declarations
GEMINI_FUNCTIONS = [
    {
        "name": "get_total_stock",
        "description": "ì „ì²´ ì¬ê³ ëŸ‰ì„ ì¡°íšŒí•©ë‹ˆë‹¤. ëª¨ë“  ì„¼í„°ì™€ SKUì˜ ì´ ì¬ê³ ë¥¼ í•©ì‚°í•©ë‹ˆë‹¤.",
        "parameters": {
            "type": "object",
            "properties": {},
            "required": []
        }
    },
    {
        "name": "get_stock_by_center",
        "description": "ì„¼í„°ë³„ ì¬ê³ ëŸ‰ì„ ì¡°íšŒí•©ë‹ˆë‹¤. íŠ¹ì • ì„¼í„°ë¥¼ ì§€ì •í•˜ê±°ë‚˜ ì „ì²´ ì„¼í„°ì˜ ì¬ê³ ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "parameters": {
            "type": "object",
            "properties": {
                "center": {
                    "type": "string",
                    "description": "ì„¼í„° ì½”ë“œ (ì˜ˆ: AMZUS, KR01). ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë“  ì„¼í„° ë°˜í™˜"
                }
            },
            "required": []
        }
    },
    {
        "name": "get_stock_by_sku",
        "description": "íŠ¹ì • SKUì˜ ì¬ê³ ëŸ‰ê³¼ ì„¼í„°ë³„ ë¶„í¬ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.",
        "parameters": {
            "type": "object",
            "properties": {
                "sku": {
                    "type": "string",
                    "description": "SKU ì½”ë“œ (ì˜ˆ: BA00021)"
                }
            },
            "required": ["sku"]
        }
    },
    {
        "name": "calculate_stockout_days",
        "description": "íŠ¹ì • SKUê°€ í’ˆì ˆë  ë•Œê¹Œì§€ ë‚¨ì€ ì¼ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ìµœê·¼ 7ì¼ í‰ê·  íŒë§¤ëŸ‰ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡í•©ë‹ˆë‹¤.",
        "parameters": {
            "type": "object",
            "properties": {
                "sku": {
                    "type": "string",
                    "description": "SKU ì½”ë“œ (ì˜ˆ: BA00021)"
                }
            },
            "required": ["sku"]
        }
    },
    {
        "name": "get_top_selling_skus",
        "description": "ìµœê·¼ 30ì¼ íŒë§¤ëŸ‰ì´ ë§ì€ ìƒìœ„ SKU ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.",
        "parameters": {
            "type": "object",
            "properties": {
                "limit": {
                    "type": "integer",
                    "description": "ì¡°íšŒí•  SKU ê°œìˆ˜. ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ 5ê°œ ë°˜í™˜"
                }
            },
            "required": []
        }
    },
    {
        "name": "get_sku_trend",
        "description": "íŠ¹ì • SKUì˜ ì‹œê³„ì—´ ì¬ê³  ì¶”ì„¸ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤. ì¼ë³„ ì¬ê³  ë³€í™”ì™€ ì˜ˆì¸¡ ë°ì´í„°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.",
        "parameters": {
            "type": "object",
            "properties": {
                "sku": {
                    "type": "string",
                    "description": "SKU ì½”ë“œ (ì˜ˆ: BA00021)"
                },
                "days": {
                    "type": "integer",
                    "description": "ì¡°íšŒí•  ì¼ìˆ˜. ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ 30ì¼ ë°˜í™˜"
                }
            },
            "required": ["sku"]
        }
    },
    {
        "name": "get_sales_summary",
        "description": "íŠ¹ì • SKUì˜ íŒë§¤ ìš”ì•½ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤. ì„¼í„°ë³„, ë‚ ì§œë³„ íŒë§¤ëŸ‰ì„ í¬í•¨í•©ë‹ˆë‹¤.",
        "parameters": {
            "type": "object",
            "properties": {
                "sku": {
                    "type": "string",
                    "description": "SKU ì½”ë“œ (ì˜ˆ: BA00021)"
                },
                "days": {
                    "type": "integer",
                    "description": "ì¡°íšŒí•  ì¼ìˆ˜. ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ 7ì¼ ë°˜í™˜"
                }
            },
            "required": ["sku"]
        }
    },
    {
        "name": "compare_skus",
        "description": "ë‘ SKUì˜ ì¬ê³ ëŸ‰, íŒë§¤ëŸ‰, ì¶”ì„¸ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.",
        "parameters": {
            "type": "object",
            "properties": {
                "sku1": {
                    "type": "string",
                    "description": "ì²« ë²ˆì§¸ SKU ì½”ë“œ"
                },
                "sku2": {
                    "type": "string",
                    "description": "ë‘ ë²ˆì§¸ SKU ì½”ë“œ"
                }
            },
            "required": ["sku1", "sku2"]
        }
    },
    {
        "name": "search_low_stock_skus",
        "description": "í’ˆì ˆ ì„ë°• SKUë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. ì§€ì •í•œ ì¼ìˆ˜ ì´ë‚´ì— í’ˆì ˆë  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” SKU ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.",
        "parameters": {
            "type": "object",
            "properties": {
                "days_threshold": {
                    "type": "integer",
                    "description": "í’ˆì ˆ ì„ë°• ê¸°ì¤€ ì¼ìˆ˜. ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ 7ì¼ ì‚¬ìš©"
                }
            },
            "required": []
        }
    },
    {
        "name": "forecast_sales",
        "description": "íŠ¹ì • SKUì˜ ë¯¸ë˜ íŒë§¤ëŸ‰ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ìµœê·¼ 4ì£¼ì˜ íŒë§¤ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„ í˜• íŠ¸ë Œë“œë¥¼ ê³„ì‚°í•˜ì—¬ ë‹¤ìŒ ì£¼ ë˜ëŠ” ë‹¤ìŒ Nì£¼ì˜ ì˜ˆìƒ íŒë§¤ëŸ‰ì„ ë°˜í™˜í•©ë‹ˆë‹¤.",
        "parameters": {
            "type": "object",
            "properties": {
                "sku": {
                    "type": "string",
                    "description": "SKU ì½”ë“œ (ì˜ˆ: BA00021)"
                },
                "weeks": {
                    "type": "integer",
                    "description": "ëª‡ ì£¼ í›„ë¥¼ ì˜ˆì¸¡í• ì§€. ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ 1ì£¼ í›„ (ë‹¤ìŒì£¼) ì˜ˆì¸¡"
                }
            },
            "required": ["sku"]
        }
    }
]


def get_latest_stock(snapshot_df: pd.DataFrame) -> pd.DataFrame:
    """
    ë‚ ì§œë³„ ìŠ¤ëƒ…ìƒ·ì—ì„œ ê°€ì¥ ìµœì‹  ë‚ ì§œì˜ ì¬ê³ ë§Œ ì¶”ì¶œ

    Args:
        snapshot_df: ë‚ ì§œë³„ ì¬ê³  ìŠ¤ëƒ…ìƒ·

    Returns:
        ìµœì‹  ë‚ ì§œì˜ ì¬ê³ ë§Œ í¬í•¨í•œ DataFrame
    """
    if snapshot_df is None or snapshot_df.empty:
        return snapshot_df

    if "date" not in snapshot_df.columns:
        return snapshot_df

    # ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
    df = snapshot_df.copy()
    df["date"] = pd.to_datetime(df["date"], errors="coerce")

    # âš ï¸ ì¤‘ìš”: NaT (ì˜ëª»ëœ ë‚ ì§œ) ì œê±°
    # NaTëŠ” ì •ë ¬ ì‹œ ë§¨ ë’¤ì— ë°°ì¹˜ë˜ì–´ .last()ì—ì„œ ì„ íƒë  ìˆ˜ ìˆìŒ
    df = df[df["date"].notna()]

    if df.empty:
        return df

    # ê° SKU + ì„¼í„°ë³„ë¡œ ê°€ì¥ ìµœì‹  ë‚ ì§œì˜ ë°ì´í„°ë§Œ ì„ íƒ
    if "resource_code" in df.columns and "center" in df.columns:
        # ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ í›„, ê° ê·¸ë£¹ì˜ ë§ˆì§€ë§‰(ìµœì‹ ) í–‰ë§Œ ì„ íƒ
        latest = df.sort_values("date").groupby(["resource_code", "center"], as_index=False).last()
    else:
        # ê·¸ëƒ¥ ê°€ì¥ ìµœì‹  ë‚ ì§œë§Œ
        latest_date = df["date"].max()
        latest = df[df["date"] == latest_date]

    return latest


def execute_function(
    function_name: str,
    parameters: Dict[str, Any],
    snapshot_df: pd.DataFrame,
    moves_df: Optional[pd.DataFrame] = None,
    timeline_df: Optional[pd.DataFrame] = None
) -> Dict[str, Any]:
    """
    OpenAIê°€ ìš”ì²­í•œ í•¨ìˆ˜ë¥¼ ì‹¤í–‰

    Args:
        function_name: í•¨ìˆ˜ ì´ë¦„
        parameters: í•¨ìˆ˜ íŒŒë¼ë¯¸í„°
        snapshot_df: ì¬ê³  ë°ì´í„° (ë‚ ì§œë³„ ìŠ¤ëƒ…ìƒ·)
        moves_df: íŒë§¤ ë°ì´í„°
        timeline_df: ì‹œê³„ì—´ ë°ì´í„°

    Returns:
        í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼ dict
    """
    try:
        # âš ï¸ ì¤‘ìš”: ë‚ ì§œë³„ ìŠ¤ëƒ…ìƒ·ì—ì„œ ìµœì‹  ë‚ ì§œì˜ ì¬ê³ ë§Œ ì‚¬ìš©
        latest_snapshot = get_latest_stock(snapshot_df)

        if function_name == "get_total_stock":
            total = latest_snapshot["stock_qty"].sum()
            return {
                "total_stock": float(total),
                "unit": "ê°œ",
                "center_count": int(latest_snapshot["center"].nunique()),
                "sku_count": int(latest_snapshot["resource_code"].nunique())
            }

        elif function_name == "get_stock_by_center":
            center = parameters.get("center")
            if center:
                center_data = latest_snapshot[latest_snapshot["center"] == center]
                if center_data.empty:
                    return {"error": f"ì„¼í„° '{center}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
                return {
                    "center": center,
                    "total_stock": float(center_data["stock_qty"].sum()),
                    "sku_count": int(center_data["resource_code"].nunique()),
                    "unit": "ê°œ"
                }
            else:
                center_stock = latest_snapshot.groupby("center")["stock_qty"].sum().to_dict()
                return {
                    "centers": {k: float(v) for k, v in center_stock.items()},
                    "unit": "ê°œ"
                }

        elif function_name == "get_stock_by_sku":
            sku = parameters.get("sku")
            if not sku:
                return {"error": "SKU íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤"}

            sku_data = latest_snapshot[latest_snapshot["resource_code"] == sku]
            if sku_data.empty:
                return {"error": f"SKU '{sku}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

            by_center = sku_data.groupby("center")["stock_qty"].sum().to_dict()
            return {
                "sku": sku,
                "total_stock": float(sku_data["stock_qty"].sum()),
                "by_center": {k: float(v) for k, v in by_center.items()},
                "unit": "ê°œ"
            }

        elif function_name == "calculate_stockout_days":
            sku = parameters.get("sku")
            if not sku:
                return {"error": "SKU íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤"}

            # âœ… FIX: snapshot_dfì˜ sales_qty ì‚¬ìš© (ì‹¤ì œ íŒë§¤ ë°ì´í„°)
            if "sales_qty" not in snapshot_df.columns:
                return {"error": "íŒë§¤ ë°ì´í„°(sales_qty)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

            sales_data = snapshot_df.copy()
            sales_data["date"] = pd.to_datetime(sales_data["date"], errors="coerce")

            # ìµœê·¼ 7ì¼ í‰ê·  íŒë§¤ëŸ‰
            cutoff_date = sales_data["date"].max() - timedelta(days=7)
            sales_recent = sales_data[
                (sales_data["date"] >= cutoff_date) &
                (sales_data["resource_code"] == sku)
            ]

            if sales_recent.empty:
                return {"error": f"SKU '{sku}'ì˜ ìµœê·¼ 7ì¼ íŒë§¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}

            daily_sales = sales_recent["sales_qty"].sum() / 7
            # âš ï¸ ìµœì‹  ë‚ ì§œì˜ ì¬ê³ ë§Œ ì‚¬ìš©
            current_stock = latest_snapshot[latest_snapshot["resource_code"] == sku]["stock_qty"].sum()

            if daily_sales <= 0:
                return {
                    "sku": sku,
                    "message": "ìµœê·¼ 7ì¼ íŒë§¤ëŸ‰ì´ 0ì…ë‹ˆë‹¤",
                    "current_stock": safe_float(current_stock)
                }

            days_left = current_stock / daily_sales
            return {
                "sku": sku,
                "current_stock": safe_float(current_stock),
                "daily_sales_avg": safe_float(daily_sales),
                "days_until_stockout": safe_float(days_left),
                "status": "urgent" if days_left < 3 else "warning" if days_left < 7 else "ok"
            }

        elif function_name == "get_top_selling_skus":
            limit = parameters.get("limit", 5)

            # âœ… FIX: snapshot_dfì˜ sales_qty ì‚¬ìš© (ì‹¤ì œ íŒë§¤ ë°ì´í„°)
            if "sales_qty" not in snapshot_df.columns:
                return {"error": "íŒë§¤ ë°ì´í„°(sales_qty)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

            sales_data = snapshot_df.copy()
            sales_data["date"] = pd.to_datetime(sales_data["date"], errors="coerce")

            # ìµœê·¼ 30ì¼ í•„í„°ë§
            cutoff_date = sales_data["date"].max() - timedelta(days=30)
            sales_recent = sales_data[sales_data["date"] >= cutoff_date]

            # SKUë³„ íŒë§¤ëŸ‰ í•©ê³„
            top_skus = sales_recent.groupby("resource_code")["sales_qty"].sum().nlargest(limit)

            # resource_name ì¶”ê°€ (ìˆìœ¼ë©´)
            result_list = []
            for sku, qty in top_skus.items():
                sku_info = {"sku": sku, "quantity": safe_float(qty)}
                if "resource_name" in latest_snapshot.columns:
                    sku_rows = latest_snapshot[latest_snapshot["resource_code"] == sku]
                    if not sku_rows.empty:
                        name = sku_rows["resource_name"].iloc[0]
                        if pd.notna(name):
                            sku_info["product_name"] = str(name)
                result_list.append(sku_info)

            return {
                "top_skus": result_list,
                "period": "last_30_days",
                "unit": "ê°œ"
            }

        elif function_name == "get_sku_trend":
            sku = parameters.get("sku")
            days = parameters.get("days", 30)

            if not sku or timeline_df is None or timeline_df.empty:
                return {"error": "SKU íŒŒë¼ë¯¸í„°ì™€ ì‹œê³„ì—´ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤"}

            timeline = timeline_df[timeline_df["resource_code"] == sku].copy()
            if timeline.empty:
                return {"error": f"SKU '{sku}'ì˜ ì¶”ì„¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}

            timeline["date"] = pd.to_datetime(timeline["date"], errors="coerce")
            # NaT ì œê±°
            timeline = timeline[timeline["date"].notna()]

            if timeline.empty:
                return {"error": f"SKU '{sku}'ì˜ ìœ íš¨í•œ ë‚ ì§œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}

            # âš ï¸ ì¤‘ìš”: í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ê³¼ê±°/ë¯¸ë˜ êµ¬ë¶„
            from datetime import datetime
            today = pd.Timestamp(datetime.now().date())

            # ê³¼ê±° ë°ì´í„°ë§Œ ì„ íƒ (ë¯¸ë˜ ë‚ ì§œëŠ” ì˜ˆì¸¡ ë°ì´í„°)
            past_timeline = timeline[timeline["date"] <= today].copy()
            future_timeline = timeline[timeline["date"] > today].copy()

            # ê³¼ê±° Nì¼ ë°ì´í„° ì„ íƒ
            if not past_timeline.empty:
                cutoff_date = today - pd.Timedelta(days=days)
                past_timeline = past_timeline[past_timeline["date"] >= cutoff_date]
                past_timeline = past_timeline.sort_values("date")

            # ì‹¤ì œ vs ì˜ˆì¸¡ ë¶„ë¦¬
            if "is_forecast" in timeline.columns:
                # is_forecast í”Œë˜ê·¸ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                actual = past_timeline[past_timeline["is_forecast"] == False] if not past_timeline.empty else pd.DataFrame()

                # ê³¼ê±° ì˜ˆì¸¡ ë°ì´í„° (is_forecast=True but date <= today)
                past_forecast = past_timeline[past_timeline["is_forecast"] == True] if not past_timeline.empty else pd.DataFrame()

                # ê³¼ê±° ì˜ˆì¸¡ + ë¯¸ë˜ ë°ì´í„° í•©ì¹˜ê¸°
                forecast_parts = [df for df in [past_forecast, future_timeline] if not df.empty]
                forecast = pd.concat(forecast_parts, ignore_index=True) if forecast_parts else pd.DataFrame()
            else:
                # is_forecast í”Œë˜ê·¸ê°€ ì—†ìœ¼ë©´ ë‚ ì§œë¡œë§Œ êµ¬ë¶„
                actual = past_timeline
                forecast = future_timeline

            result = {
                "sku": sku,
                "actual_data": [
                    {
                        "date": row["date"].strftime('%Y-%m-%d') if pd.notna(row["date"]) else None,
                        "stock_qty": float(row["stock_qty"])
                    }
                    for _, row in actual.iterrows()
                ],
                "forecast_data": [
                    {
                        "date": row["date"].strftime('%Y-%m-%d') if pd.notna(row["date"]) else None,
                        "stock_qty": float(row["stock_qty"])
                    }
                    for _, row in forecast.iterrows()
                ]
            }

            # ì¶”ì„¸ ê³„ì‚°
            if len(actual) >= 2:
                first_qty = actual.iloc[0]["stock_qty"]
                last_qty = actual.iloc[-1]["stock_qty"]
                change = last_qty - first_qty
                change_pct = (change / first_qty * 100) if first_qty > 0 else 0
                result["trend"] = {
                    "direction": "ì¦ê°€" if change > 0 else "ê°ì†Œ" if change < 0 else "ìœ ì§€",
                    "change": float(change),
                    "change_percent": float(change_pct)
                }

            return result

        elif function_name == "get_sales_summary":
            sku = parameters.get("sku")
            days = parameters.get("days", 7)

            if not sku:
                return {"error": "SKU íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤"}

            # âœ… FIX: snapshot_dfì˜ sales_qty ì‚¬ìš© (ì‹¤ì œ íŒë§¤ ë°ì´í„°)
            if "sales_qty" not in snapshot_df.columns:
                return {"error": "íŒë§¤ ë°ì´í„°(sales_qty)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

            sales_data = snapshot_df.copy()
            sales_data["date"] = pd.to_datetime(sales_data["date"], errors="coerce")

            # ìµœê·¼ Nì¼ í•„í„°ë§
            cutoff_date = sales_data["date"].max() - timedelta(days=days)
            sku_sales = sales_data[
                (sales_data["date"] >= cutoff_date) &
                (sales_data["resource_code"] == sku)
            ]

            if sku_sales.empty:
                return {"error": f"SKU '{sku}'ì˜ ìµœê·¼ {days}ì¼ íŒë§¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}

            # ì„¼í„°ë³„ íŒë§¤ëŸ‰
            by_center = sku_sales.groupby("center")["sales_qty"].sum().to_dict()

            # ì¼ë³„ íŒë§¤ëŸ‰
            by_date = sku_sales.groupby(sku_sales["date"].dt.date)["sales_qty"].sum()

            total_sales = sku_sales["sales_qty"].sum()

            return {
                "sku": sku,
                "period_days": days,
                "total_sales": safe_float(total_sales),
                "daily_avg": safe_float(total_sales / days),
                "by_center": {k: safe_float(v) for k, v in by_center.items()},
                "daily_breakdown": [
                    {"date": str(date), "quantity": safe_float(qty)}
                    for date, qty in by_date.items()
                ],
                "unit": "ê°œ"
            }

        elif function_name == "compare_skus":
            sku1 = parameters.get("sku1")
            sku2 = parameters.get("sku2")

            if not sku1 or not sku2:
                return {"error": "ë‘ ê°œì˜ SKU íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤"}

            # ì¬ê³  ë¹„êµ - âš ï¸ ìµœì‹  ë‚ ì§œì˜ ì¬ê³ ë§Œ ì‚¬ìš©
            stock1 = latest_snapshot[latest_snapshot["resource_code"] == sku1]["stock_qty"].sum()
            stock2 = latest_snapshot[latest_snapshot["resource_code"] == sku2]["stock_qty"].sum()

            result = {
                "sku1": {
                    "code": sku1,
                    "stock": float(stock1)
                },
                "sku2": {
                    "code": sku2,
                    "stock": float(stock2)
                },
                "stock_diff": float(stock1 - stock2),
                "unit": "ê°œ"
            }

            # íŒë§¤ ë¹„êµ - âœ… FIX: snapshot_dfì˜ sales_qty ì‚¬ìš©
            if "sales_qty" in snapshot_df.columns:
                sales_data = snapshot_df.copy()
                sales_data["date"] = pd.to_datetime(sales_data["date"], errors="coerce")
                cutoff_date = sales_data["date"].max() - timedelta(days=30)
                sales_recent = sales_data[sales_data["date"] >= cutoff_date]

                sales1 = sales_recent[sales_recent["resource_code"] == sku1]["sales_qty"].sum()
                sales2 = sales_recent[sales_recent["resource_code"] == sku2]["sales_qty"].sum()

                result["sku1"]["sales_30d"] = safe_float(sales1)
                result["sku2"]["sales_30d"] = safe_float(sales2)
                result["sales_diff"] = safe_float(sales1 - sales2)

            return result

        elif function_name == "search_low_stock_skus":
            days_threshold = parameters.get("days_threshold", 7)

            # âœ… FIX: snapshot_dfì˜ sales_qty ì‚¬ìš© (ì‹¤ì œ íŒë§¤ ë°ì´í„°)
            if "sales_qty" not in snapshot_df.columns:
                return {"error": "íŒë§¤ ë°ì´í„°(sales_qty)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

            sales_data = snapshot_df.copy()
            sales_data["date"] = pd.to_datetime(sales_data["date"], errors="coerce")

            # ìµœê·¼ 7ì¼ íŒë§¤ ë°ì´í„°
            cutoff_date = sales_data["date"].max() - timedelta(days=7)
            sales_recent = sales_data[sales_data["date"] >= cutoff_date]

            # SKUë³„ ì¼í‰ê·  íŒë§¤ëŸ‰
            daily_sales_by_sku = sales_recent.groupby("resource_code")["sales_qty"].sum() / 7

            low_stock_skus = []
            for sku in daily_sales_by_sku.index:
                if daily_sales_by_sku[sku] <= 0:
                    continue

                # âš ï¸ ìµœì‹  ë‚ ì§œì˜ ì¬ê³ ë§Œ ì‚¬ìš©
                current_stock = latest_snapshot[latest_snapshot["resource_code"] == sku]["stock_qty"].sum()
                days_left = current_stock / daily_sales_by_sku[sku]

                if 0 < days_left <= days_threshold:
                    sku_info = {
                        "sku": sku,
                        "current_stock": safe_float(current_stock),
                        "daily_sales": safe_float(daily_sales_by_sku[sku]),
                        "days_left": safe_float(days_left),
                        "severity": "urgent" if days_left < 3 else "warning"
                    }
                    # resource_name ì¶”ê°€ (ìˆìœ¼ë©´)
                    if "resource_name" in latest_snapshot.columns:
                        sku_rows = latest_snapshot[latest_snapshot["resource_code"] == sku]
                        if not sku_rows.empty:
                            name = sku_rows["resource_name"].iloc[0]
                            if pd.notna(name):
                                sku_info["product_name"] = str(name)
                    low_stock_skus.append(sku_info)

            # ì‹¬ê°ë„ ìˆœ ì •ë ¬
            low_stock_skus.sort(key=lambda x: x["days_left"])

            return {
                "low_stock_skus": low_stock_skus[:10],  # ìƒìœ„ 10ê°œ
                "threshold_days": days_threshold,
                "total_found": len(low_stock_skus)
            }

        elif function_name == "forecast_sales":
            sku = parameters.get("sku")
            weeks = parameters.get("weeks", 1)

            if not sku:
                return {"error": "SKU íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤"}

            # âœ… snapshot_dfì˜ sales_qty ì‚¬ìš© (ì‹¤ì œ íŒë§¤ ë°ì´í„°)
            if "sales_qty" not in snapshot_df.columns:
                return {"error": "íŒë§¤ ë°ì´í„°(sales_qty)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

            sales_data = snapshot_df.copy()
            sales_data["date"] = pd.to_datetime(sales_data["date"], errors="coerce")

            # ìµœê·¼ 4ì£¼ ë°ì´í„° (28ì¼)
            latest_date = sales_data["date"].max()
            cutoff_date = latest_date - timedelta(days=28)
            sales_recent = sales_data[
                (sales_data["date"] >= cutoff_date) &
                (sales_data["resource_code"] == sku)
            ]

            if sales_recent.empty:
                return {"error": f"SKU '{sku}'ì˜ ìµœê·¼ 4ì£¼ íŒë§¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}

            # ì£¼ì°¨ë³„ íŒë§¤ëŸ‰ ê³„ì‚°
            sales_recent["week"] = sales_recent["date"].dt.isocalendar().week
            sales_recent["year"] = sales_recent["date"].dt.year
            weekly_sales = sales_recent.groupby(["year", "week"])["sales_qty"].sum().reset_index()
            weekly_sales = weekly_sales.sort_values(["year", "week"])

            if len(weekly_sales) < 2:
                # ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ìµœê·¼ 7ì¼ í‰ê·  ì‚¬ìš©
                recent_7days = sales_data[
                    (sales_data["date"] >= latest_date - timedelta(days=7)) &
                    (sales_data["resource_code"] == sku)
                ]
                avg_weekly = recent_7days["sales_qty"].sum()  # ìµœê·¼ 1ì£¼ ì´í•©

                return {
                    "sku": sku,
                    "forecast_weeks": weeks,
                    "predicted_sales": safe_float(avg_weekly * weeks),
                    "method": "recent_average",
                    "weekly_average": safe_float(avg_weekly),
                    "confidence": "low",
                    "note": "ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ìµœê·¼ 1ì£¼ í‰ê·  ì‚¬ìš©"
                }

            # ì„ í˜• íŠ¸ë Œë“œ ê³„ì‚°
            weekly_sales["week_index"] = list(range(len(weekly_sales)))
            x = weekly_sales["week_index"].values
            y = weekly_sales["sales_qty"].values

            # ê°„ë‹¨í•œ ì„ í˜• íšŒê·€ (y = mx + b)
            n = len(x)
            x_mean = x.mean()
            y_mean = y.mean()

            numerator = ((x - x_mean) * (y - y_mean)).sum()
            denominator = ((x - x_mean) ** 2).sum()

            if denominator != 0:
                slope = numerator / denominator
                intercept = y_mean - slope * x_mean

                # ë‹¤ìŒ weeksì£¼ ì˜ˆì¸¡
                next_week_index = len(weekly_sales)
                predicted_sales = []
                for i in range(int(weeks)):  # weeksë¥¼ intë¡œ ë³€í™˜
                    pred = intercept + slope * (next_week_index + i)
                    predicted_sales.append(max(0, pred))  # ìŒìˆ˜ ë°©ì§€

                total_predicted = sum(predicted_sales)

                # íŠ¸ë Œë“œ ë°©í–¥
                trend = "ì¦ê°€" if slope > 0 else "ê°ì†Œ" if slope < 0 else "ìœ ì§€"

                result = {
                    "sku": sku,
                    "forecast_weeks": int(weeks),
                    "predicted_sales": safe_float(total_predicted),
                    "weekly_breakdown": [safe_float(p) for p in predicted_sales],
                    "method": "linear_trend",
                    "trend": trend,
                    "trend_slope": safe_float(slope),
                    "recent_4weeks_average": safe_float(y_mean),
                    "confidence": "medium" if abs(slope) < y_mean * 0.2 else "low"
                }

                # product_name ì¶”ê°€ (ìˆìœ¼ë©´)
                if "resource_name" in latest_snapshot.columns:
                    sku_rows = latest_snapshot[latest_snapshot["resource_code"] == sku]
                    if not sku_rows.empty:
                        name = sku_rows["resource_name"].iloc[0]
                        if pd.notna(name):
                            result["product_name"] = str(name)

                return result
            else:
                # slope ê³„ì‚° ë¶ˆê°€ ì‹œ í‰ê·  ì‚¬ìš©
                avg_weekly = y_mean
                return {
                    "sku": sku,
                    "forecast_weeks": int(weeks),
                    "predicted_sales": safe_float(avg_weekly * weeks),
                    "method": "average",
                    "weekly_average": safe_float(avg_weekly),
                    "confidence": "medium"
                }

        else:
            return {"error": f"ì•Œ ìˆ˜ ì—†ëŠ” í•¨ìˆ˜: {function_name}"}

    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        st.error(f"ğŸ› í•¨ìˆ˜ ì‹¤í–‰ ìƒì„¸ ì˜¤ë¥˜:\n```\n{error_detail}\n```")
        return {"error": f"í•¨ìˆ˜ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"}


def ask_ai_with_functions(
    question: str,
    metadata: Dict[str, Any],
    snapshot_df: pd.DataFrame,
    moves_df: Optional[pd.DataFrame] = None,
    timeline_df: Optional[pd.DataFrame] = None,
    max_iterations: int = 5
) -> str:
    """
    OpenAI GPT-4o Function Callingìœ¼ë¡œ ì§ˆë¬¸ ë‹µë³€

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        metadata: ìµœì†Œ ë©”íƒ€ë°ì´í„°
        snapshot_df, moves_df, timeline_df: ì‹¤ì œ ë°ì´í„° (í•¨ìˆ˜ ì‹¤í–‰ìš©)
        max_iterations: ìµœëŒ€ í•¨ìˆ˜ í˜¸ì¶œ íšŸìˆ˜

    Returns:
        AI ë‹µë³€
    """
    try:
        today = datetime.now().strftime('%Y-%m-%d')

        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = openai.OpenAI(api_key=st.secrets["openai"]["api_key"])

        # ğŸ†• ì´ì „ ëŒ€í™” ë§¥ë½ ì¶”ê°€ (ëŒ€í™” ì—°ì†ì„±)
        context_section = ""
        if hasattr(st.session_state, 'last_question') and st.session_state.last_question:
            context_section = f"""

**ì´ì „ ëŒ€í™” ë§¥ë½:**
- ì´ì „ ì§ˆë¬¸: {st.session_state.last_question}
- ì´ì „ ë‹µë³€: {st.session_state.last_answer[:200]}...

**ì¤‘ìš”:** í˜„ì¬ ì§ˆë¬¸ì— "ê·¸ëŸ¼", "ê·¸ê²ƒë„", "ê°™ì€" ê°™ì€ ìƒëŒ€ì  í‘œí˜„ì´ ìˆë‹¤ë©´, ì´ì „ ëŒ€í™”ì˜ ë§¥ë½(SKU, ì„¼í„°, ê¸°ê°„ ë“±)ì„ ì°¸ê³ í•˜ì„¸ìš”.
"""

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = f"""ë‹¹ì‹ ì€ SCM ì¬ê³  ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**í˜„ì¬ ë‚ ì§œ: {today}**

**ì´ìš© ê°€ëŠ¥í•œ ë°ì´í„°:**
{json.dumps(metadata, ensure_ascii=False, indent=2)}{context_section}

**ë‹µë³€ ê·œì¹™ (ë§¤ìš° ì¤‘ìš”!):**
1. âš ï¸ ì¬ê³ ëŸ‰, íŒë§¤ëŸ‰, SKU ì •ë³´, ì˜ˆì¸¡ ë“± ë°ì´í„° ì¡°íšŒê°€ í•„ìš”í•œ ì§ˆë¬¸ì€ **ë°˜ë“œì‹œ ë¨¼ì € í•¨ìˆ˜ë¥¼ í˜¸ì¶œ**í•˜ì„¸ìš”
2. âš ï¸ ì ˆëŒ€ "í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê² ìŠµë‹ˆë‹¤"ë¼ê³  ë§ë§Œ í•˜ì§€ ë§ˆì„¸ìš” - ì¦‰ì‹œ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”
3. âš ï¸ "ë‹¤ìŒì£¼", "ëª‡ê°œ íŒ”ë¦´ê¹Œ", "ì˜ˆìƒ", "ì˜ˆì¸¡" ê°™ì€ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ forecast_sales() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”
4. í•¨ìˆ˜ ê²°ê³¼ë¥¼ ë°›ìœ¼ë©´ ê·¸ ì •í™•í•œ ìˆ«ìë¡œ ë‹µë³€í•˜ì„¸ìš” (ì‰¼í‘œ í¬ë§·: 1,234ê°œ)
5. 2-4ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
6. ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ë§Œ "ë°ì´í„°ì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
7. í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”

**ì¤‘ìš”:** í•¨ìˆ˜ í˜¸ì¶œ ê°€ëŠ¥í•œ ì§ˆë¬¸ì—ëŠ” ë°˜ë“œì‹œ í•¨ìˆ˜ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”. í…ìŠ¤íŠ¸ ì„¤ëª…ë§Œ í•˜ì§€ ë§ˆì„¸ìš”."""

        # OpenAI í˜•ì‹ìœ¼ë¡œ í•¨ìˆ˜ ì„ ì–¸ ë³€í™˜
        tools = []
        for func in GEMINI_FUNCTIONS:
            tools.append({
                "type": "function",
                "function": {
                    "name": func["name"],
                    "description": func["description"],
                    "parameters": func["parameters"]
                }
            })

        # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": question}
        ]

        # Function calling loop
        iteration = 0
        while iteration < max_iterations:
            # OpenAI API í˜¸ì¶œ
            response = client.chat.completions.create(
                model="gpt-4o",  # GPT-4o ëª¨ë¸
                messages=messages,
                tools=tools,
                tool_choice="auto"
            )

            message = response.choices[0].message

            # DEBUG: ì‘ë‹µ íƒ€ì… í™•ì¸
            if iteration == 0:
                has_tool_calls = message.tool_calls is not None
                st.caption(f"ğŸ” DEBUG: ì²« ì‘ë‹µ - tool_calls={has_tool_calls}")

            # í•¨ìˆ˜ í˜¸ì¶œì´ ìˆëŠ” ê²½ìš°
            if message.tool_calls:
                messages.append(message)  # assistant ë©”ì‹œì§€ ì¶”ê°€

                for tool_call in message.tool_calls:
                    function_name = tool_call.function.name
                    function_args = json.loads(tool_call.function.arguments)

                    st.caption(f"ğŸ”§ í•¨ìˆ˜ í˜¸ì¶œ: `{function_name}({json.dumps(function_args, ensure_ascii=False)})`")

                    # í•¨ìˆ˜ ì‹¤í–‰
                    result = execute_function(
                        function_name,
                        function_args,
                        snapshot_df,
                        moves_df,
                        timeline_df
                    )

                    # ğŸ” DEBUG: í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼ ë¡œê¹…
                    st.caption(f"ğŸ” DEBUG: í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼ - {json.dumps(result, ensure_ascii=False)[:200]}...")

                    # í•¨ìˆ˜ ê²°ê³¼ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "name": function_name,
                        "content": json.dumps(result, ensure_ascii=False)
                    })

                iteration += 1

            # í…ìŠ¤íŠ¸ ì‘ë‹µì´ë©´ ì¢…ë£Œ
            elif message.content:
                return message.content.strip()

            # finish_reasonì´ stopì´ë©´ ì¢…ë£Œ
            elif response.choices[0].finish_reason == "stop":
                if message.content:
                    return message.content.strip()
                else:
                    return "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            else:
                break

        # ìµœì¢… ì‘ë‹µ
        return "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        st.error(f"ğŸ› ask_ai_with_functions ìƒì„¸ ì˜¤ë¥˜:\n```\n{error_detail}\n```")
        return f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
def _safe_float(value) -> Optional[float]:
    """NaN, Infinityë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ì—¬ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
    try:
        if pd.isna(value):
            return None
        if isinstance(value, (float, np.floating)):
            if math.isinf(value):
                return None
        return float(value)
    except (ValueError, TypeError):
        return None


def detect_stockout_risks(
    snapshot_df: pd.DataFrame,
    moves_df: pd.DataFrame = None,
    timeline_df: pd.DataFrame = None,
    days_threshold: int = 7
) -> list[dict]:
    """
    í’ˆì ˆ ì„ë°• SKU ê°ì§€ (ê°œì„ ëœ ë²¡í„°í™” ë²„ì „)

    ì„±ëŠ¥ ê°œì„ :
    - ì´ì „: O(n Ã— m) ë³µì¡ë„ (SKU 1,000ê°œ Ã— Snapshot 10,000í–‰ = 10,000,000 ë¹„êµ)
    - ê°œì„ : O(n + m) ë³µì¡ë„ (ë²¡í„°í™” ì—°ì‚°)
    - ì˜ˆìƒ ì„±ëŠ¥: 2-3ì´ˆ â†’ 2-3ms (1000ë°° í–¥ìƒ)

    Gemini Function Calling í˜¸í™˜ì„±:
    - âœ… JSON ì§ë ¬í™” ê°€ëŠ¥ (NaN/Inf ì²˜ë¦¬)
    - âœ… float() ë³€í™˜ ì™„ë£Œ
    - âœ… None ê°’ ì•ˆì „ ì²˜ë¦¬

    Args:
        snapshot_df: ì¬ê³  + íŒë§¤ ë°ì´í„° (ì»¬ëŸ¼: resource_code, stock_qty, sales_qty, date)
        moves_df: (ì‚¬ìš© ì•ˆ í•¨ - í•˜ìœ„ í˜¸í™˜ì„±)
        timeline_df: ì˜ˆì¸¡ ë°ì´í„° (ì˜µì…˜)
        days_threshold: í’ˆì ˆ ì„ë°• ê¸°ì¤€ (ì¼)

    Returns:
        í’ˆì ˆ ì„ë°• SKU ë¦¬ìŠ¤íŠ¸ (ìƒìœ„ 5ê°œ)
    """
    risks = []

    # âœ… ê°œì„ : None ì²´í¬ + sales_qty í™•ì¸
    if snapshot_df is None or snapshot_df.empty or "sales_qty" not in snapshot_df.columns:
        return risks

    try:
        # âš ï¸ ì¤‘ìš”: ìµœì‹  ë‚ ì§œì˜ ì¬ê³ ë§Œ ì‚¬ìš©
        latest_snapshot = get_latest_stock(snapshot_df)

        # âœ… FIX: snapshot_dfì˜ sales_qty ì‚¬ìš© (ì‹¤ì œ íŒë§¤ ë°ì´í„°)
        sales_data = snapshot_df.copy()
        sales_data["date"] = pd.to_datetime(sales_data["date"], errors="coerce")

        # Phase 1: ìµœê·¼ 7ì¼ íŒë§¤ ë°ì´í„°
        cutoff_date = sales_data["date"].max() - pd.Timedelta(days=7)
        sales_recent = sales_data[sales_data["date"] >= cutoff_date]

        if not sales_recent.empty:
            # SKUë³„ ì¼í‰ê·  íŒë§¤ëŸ‰
            if "resource_code" in sales_recent.columns:
                daily_sales = sales_recent.groupby("resource_code")["sales_qty"].sum() / 7

                # âœ… Phase 2: ë²¡í„°í™”ëœ ì¬ê³  ë¶„ì„ (ë°˜ë³µë¬¸ ì œê±°!)
                # ì´ì „: for sku in daily_sales.index: current_stock = snapshot_df[...] (O(nÃ—m))
                # ê°œì„ : í•œ ë²ˆì— ëª¨ë“  SKUì˜ í˜„ì¬ ì¬ê³  ê³„ì‚° (O(m))
                # âš ï¸ ìµœì‹  ë‚ ì§œì˜ ì¬ê³ ë§Œ ì‚¬ìš©
                current_stock_by_sku = latest_snapshot.groupby("resource_code")["stock_qty"].sum()

                # íŒë§¤ ë°ì´í„°ì™€ ì¬ê³  ë°ì´í„° ë³‘í•©
                stock_analysis = pd.DataFrame({
                    "daily_sales": daily_sales,
                    "current_stock": current_stock_by_sku
                }).dropna()

                if not stock_analysis.empty:
                    # ë²¡í„°í™”ëœ ê³„ì‚°
                    stock_analysis["days_left"] = (
                        stock_analysis["current_stock"] / stock_analysis["daily_sales"]
                    )

                    # ë²¡í„°í™”ëœ ì¡°ê±´ í•„í„°ë§ (NaN/Inf ì•ˆì „ ì²˜ë¦¬)
                    risk_mask = (
                        (stock_analysis["daily_sales"] > 0) &
                        (stock_analysis["days_left"] > 0) &
                        (stock_analysis["days_left"] <= days_threshold) &
                        ~stock_analysis["days_left"].isna() &
                        ~stock_analysis["days_left"].isin([np.inf, -np.inf])
                    )

                    risk_skus = stock_analysis[risk_mask].sort_values("days_left")

                    # âœ… Phase 3: JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    for sku, row in risk_skus.iterrows():
                        current_stock = _safe_float(row["current_stock"])
                        daily_sales_val = _safe_float(row["daily_sales"])
                        days_left = _safe_float(row["days_left"])

                        if None not in [current_stock, daily_sales_val, days_left]:
                            risks.append({
                                "sku": str(sku),
                                "current_stock": current_stock,
                                "daily_sales": daily_sales_val,
                                "days_left": days_left,
                                "severity": "high" if days_left <= 3 else "medium"
                            })

    except Exception as e:
        st.warning(f"í’ˆì ˆ ìœ„í—˜ ê°ì§€ ì˜¤ë¥˜: {e}")

    return risks[:5]  # ìƒìœ„ 5ê°œë§Œ


def detect_anomalies(
    snapshot_df: pd.DataFrame,
    timeline_df: pd.DataFrame = None,
    threshold: float = 0.5
) -> list[dict]:
    """
    ì¬ê³  ì´ìƒì¹˜ ê°ì§€ (ê¸‰ì¦/ê¸‰ê°)

    Args:
        snapshot_df: í˜„ì¬ ì¬ê³ 
        timeline_df: ì‹œê³„ì—´ ë°ì´í„°
        threshold: ë³€í™”ìœ¨ ì„ê³„ê°’ (50% = 0.5)

    Returns:
        ì´ìƒì¹˜ SKU ë¦¬ìŠ¤íŠ¸
    """
    anomalies = []

    if timeline_df is None or timeline_df.empty:
        return anomalies

    try:
        timeline = timeline_df.copy()
        if "date" in timeline.columns and "resource_code" in timeline.columns:
            timeline["date"] = pd.to_datetime(timeline["date"], errors="coerce")

            # ì‹¤ì œ ë°ì´í„°ë§Œ
            if "is_forecast" in timeline.columns:
                timeline = timeline[timeline["is_forecast"] == False]

            # SKUë³„ ìµœê·¼ 7ì¼ vs ì´ì „ 7ì¼ ë¹„êµ
            latest_date = timeline["date"].max()
            recent_7days = timeline[timeline["date"] >= latest_date - pd.Timedelta(days=7)]
            prev_7days = timeline[
                (timeline["date"] >= latest_date - pd.Timedelta(days=14)) &
                (timeline["date"] < latest_date - pd.Timedelta(days=7))
            ]

            for sku in timeline["resource_code"].unique():
                recent_avg = recent_7days[recent_7days["resource_code"] == sku]["stock_qty"].mean()
                prev_avg = prev_7days[prev_7days["resource_code"] == sku]["stock_qty"].mean()

                if pd.notna(recent_avg) and pd.notna(prev_avg) and prev_avg > 0:
                    change_rate = (recent_avg - prev_avg) / prev_avg

                    if abs(change_rate) >= threshold:
                        anomalies.append({
                            "sku": sku,
                            "recent_avg": recent_avg,
                            "prev_avg": prev_avg,
                            "change_rate": change_rate,
                            "type": "ê¸‰ì¦" if change_rate > 0 else "ê¸‰ê°"
                        })

            # ë³€í™”ìœ¨ ì ˆëŒ“ê°’ ìˆœìœ¼ë¡œ ì •ë ¬
            anomalies.sort(key=lambda x: abs(x["change_rate"]), reverse=True)

    except Exception as e:
        st.warning(f"ì´ìƒì¹˜ ê°ì§€ ì˜¤ë¥˜: {e}")

    return anomalies[:3]  # ìƒìœ„ 3ê°œë§Œ


def check_data_quality(
    snapshot_df: pd.DataFrame,
    moves_df: pd.DataFrame = None,
    timeline_df: pd.DataFrame = None
) -> list[dict]:
    """
    ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ê°ì§€

    Returns:
        í’ˆì§ˆ ì´ìŠˆ ë¦¬ìŠ¤íŠ¸
    """
    issues = []

    try:
        # 1. ìŒìˆ˜ ì¬ê³  ì²´í¬
        if "stock_qty" in snapshot_df.columns:
            negative_stock = snapshot_df[snapshot_df["stock_qty"] < 0]
            if not negative_stock.empty:
                issues.append({
                    "type": "negative_stock",
                    "severity": "high",
                    "message": f"âš ï¸ ìŒìˆ˜ ì¬ê³  ë°œê²¬: {len(negative_stock)}ê°œ SKU",
                    "details": negative_stock[["resource_code", "center", "stock_qty"]].head(3).to_dict("records")
                })

        # 2. ë‚ ì§œ ëˆ„ë½ ì²´í¬ (moves_df)
        if moves_df is not None and not moves_df.empty and "date" in moves_df.columns:
            moves_df_copy = moves_df.copy()
            moves_df_copy["date"] = pd.to_datetime(moves_df_copy["date"], errors="coerce")
            null_dates = moves_df_copy["date"].isna().sum()
            if null_dates > 0:
                issues.append({
                    "type": "missing_dates",
                    "severity": "medium",
                    "message": f"âš ï¸ íŒë§¤ ë°ì´í„° ë‚ ì§œ ëˆ„ë½: {null_dates}ê±´",
                    "details": None
                })

        # 3. ìµœì‹  ë°ì´í„° í™•ì¸
        if "date" in snapshot_df.columns:
            snapshot_df_copy = snapshot_df.copy()
            snapshot_df_copy["date"] = pd.to_datetime(snapshot_df_copy["date"], errors="coerce")
            latest_date = snapshot_df_copy["date"].max()
            if pd.notna(latest_date):
                from datetime import datetime, timedelta
                days_old = (datetime.now() - latest_date).days
                if days_old > 1:
                    issues.append({
                        "type": "stale_data",
                        "severity": "low",
                        "message": f"â„¹ï¸ ì¬ê³  ë°ì´í„°ê°€ {days_old}ì¼ ì „ì…ë‹ˆë‹¤ (ìµœì‹ : {latest_date.strftime('%Y-%m-%d')})",
                        "details": None
                    })

    except Exception as e:
        st.warning(f"ë°ì´í„° í’ˆì§ˆ ì²´í¬ ì˜¤ë¥˜: {e}")

    return issues


def render_proactive_insights(
    snapshot_df: pd.DataFrame,
    moves_df: pd.DataFrame,
    timeline_df: pd.DataFrame
):
    """
    í”„ë¡œì•¡í‹°ë¸Œ ì¸ì‚¬ì´íŠ¸ UI ë Œë”ë§
    """
    # ì¸ì‚¬ì´íŠ¸ ê°ì§€
    stockout_risks = detect_stockout_risks(snapshot_df, moves_df, timeline_df)
    anomalies = detect_anomalies(snapshot_df, timeline_df)
    quality_issues = check_data_quality(snapshot_df, moves_df, timeline_df)

    # ì¸ì‚¬ì´íŠ¸ê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ í‘œì‹œ
    if stockout_risks or anomalies or quality_issues:
        with st.expander("ğŸ”” ì£¼ëª©í•  ì´ìŠˆ", expanded=True):
            col1, col2, col3 = st.columns(3)

            # í’ˆì ˆ ìœ„í—˜
            with col1:
                if stockout_risks:
                    st.markdown("**âš ï¸ í’ˆì ˆ ì„ë°•**")
                    for risk in stockout_risks[:3]:
                        severity_icon = "ğŸ”´" if risk["severity"] == "high" else "ğŸŸ¡"
                        st.caption(
                            f"{severity_icon} {risk['sku']}: "
                            f"{risk['days_left']:.1f}ì¼ ë‚¨ìŒ "
                            f"(ì¬ê³  {risk['current_stock']:.0f}ê°œ)"
                        )

            # ì´ìƒì¹˜
            with col2:
                if anomalies:
                    st.markdown("**ğŸ“Š ê¸‰ê²©í•œ ë³€í™”**")
                    for anomaly in anomalies[:3]:
                        icon = "ğŸ“ˆ" if anomaly["type"] == "ê¸‰ì¦" else "ğŸ“‰"
                        st.caption(
                            f"{icon} {anomaly['sku']}: "
                            f"{anomaly['type']} {abs(anomaly['change_rate'])*100:.0f}% "
                            f"({anomaly['prev_avg']:.0f}â†’{anomaly['recent_avg']:.0f})"
                        )

            # ë°ì´í„° í’ˆì§ˆ
            with col3:
                if quality_issues:
                    st.markdown("**ğŸ” ë°ì´í„° ì´ìŠˆ**")
                    for issue in quality_issues[:3]:
                        st.caption(issue["message"])


def calculate_kpi(
    function_name: str,
    snapshot_df: pd.DataFrame,
    moves_df: pd.DataFrame = None,
    **kwargs
) -> dict:
    """
    KPI ê³„ì‚° í•¨ìˆ˜ (Function callingìš©)

    Args:
        function_name: í˜¸ì¶œí•  í•¨ìˆ˜ ì´ë¦„
        snapshot_df: ì¬ê³  ë°ì´í„°
        moves_df: íŒë§¤ ë°ì´í„°
        **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°

    Returns:
        ê³„ì‚° ê²°ê³¼ dict
    """
    try:
        if function_name == "calculate_total_stock":
            total = snapshot_df["stock_qty"].sum()
            return {"total_stock": float(total), "unit": "ê°œ"}

        elif function_name == "get_stock_by_center":
            center_stock = snapshot_df.groupby("center")["stock_qty"].sum().to_dict()
            return {"center_stock": {k: float(v) for k, v in center_stock.items()}, "unit": "ê°œ"}

        elif function_name == "get_stock_by_sku":
            sku = kwargs.get("sku")
            if sku:
                sku_data = snapshot_df[snapshot_df["resource_code"] == sku]
                if not sku_data.empty:
                    total = sku_data["stock_qty"].sum()
                    by_center = sku_data.groupby("center")["stock_qty"].sum().to_dict()
                    return {
                        "sku": sku,
                        "total_stock": float(total),
                        "by_center": {k: float(v) for k, v in by_center.items()},
                        "unit": "ê°œ"
                    }
            return {"error": "SKU not found"}

        elif function_name == "calculate_stockout_days":
            sku = kwargs.get("sku")
            if sku and moves_df is not None:
                # ìµœê·¼ 7ì¼ í‰ê·  íŒë§¤ëŸ‰
                moves_recent = moves_df.copy()
                moves_recent["date"] = pd.to_datetime(moves_recent["date"], errors="coerce")
                cutoff_date = moves_recent["date"].max() - pd.Timedelta(days=7)
                moves_recent = moves_recent[
                    (moves_recent["date"] >= cutoff_date) &
                    (moves_recent["resource_code"] == sku)
                ]

                if not moves_recent.empty:
                    daily_sales = moves_recent["quantity"].sum() / 7
                    current_stock = snapshot_df[snapshot_df["resource_code"] == sku]["stock_qty"].sum()

                    if daily_sales > 0:
                        days_left = current_stock / daily_sales
                        return {
                            "sku": sku,
                            "current_stock": float(current_stock),
                            "daily_sales_avg": float(daily_sales),
                            "days_until_stockout": float(days_left),
                            "status": "urgent" if days_left < 3 else "warning" if days_left < 7 else "ok"
                        }

            return {"error": "Cannot calculate stockout days"}

        elif function_name == "get_top_selling_skus":
            limit = kwargs.get("limit", 5)
            if moves_df is not None:
                moves_recent = moves_df.copy()
                moves_recent["date"] = pd.to_datetime(moves_recent["date"], errors="coerce")
                cutoff_date = moves_recent["date"].max() - pd.Timedelta(days=30)
                moves_recent = moves_recent[moves_recent["date"] >= cutoff_date]

                top_skus = moves_recent.groupby("resource_code")["quantity"].sum().nlargest(limit)
                return {
                    "top_skus": {k: float(v) for k, v in top_skus.items()},
                    "period": "last_30_days",
                    "unit": "ê°œ"
                }

            return {"error": "No sales data available"}

    except Exception as e:
        return {"error": str(e)}

    return {"error": "Unknown function"}


def detect_kpi_need(question: str) -> tuple[bool, str, dict]:
    """
    ì§ˆë¬¸ì—ì„œ KPI ê³„ì‚° í•„ìš” ì—¬ë¶€ ê°ì§€

    Returns:
        (need_kpi, function_name, kwargs)
    """
    question_lower = question.lower()

    # ì´ ì¬ê³ 
    if "ì´ ì¬ê³ " in question_lower or "ì „ì²´ ì¬ê³ " in question_lower:
        return (True, "calculate_total_stock", {})

    # ì„¼í„°ë³„ ì¬ê³ 
    if ("ì„¼í„°ë³„" in question_lower or "center" in question_lower) and "ì¬ê³ " in question_lower:
        return (True, "get_stock_by_center", {})

    # SKUë³„ ì¬ê³ 
    import re
    sku_pattern = r'\b[A-Z]{2}\d{5}\b'
    skus = re.findall(sku_pattern, question)
    if skus and "ì¬ê³ " in question_lower:
        return (True, "get_stock_by_sku", {"sku": skus[0]})

    # í’ˆì ˆ ì„ë°•
    if skus and ("í’ˆì ˆ" in question_lower or "ì†Œì§„" in question_lower or "ë‚¨ì€" in question_lower):
        return (True, "calculate_stockout_days", {"sku": skus[0]})

    # ìƒìœ„ íŒë§¤
    if "ìƒìœ„" in question_lower and ("íŒë§¤" in question_lower or "íŒë§¤ëŸ‰" in question_lower):
        return (True, "get_top_selling_skus", {"limit": 5})

    return (False, None, {})


def ask_ai(question: str, data_context: str, snapshot_df: pd.DataFrame = None, moves_df: pd.DataFrame = None) -> str:
    """
    OpenAIì—ê²Œ ì§ˆë¬¸í•˜ê¸° (ë°±ì—… í•¨ìˆ˜, Function calling ì—†ì´ ë‹¨ìˆœ í…ìŠ¤íŠ¸)

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        data_context: ë°ì´í„° ì»¨í…ìŠ¤íŠ¸
        snapshot_df: ì¬ê³  ë°ì´í„° (KPI ê³„ì‚°ìš©)
        moves_df: íŒë§¤ ë°ì´í„° (KPI ê³„ì‚°ìš©)

    Returns:
        AI ë‹µë³€
    """
    try:
        from datetime import datetime
        today = datetime.now().strftime('%Y-%m-%d')

        # 1. KPI ê³„ì‚° í•„ìš” ì—¬ë¶€ ê°ì§€
        need_kpi, func_name, kwargs = detect_kpi_need(question)

        kpi_result = None
        if need_kpi and snapshot_df is not None:
            kpi_result = calculate_kpi(func_name, snapshot_df, moves_df, **kwargs)

        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = openai.OpenAI(api_key=st.secrets["openai"]["api_key"])

        # 2. KPI ê²°ê³¼ê°€ ìˆìœ¼ë©´ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
        kpi_section = ""
        if kpi_result:
            import json
            kpi_section = f"""

[ì •í™•í•œ ê³„ì‚° ê²°ê³¼]
{json.dumps(kpi_result, ensure_ascii=False, indent=2)}

**ì¤‘ìš”:** ìœ„ ê³„ì‚° ê²°ê³¼ëŠ” ì½”ë“œë¡œ ì •í™•í•˜ê²Œ ê³„ì‚°ëœ ê°’ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ì´ ìˆ«ìë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."""

        prompt = f"""ë‹¹ì‹ ì€ SCM ì¬ê³  ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**í˜„ì¬ ë‚ ì§œ: {today}**

ì•„ë˜ ì¬ê³  ë°ì´í„°ë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

[ì¬ê³  ë°ì´í„°]
{data_context}{kpi_section}

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

**ë‹µë³€ ê·œì¹™:**
1. [ì •í™•í•œ ê³„ì‚° ê²°ê³¼]ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ê·¸ ìˆ«ìë¥¼ ì‚¬ìš©í•˜ì„¸ìš” (í…ìŠ¤íŠ¸ ë°ì´í„° ëŒ€ì‹ )
2. ìˆ«ìëŠ” ì‰¼í‘œë¡œ í¬ë§·íŒ…í•˜ì„¸ìš” (ì˜ˆ: 1,234ê°œ)
3. 2-4ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”
4. ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì€ "ë°ì´í„°ì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
5. ë‚ ì§œë¥¼ ì–¸ê¸‰í•  ë•ŒëŠ” í˜„ì¬ ë‚ ì§œ({today})ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³¼ê±°/ë¯¸ë˜ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”
6. "ğŸ“Š ì‹¤ì œ ë°ì´í„° ê¸°ê°„"ì€ ê³¼ê±° ì‹¤ì œ ë°ì´í„°, "ğŸ”® ì˜ˆì¸¡ ë°ì´í„° ê¸°ê°„"ì€ ë¯¸ë˜ ì˜ˆì¸¡ ë°ì´í„°ì…ë‹ˆë‹¤
7. í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”

ë‹µë³€:"""

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ SCM ì¬ê³  ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ]
        )
        return response.choices[0].message.content.strip()

    except Exception as e:
        return f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}\n\nì œê³µëœ ë°ì´í„°:\n{data_context}"


def suggest_followup_questions(question: str, answer: str, data_context: str) -> list[str]:
    """
    ë‹µë³€ì„ ê¸°ë°˜ìœ¼ë¡œ í›„ì† ì§ˆë¬¸ ì œì•ˆ

    Args:
        question: ì›ë˜ ì§ˆë¬¸
        answer: AI ë‹µë³€
        data_context: ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ (ê°„ëµ ë²„ì „)

    Returns:
        í›„ì† ì§ˆë¬¸ 3ê°œ
    """
    try:
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = openai.OpenAI(api_key=st.secrets["openai"]["api_key"])

        # ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ (í† í° ì ˆì•½)
        context_summary = data_context[:500] + "..." if len(data_context) > 500 else data_context

        prompt = f"""ë‹¹ì‹ ì€ SCM ì¬ê³  ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ìê°€ ë‹¤ìŒ ì§ˆë¬¸ì„ í–ˆê³ , ë‹µë³€ì„ ë°›ì•˜ìŠµë‹ˆë‹¤:

[ì§ˆë¬¸] {question}
[ë‹µë³€] {answer}

[ì´ìš© ê°€ëŠ¥í•œ ë°ì´í„°]
{context_summary}

ì´ì œ ì‚¬ìš©ìê°€ ê¶ê¸ˆí•´í•  ë§Œí•œ **í›„ì† ì§ˆë¬¸ 3ê°œ**ë¥¼ ì œì•ˆí•˜ì„¸ìš”.

**ê·œì¹™:**
1. ì›ë˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ë˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ëŠ” ì§ˆë¬¸
2. ì œê³µëœ ë°ì´í„°ë¡œ ë‹µë³€ ê°€ëŠ¥í•œ ì§ˆë¬¸ë§Œ ì œì•ˆ
3. ê° ì§ˆë¬¸ì€ 15ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ
4. êµ¬ì²´ì ì¸ SKU/ì„¼í„°/ë‚ ì§œê°€ ìˆìœ¼ë©´ í¬í•¨
5. í•œ ì¤„ì— í•˜ë‚˜ì”©, ë²ˆí˜¸ ì—†ì´ ì‘ì„±

ì˜ˆì‹œ:
BA00021ì˜ íŒë§¤ ì¶”ì„¸ëŠ”?
ë‹¤ìŒì£¼ ì˜ˆìƒ ì¬ê³ ëŠ”?
ì–´ëŠ ì„¼í„°ê°€ ì¬ê³ ê°€ ë¶€ì¡±í•œê°€ìš”?

í›„ì† ì§ˆë¬¸:"""

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ SCM ì¬ê³  ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ]
        )

        questions = [q.strip() for q in response.choices[0].message.content.strip().split('\n') if q.strip()]
        return questions[:3]  # ìƒìœ„ 3ê°œë§Œ

    except Exception as e:
        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì§ˆë¬¸ ë°˜í™˜
        return [
            "ì„¼í„°ë³„ ì¬ê³  ë¶„í¬ëŠ”?",
            "ì¬ê³ ê°€ ë¶€ì¡±í•œ SKUëŠ”?",
            "ìµœê·¼ íŒë§¤ ì¶”ì„¸ëŠ”?"
        ]


def extract_entities_from_question(question: str, snapshot_df: pd.DataFrame, moves_df: pd.DataFrame = None) -> dict:
    """
    ì§ˆë¬¸ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ (SKU, ì„¼í„°, ë‚ ì§œ ë“±)

    Returns:
        {"skus": [list], "centers": [list], "date_range": tuple or None}
    """
    import re
    from datetime import datetime, timedelta

    entities = {
        "skus": [],
        "centers": [],
        "date_range": None
    }

    # 1. SKU ì¶”ì¶œ (BA00021 í˜•ì‹)
    sku_pattern = r'\b[A-Z]{2}\d{5}\b'
    found_skus = re.findall(sku_pattern, question)
    if found_skus and "resource_code" in snapshot_df.columns:
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” SKUë§Œ
        valid_skus = snapshot_df["resource_code"].unique()
        entities["skus"] = [sku for sku in found_skus if sku in valid_skus]

    # 2. ì„¼í„° ì¶”ì¶œ
    question_upper = question.upper()
    question_lower = question.lower()

    # ì„¼í„° ë³„ì¹­ ë§¤í•‘ (ì‚¬ìš©ìê°€ ìì—°ì–´ë¡œ ì…ë ¥í•œ ê²½ìš°)
    center_aliases = {
        "ì•„ë§ˆì¡´": "AMZUS",
        "amazon": "AMZUS",
        # "ë¯¸êµ­", "us" ì œê±° - CJì„œë¶€USì™€ ì¶©ëŒ ë°©ì§€
        "í•œêµ­": "íƒœê´‘KR",
        "korea": "íƒœê´‘KR",
        "kr": "íƒœê´‘KR",
        "íƒœê´‘": "íƒœê´‘KR",
        # CJì„œë¶€US ë³„ì¹­
        "ë¯¸êµ­cj": "CJì„œë¶€US",
        "cjë¯¸êµ­": "CJì„œë¶€US",
        "ë¯¸êµ­ì°½ê³ ": "CJì„œë¶€US",
        "ë¯¸êµ­ì„¼í„°": "CJì„œë¶€US",
        "ì„œë¶€ì°½ê³ ": "CJì„œë¶€US",
        "ë¯¸êµ­ì„œë¶€": "CJì„œë¶€US"
    }

    # ë³„ì¹­ìœ¼ë¡œ ì„¼í„° ì°¾ê¸°
    for alias, center_code in center_aliases.items():
        if alias in question_lower:
            entities["centers"].append(center_code)

    # ì •í™•í•œ ì„¼í„°ëª…ìœ¼ë¡œë„ ì°¾ê¸°
    if "center" in snapshot_df.columns:
        all_centers = snapshot_df["center"].unique()
        for center in all_centers:
            if center in question_upper or center.lower() in question_lower:
                if center not in entities["centers"]:
                    entities["centers"].append(center)

    # AMZUS, KR01 ë“± í”í•œ íŒ¨í„´ (ì •ê·œì‹)
    center_patterns = [r'\bAMZUS\b', r'\bAMZKR\b', r'\bKR0[1-9]\b']
    for pattern in center_patterns:
        matches = re.findall(pattern, question_upper)
        entities["centers"].extend(matches)

    entities["centers"] = list(set(entities["centers"]))  # ì¤‘ë³µ ì œê±°

    # 3. ë‚ ì§œ ì¶”ì¶œ (ìƒëŒ€ì  í‘œí˜„)
    today = datetime.now()
    question_lower = question.lower()

    if "ì˜¤ëŠ˜" in question_lower:
        entities["date_range"] = (today, today)
    elif "ì–´ì œ" in question_lower:
        yesterday = today - timedelta(days=1)
        entities["date_range"] = (yesterday, yesterday)
    elif "ìµœê·¼ 7ì¼" in question_lower or "ì§€ë‚œ ì¼ì£¼ì¼" in question_lower:
        entities["date_range"] = (today - timedelta(days=7), today)
    elif "ìµœê·¼ 30ì¼" in question_lower or "ì§€ë‚œ í•œë‹¬" in question_lower:
        entities["date_range"] = (today - timedelta(days=30), today)
    elif "ì´ë²ˆì£¼" in question_lower:
        # ì´ë²ˆ ì£¼ ì›”ìš”ì¼ë¶€í„°
        weekday = today.weekday()
        monday = today - timedelta(days=weekday)
        entities["date_range"] = (monday, today)

    # ì ˆëŒ€ ë‚ ì§œ íŒ¨í„´ (YYYY-MM-DD)
    date_pattern = r'\d{4}-\d{2}-\d{2}'
    date_matches = re.findall(date_pattern, question)
    if date_matches:
        try:
            date_obj = datetime.strptime(date_matches[0], '%Y-%m-%d')
            entities["date_range"] = (date_obj, date_obj)
        except:
            pass

    return entities


def analyze_question_for_chart(question: str) -> dict:
    """
    ì§ˆë¬¸ì„ ë¶„ì„í•´ì„œ ì°¨íŠ¸ í•„ìš” ì—¬ë¶€ ë° íƒ€ì… íŒë‹¨

    Returns:
        {"need_chart": bool, "chart_type": str, "entities": dict}
    """
    question_lower = question.lower()

    # ì°¨íŠ¸ê°€ ëª…ì‹œì ìœ¼ë¡œ í•„ìš”í•œ í‚¤ì›Œë“œ (ë” ì—„ê²©í•˜ê²Œ)
    explicit_chart_keywords = ["ê·¸ë˜í”„", "ì°¨íŠ¸", "ì‹œê°í™”", "ë³´ì—¬ì¤˜", "ê·¸ë ¤ì¤˜"]
    has_explicit_request = any(kw in question_lower for kw in explicit_chart_keywords)

    # ì°¨íŠ¸ê°€ ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” í‚¤ì›Œë“œ (ì¶”ì„¸, ë¹„êµ, ë¶„í¬ ë“±)
    implicit_chart_keywords = ["ì¶”ì„¸", "ë³€í™”", "ë¹„êµ", "ë¶„í¬", "íŠ¸ë Œë“œ", "ì„¼í„°ë³„", "skuë³„"]
    has_implicit_need = any(kw in question_lower for kw in implicit_chart_keywords)

    # ë‹¨ìˆœ ì‚¬ì‹¤ ì§ˆë¬¸ í‚¤ì›Œë“œ (ì°¨íŠ¸ ë¶ˆí•„ìš”)
    # ë‹¨, "ì„¼í„°ë³„"ì´ë‚˜ "skuë³„" ê°™ì€ ë¶„í¬ ì§ˆë¬¸ì€ ì œì™¸
    simple_fact_keywords = ["ì´ ëª‡", "ëª‡ê°œ", "ì–¼ë§ˆ"]
    is_simple_fact = any(kw in question_lower for kw in simple_fact_keywords) and not has_implicit_need

    # ëª…ì‹œì  ìš”ì²­ì´ ìˆê±°ë‚˜, ì•”ë¬µì  í•„ìš”ê°€ ìˆìœ¼ë©´ì„œ ë‹¨ìˆœ ì‚¬ì‹¤ ì§ˆë¬¸ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì°¨íŠ¸ ìƒì„±
    need_chart = has_explicit_request or (has_implicit_need and not is_simple_fact)

    # ì°¨íŠ¸ íƒ€ì… íŒë‹¨
    chart_type = None
    if need_chart:
        if "ì¶”ì„¸" in question_lower or "ë³€í™”" in question_lower or "íŠ¸ë Œë“œ" in question_lower:
            chart_type = "line"  # ì‹œê³„ì—´
        elif "ë¹„êµ" in question_lower or "ë¶„í¬" in question_lower or "ì„¼í„°ë³„" in question_lower or "skuë³„" in question_lower:
            chart_type = "bar"  # ë°” ì°¨íŠ¸
        elif "ë¹„ìœ¨" in question_lower or "ì ìœ " in question_lower:
            chart_type = "pie"  # íŒŒì´ ì°¨íŠ¸

    # ì—”í‹°í‹° ì¶”ì¶œ (ê°„ë‹¨ ë²„ì „)
    entities = {
        "has_sku": bool([s for s in question if s.isupper() and len(s) >= 6]),  # BA00021 ê°™ì€ íŒ¨í„´
        "has_center": any(c in question_lower for c in ["amz", "kr0", "ì„¼í„°"]),
        "time_related": any(t in question_lower for t in ["ì¼", "ì£¼", "ì›”", "ë‚ ì§œ", "ê¸°ê°„", "ì–´ì œ", "ì˜¤ëŠ˜"])
    }

    return {
        "need_chart": need_chart,
        "chart_type": chart_type,
        "entities": entities
    }


def generate_chart(
    question: str,
    snapshot_df: pd.DataFrame,
    moves_df: pd.DataFrame = None,
    timeline_df: pd.DataFrame = None
):
    """
    ì§ˆë¬¸ì— ë§ëŠ” ì°¨íŠ¸ ìë™ ìƒì„±

    Returns:
        plotly figure ë˜ëŠ” None
    """
    try:
        analysis = analyze_question_for_chart(question)

        if not analysis["need_chart"]:
            return None

        chart_type = analysis["chart_type"]
        entities = analysis["entities"]

        # 1. ì‹œê³„ì—´ ì°¨íŠ¸ (ì¶”ì„¸, ë³€í™”)
        if chart_type == "line" and timeline_df is not None and not timeline_df.empty:
            timeline = timeline_df.copy()
            if "date" in timeline.columns and "stock_qty" in timeline.columns:
                timeline["date"] = pd.to_datetime(timeline["date"], errors="coerce")
                timeline = timeline.sort_values("date")

                # íŠ¹ì • SKUê°€ ì–¸ê¸‰ë˜ì—ˆìœ¼ë©´ ê·¸ê²ƒë§Œ
                if entities["has_sku"] and "resource_code" in timeline.columns:
                    # ì§ˆë¬¸ì—ì„œ SKU ì¶”ì¶œ (ê°„ë‹¨ ë²„ì „)
                    import re
                    sku_pattern = r'\b[A-Z]{2}\d{5}\b'
                    skus = re.findall(sku_pattern, question)
                    if skus:
                        timeline = timeline[timeline["resource_code"].isin(skus)]

                # ì‹¤ì œ vs ì˜ˆì¸¡ êµ¬ë¶„
                if "is_forecast" in timeline.columns:
                    fig = go.Figure()

                    actual = timeline[timeline["is_forecast"] == False]
                    forecast = timeline[timeline["is_forecast"] == True]

                    if "resource_code" in timeline.columns:
                        for sku in timeline["resource_code"].unique()[:3]:  # ìµœëŒ€ 3ê°œ
                            sku_actual = actual[actual["resource_code"] == sku]
                            sku_forecast = forecast[forecast["resource_code"] == sku]

                            if not sku_actual.empty:
                                fig.add_trace(go.Scatter(
                                    x=sku_actual["date"],
                                    y=sku_actual["stock_qty"],
                                    name=f"{sku} (ì‹¤ì œ)",
                                    mode="lines+markers"
                                ))

                            if not sku_forecast.empty:
                                fig.add_trace(go.Scatter(
                                    x=sku_forecast["date"],
                                    y=sku_forecast["stock_qty"],
                                    name=f"{sku} (ì˜ˆì¸¡)",
                                    mode="lines",
                                    line=dict(dash="dash")
                                ))
                    else:
                        fig.add_trace(go.Scatter(x=actual["date"], y=actual["stock_qty"], name="ì‹¤ì œ"))
                        if not forecast.empty:
                            fig.add_trace(go.Scatter(
                                x=forecast["date"],
                                y=forecast["stock_qty"],
                                name="ì˜ˆì¸¡",
                                line=dict(dash="dash")
                            ))

                    fig.update_layout(
                        title="ì¬ê³  ì¶”ì„¸",
                        xaxis_title="ë‚ ì§œ",
                        yaxis_title="ì¬ê³ ëŸ‰",
                        height=400
                    )
                    return fig

                else:
                    fig = px.line(
                        timeline,
                        x="date",
                        y="stock_qty",
                        color="resource_code" if "resource_code" in timeline.columns else None,
                        title="ì¬ê³  ì¶”ì„¸"
                    )
                    fig.update_layout(height=400)
                    return fig

        # 2. ë°” ì°¨íŠ¸ (ì„¼í„°ë³„, SKUë³„ ë¹„êµ)
        elif chart_type == "bar":
            if "ì„¼í„°" in question or "center" in question.lower():
                # ì„¼í„°ë³„ ì¬ê³ 
                center_stock = snapshot_df.groupby("center")["stock_qty"].sum().reset_index()
                center_stock = center_stock.sort_values("stock_qty", ascending=False)

                fig = px.bar(
                    center_stock,
                    x="center",
                    y="stock_qty",
                    title="ì„¼í„°ë³„ ì¬ê³ ",
                    labels={"center": "ì„¼í„°", "stock_qty": "ì¬ê³ ëŸ‰"}
                )
                fig.update_layout(height=400)
                return fig

            elif "sku" in question.lower() or entities["has_sku"]:
                # SKUë³„ ì¬ê³  (ìƒìœ„ 10ê°œ)
                sku_stock = snapshot_df.groupby("resource_code")["stock_qty"].sum().reset_index()
                sku_stock = sku_stock.sort_values("stock_qty", ascending=False).head(10)

                fig = px.bar(
                    sku_stock,
                    x="resource_code",
                    y="stock_qty",
                    title="SKUë³„ ì¬ê³  (ìƒìœ„ 10ê°œ)",
                    labels={"resource_code": "SKU", "stock_qty": "ì¬ê³ ëŸ‰"}
                )
                fig.update_layout(height=400)
                return fig

        # 3. íŒŒì´ ì°¨íŠ¸ (ë¹„ìœ¨, ì ìœ ìœ¨)
        elif chart_type == "pie":
            center_stock = snapshot_df.groupby("center")["stock_qty"].sum().reset_index()

            fig = px.pie(
                center_stock,
                names="center",
                values="stock_qty",
                title="ì„¼í„°ë³„ ì¬ê³  ë¹„ìœ¨"
            )
            fig.update_layout(height=400)
            return fig

    except Exception as e:
        st.warning(f"ì°¨íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")

    return None


def render_simple_chatbot_tab(
    snapshot_df: pd.DataFrame,
    moves_df: pd.DataFrame,
    timeline_df: pd.DataFrame,
    selected_centers: list[str],
    selected_skus: list[str]
):
    """
    ê°„ë‹¨í•œ AI ì±—ë´‡ íƒ­ ë Œë”ë§

    Args:
        snapshot_df: ì „ì²´ ìŠ¤ëƒ…ìƒ· ë°ì´í„°
        moves_df: íŒë§¤/ì…ê³  ì´ë™ ë°ì´í„°
        timeline_df: 30ì¼ ì‹œê³„ì—´ + ì˜ˆì¸¡ ë°ì´í„°
        selected_centers: ì„ íƒëœ ì„¼í„°
        selected_skus: ì„ íƒëœ SKU
    """
    st.subheader("ğŸ¤– AI ì–´ì‹œìŠ¤í„´íŠ¸ (GPT-4o Function Calling - í† í° 90% ì ˆì•½)")

    # í•„í„°ë§
    snap = snapshot_df.copy()
    if "center" in snap.columns:
        snap = snap[snap["center"].astype(str).isin(selected_centers)]
    if "resource_code" in snap.columns:
        snap = snap[snap["resource_code"].astype(str).isin(selected_skus)]

    if snap.empty:
        st.warning("ì„ íƒëœ í•„í„°ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return

    st.caption(f"ğŸ“Š í•„í„°ë§ëœ ë°ì´í„°: {len(snap):,}í–‰ (ì„¼í„° {snap['center'].nunique()}ê³³, SKU {snap['resource_code'].nunique()}ê°œ)")

    # í”„ë¡œì•¡í‹°ë¸Œ ì¸ì‚¬ì´íŠ¸ í‘œì‹œ
    render_proactive_insights(snap, moves_df, timeline_df)

    st.divider()

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "last_question" not in st.session_state:
        st.session_state.last_question = ""
    if "last_answer" not in st.session_state:
        st.session_state.last_answer = ""
    if "last_metadata" not in st.session_state:
        st.session_state.last_metadata = {}
    if "last_entities" not in st.session_state:
        st.session_state.last_entities = {}  # ì´ì „ ì§ˆë¬¸ì˜ SKU, ì„¼í„°, ê¸°ê°„ ì €ì¥

    # pending_questionì´ ìˆìœ¼ë©´ ìë™ ì‹¤í–‰
    auto_ask = False
    if "pending_question" in st.session_state and st.session_state.pending_question:
        auto_ask = True

    # ì§ˆë¬¸ ì…ë ¥ í¼ (Enter í‚¤ ì§€ì›)
    with st.form(key="question_form", clear_on_submit=False):
        # pending_questionì´ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ì…ë ¥
        default_value = st.session_state.get("pending_question", "")

        question = st.text_input(
            "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
            placeholder="ì˜ˆ: ì´ ì¬ê³ ëŠ”? / BA00021ì€ ì–´ëŠ ì„¼í„°ì—? / ì¬ê³ ê°€ ê°€ì¥ ë§ì€ ì„¼í„°ëŠ”?",
            key="simple_q",
            value=default_value
        )
        submit_button = st.form_submit_button("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°", type="primary")

    # pending_question ì •ë¦¬ ë° ìë™ ì‹¤í–‰ íŠ¸ë¦¬ê±°
    if auto_ask:
        # pending_questionì„ questionìœ¼ë¡œ ë³µì‚¬
        question = st.session_state.pending_question
        st.session_state.pop("pending_question")
        # ë‹¤ìŒ rerunì—ì„œ formì´ ì œì¶œëœ ê²ƒì²˜ëŸ¼ ì²˜ë¦¬
        submit_button = True

    if submit_button and question:
        with st.spinner("ğŸ¤” ìƒê° ì¤‘..."):
            # ì§ˆë¬¸ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ (SKU, ì„¼í„°, ë‚ ì§œ)
            entities = extract_entities_from_question(question, snap, moves_df)

            # ğŸ†• ìƒëŒ€ì  í‘œí˜„ ê°ì§€ (ëŒ€í™” ë§¥ë½ ìœ ì§€)
            relative_keywords = ["ê·¸ëŸ¼", "ê·¸ê²ƒë„", "ê±°ê¸°ë„", "ê°™ì€", "ê·¸ sku", "ê·¸ ì„¼í„°", "ì—­ì‹œ", "ë˜"]
            has_relative_ref = any(kw in question.lower() for kw in relative_keywords)

            # ìƒëŒ€ì  í‘œí˜„ì´ ìˆê³ , í˜„ì¬ ì§ˆë¬¸ì— ì—”í‹°í‹°ê°€ ë¶€ì¡±í•˜ë©´ ì´ì „ ì—”í‹°í‹° ì¬ì‚¬ìš©
            context_note = ""
            if has_relative_ref and st.session_state.last_entities:
                # SKUê°€ ì—†ìœ¼ë©´ ì´ì „ SKU ì¬ì‚¬ìš©
                if not entities["skus"] and st.session_state.last_entities.get("skus"):
                    entities["skus"] = st.session_state.last_entities["skus"]
                    context_note += f"ì´ì „ SKU({entities['skus']}) ì¬ì‚¬ìš©. "

                # ì„¼í„°ê°€ ì—†ìœ¼ë©´ ì´ì „ ì„¼í„° ì¬ì‚¬ìš©
                if not entities["centers"] and st.session_state.last_entities.get("centers"):
                    entities["centers"] = st.session_state.last_entities["centers"]
                    context_note += f"ì´ì „ ì„¼í„°({entities['centers']}) ì¬ì‚¬ìš©. "

                # ë‚ ì§œê°€ ì—†ìœ¼ë©´ ì´ì „ ë‚ ì§œ ì¬ì‚¬ìš©
                if not entities["date_range"] and st.session_state.last_entities.get("date_range"):
                    entities["date_range"] = st.session_state.last_entities["date_range"]
                    context_note += f"ì´ì „ ê¸°ê°„ ì¬ì‚¬ìš©. "

                if context_note:
                    st.caption(f"ğŸ”— ëŒ€í™” ë§¥ë½: {context_note}")

            # ìë™ í•„í„°ë§
            filtered_snap = snap.copy()
            filtered_moves = moves_df.copy() if moves_df is not None else None
            filtered_timeline = timeline_df.copy() if timeline_df is not None else None

            filter_applied = False
            filter_msg = ""

            if entities["skus"]:
                filtered_snap = filtered_snap[filtered_snap["resource_code"].isin(entities["skus"])]
                if filtered_timeline is not None and "resource_code" in filtered_timeline.columns:
                    filtered_timeline = filtered_timeline[filtered_timeline["resource_code"].isin(entities["skus"])]
                filter_msg += f"SKU: {', '.join(entities['skus'])} "
                filter_applied = True

            if entities["centers"]:
                filtered_snap = filtered_snap[filtered_snap["center"].isin(entities["centers"])]
                if filtered_moves is not None and "center" in filtered_moves.columns:
                    filtered_moves = filtered_moves[filtered_moves["center"].isin(entities["centers"])]
                if filtered_timeline is not None and "center" in filtered_timeline.columns:
                    filtered_timeline = filtered_timeline[filtered_timeline["center"].isin(entities["centers"])]
                filter_msg += f"ì„¼í„°: {', '.join(entities['centers'])} "
                filter_applied = True

            if entities["date_range"] and filtered_moves is not None:
                start_date, end_date = entities["date_range"]
                if "date" in filtered_moves.columns:
                    filtered_moves["date"] = pd.to_datetime(filtered_moves["date"], errors="coerce")
                    filtered_moves = filtered_moves[
                        (filtered_moves["date"] >= start_date) &
                        (filtered_moves["date"] <= end_date)
                    ]
                filter_msg += f"ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}"
                filter_applied = True

            if filter_applied:
                st.info(f"ğŸ¯ ìë™ í•„í„° ì ìš©: {filter_msg}")

            # ìµœì†Œ ë©”íƒ€ë°ì´í„°ë§Œ ì¤€ë¹„ (í† í° 90% ì ˆì•½!)
            metadata = prepare_minimal_metadata(filtered_snap, filtered_moves, filtered_timeline)

            # Gemini 2.0 Native Function Callingìœ¼ë¡œ ì§ˆë¬¸
            answer = ask_ai_with_functions(
                question,
                metadata,
                filtered_snap,
                filtered_moves,
                filtered_timeline
            )

            # ì„¸ì…˜ì— ì €ì¥ (í•„í„°ë§ëœ ë°ì´í„°ë„ í•¨ê»˜)
            st.session_state.last_question = question
            st.session_state.last_answer = answer
            st.session_state.last_metadata = metadata
            st.session_state.last_filtered_snap = filtered_snap
            st.session_state.last_filtered_timeline = filtered_timeline
            st.session_state.last_entities = entities  # ğŸ†• ì—”í‹°í‹° ì €ì¥ (ëŒ€í™” ë§¥ë½ìš©)

    # ë‹µë³€ í‘œì‹œ (ì„¸ì…˜ì—ì„œ ë¡œë“œ)
    if st.session_state.last_answer:
        st.markdown("### ğŸ“Š ë‹µë³€")
        st.markdown(st.session_state.last_answer)

        # ì°¨íŠ¸ ìë™ ìƒì„± (ë¹„í™œì„±í™”)
        # ì°¨íŠ¸ê°€ íš¨ê³¼ì ì´ì§€ ì•Šì•„ì„œ ì¼ë‹¨ ë¹„í™œì„±í™”
        # chart_snap = st.session_state.get("last_filtered_snap", snap)
        # chart_timeline = st.session_state.get("last_filtered_timeline", timeline_df)
        #
        # chart_fig = generate_chart(
        #     st.session_state.last_question,
        #     chart_snap,
        #     moves_df,
        #     chart_timeline
        # )
        # if chart_fig:
        #     st.plotly_chart(chart_fig, use_container_width=True)

        # í›„ì† ì§ˆë¬¸ ì œì•ˆ
        with st.spinner("ğŸ’¡ í›„ì† ì§ˆë¬¸ ì œì•ˆ ì¤‘..."):
            # ë©”íƒ€ë°ì´í„°ë¥¼ ê°„ëµí•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            metadata_text = json.dumps(st.session_state.get("last_metadata", {}), ensure_ascii=False, indent=2)[:500]
            followup_questions = suggest_followup_questions(
                st.session_state.last_question,
                st.session_state.last_answer,
                metadata_text
            )

        if followup_questions:
            st.caption("**ğŸ’¬ ì´ëŸ° ê²ƒë„ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?**")
            cols = st.columns(3)
            for i, fq in enumerate(followup_questions):
                with cols[i]:
                    if st.button(fq, key=f"followup_{i}"):
                        st.session_state.pending_question = fq
                        st.rerun()

        # ë©”íƒ€ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…ìš©)
        with st.expander("ğŸ” AIê°€ ë³¸ ë©”íƒ€ë°ì´í„° (Function Calling)"):
            st.json(st.session_state.get("last_metadata", {}))

    # ì˜ˆì‹œ ì§ˆë¬¸
    st.divider()
    st.caption("**ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸:**")

    col1, col2 = st.columns(2)
    with col1:
        st.caption("**ì¬ê³  ì¡°íšŒ**")
        st.caption("â€¢ ì´ ì¬ê³ ëŠ”?")
        st.caption("â€¢ ì„¼í„°ë³„ ì¬ê³ ëŠ”?")
        st.caption("â€¢ BA00021ì€ ì–´ëŠ ì„¼í„°ì— ìˆë‚˜ìš”?")

    with col2:
        st.caption("**íŒë§¤/ì¬ê³  ì˜ˆì¸¡ ğŸ†•**")
        st.caption("â€¢ BA00021 ë‹¤ìŒì£¼ ì˜ˆìƒ íŒë§¤ëŸ‰ì€?")
        st.caption("â€¢ BA00022ì˜ íŒë§¤ ì¶”ì„¸ëŠ”?")
        st.caption("â€¢ ë‹¤ìŒ 2ì£¼ê°„ BA00021ì€ ëª‡ê°œ íŒ”ë¦´ê¹Œ?")
